{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>LLamaSharp is a cross-platform library to run \ud83e\udd99LLaMA/LLaVA model (and others) in local device. Based on llama.cpp, inference with LLamaSharp is efficient on both CPU and GPU. With the higher-level APIs and RAG support, it's convenient to deploy LLM (Large Language Model) in your application with LLamaSharp.</p>"},{"location":"#essential-insights-for-novice-learners","title":"Essential insights for novice learners","text":"<p>If you are new to LLM, here're some tips for you to help you to get start with <code>LLamaSharp</code>. If you are experienced in this field, we'd still recommend you to take a few minutes to read it because some things perform differently compared to cpp/python.</p> <ol> <li>The main ability of LLamaSharp is to provide an efficient way to run inference of LLM on your device (and fine-tune model in the future). The model weights, however, need to be downloaded from other resources such as huggingface.</li> <li>To gain high performance, LLamaSharp interacts with a native library compiled from c++, which is called <code>backend</code>. We provide backend packages for Windows, Linux and MAC with CPU, Cuda, Metal and OpenCL. You don't need to handle anything about c++ but just install the backend packages. If no published backend match your device, please open an issue to let us know. If compiling c++ code is not difficult for you, you could also follow this guide to compile a backend and run LLamaSharp with it.</li> <li><code>LLaMA</code> originally refers to the weights released by Meta (Facebook Research). After that, many models are fine-tuned based on it, such as <code>Vicuna</code>, <code>GPT4All</code>, and <code>Pyglion</code>. There are two popular file format of these model now, which are PyTorch format (.pth) and Huggingface format (.bin). LLamaSharp uses <code>GGUF</code> format file, which could be converted from these two formats. There are two options for you to get GGUF format file. a) Search model name + 'gguf' in Huggingface, you will find lots of model files that have already been converted to GGUF format. Please take care of the publishing time of them because some old ones could only work with old version of LLamaSharp. b) Convert PyTorch or Huggingface format to GGUF format yourself. Please follow the instructions of this part of llama.cpp readme to convert them with the python scripts.</li> <li>LLamaSharp supports multi-modal, which means that the model could take both text and image as input. Note that there are two model files required for using multi-modal (LLaVA), which are main model and mm-proj model. Here is a huggingface repo which shows that: link.</li> </ol>"},{"location":"#integrations","title":"Integrations","text":"<p>There are integarions for the following libraries, which help to expand the application of LLamaSharp. Integrations for semantic-kernel and kernel-memory are developed in LLamaSharp repository, while others are developed in their own repositories.</p> <ul> <li>semantic-kernel: an SDK that integrates LLM like OpenAI, Azure OpenAI, and Hugging Face.</li> <li>kernel-memory: a multi-modal AI Service specialized in the efficient indexing of datasets through custom continuous data hybrid pipelines, with support for RAG (Retrieval Augmented Generation), synthetic memory, prompt engineering, and custom semantic memory processing.</li> <li>BotSharp: an open source machine learning framework for AI Bot platform builder.</li> <li>Langchain: a framework for developing applications powered by language models.</li> </ul> <p></p>"},{"location":"#welcome-to-join-the-development","title":"Welcome to join the development!","text":"<p>Community effort is always one of the most important things in open-source projects. Any contribution in any way is welcomed here. For example, the following things mean a lot for LLamaSharp:</p> <ol> <li>Open an issue when you find something wrong.</li> <li>Open an PR if you've fixed something. Even if just correcting a typo, it also makes great sense.</li> <li>Help to optimize the documentation. </li> <li>Write an example or blog about how to integrate LLamaSharp with your APPs.</li> <li>Ask for a missing feature and discuss with us.</li> </ol> <p>If you'd like to get deeply involved in development, please touch us in discord channel or send email to <code>AsakusaRinne@gmail.com</code>. \ud83e\udd17</p>"},{"location":"Architecture/","title":"Architecture","text":""},{"location":"Architecture/#architecture-of-main-functions","title":"Architecture of main functions","text":"<p>The figure below shows the core framework structure of LLamaSharp.</p> <ul> <li>Native APIs: LLamaSharp calls the exported C APIs to load and run the model. The APIs defined in LLamaSharp specially for calling C APIs are named <code>Native APIs</code>. We have made all the native APIs public under namespace <code>LLama.Native</code>. However, it's strongly recommended not to use them unless you know what you are doing.</li> <li>LLamaWeights: The holder of the model weight.</li> <li>LLamaContext: A context which directly interact with the native library and provide some basic APIs such as tokenization and embedding. It takes use of <code>LLamaWeights</code>.</li> <li>LLamaExecutors: Executors which define the way to run the LLama model. It provides text-to-text and image-to-text APIs to make it easy to use. Currently we provide four kinds of executors: <code>InteractiveExecutor</code>, <code>InstructExecutor</code>, <code>StatelessExecutor</code> and <code>BatchedExecutor</code>. </li> <li>ChatSession: A wrapping for <code>InteractiveExecutor</code> and <code>LLamaContext</code>, which supports interactive tasks and saving/re-loading sessions. It also provides a flexible way to customize the text process by <code>IHistoryTransform</code>, <code>ITextTransform</code> and <code>ITextStreamTransform</code>.</li> <li>Integrations: Integrations with other libraries to expand the application of LLamaSharp. For example, if you want to do RAG (Retrieval Augmented Generation), kernel-memory integration is a good option for you.</li> </ul> <p></p>"},{"location":"ContributingGuide/","title":"LLamaSharp Contributing Guide","text":"<p>Hi, welcome to develop LLamaSharp with us together! We are always open for every contributor and any format of contributions! If you want to maintain this library actively together, please contact us to get the write access after some PRs. (Email: AsakusaRinne@gmail.com)</p> <p>In this page, we introduce how to make contributions here easily. \ud83d\ude0a</p>"},{"location":"ContributingGuide/#the-goal-of-llamasharp","title":"The goal of LLamaSharp","text":"<p>At the beginning, LLamaSharp is a C# binding of llama.cpp. It provided only some wrappers for llama.cpp to let C#/.NET users could run LLM models on their local device efficiently even if without any experience with C++. After around a year of development, more tools and integrations has been added to LLamaSharp, significantly expanding the application of LLamaSharp. Though llama.cpp is still the only backend of LLamaSharp, the goal of this repository is more likely to be an efficient and easy-to-use library of LLM inference, rather than just a binding of llama.cpp.</p> <p>In this way, our development of LLamaSharp is divided into two main directions:</p> <ol> <li>To make LLamaSharp more efficient. For example, <code>BatchedExecutor</code> could accept multiple queries and generate the response for them at the same time, which significantly improves the throughput. This part is always related with native APIs and executors in LLamaSharp.</li> <li>To make it easier to use LLamaSharp. We believe the best library is to let users build powerful functionalities with simple code. Higher-level APIs and integrations with other libraries are the key points of it.</li> </ol>"},{"location":"ContributingGuide/#how-to-compile-the-native-library-from-source","title":"How to compile the native library from source","text":"<p>If you want to contribute to the first direction of our goal, you may need to compile the native library yourself.</p> <p>Firstly, please follow the instructions in llama.cpp readme to configure your local environment. Most importantly, CMake with version higher than 3.14 should be installed on your device.</p> <p>Secondly, clone the llama.cpp repositories. You could manually clone it and checkout to the right commit according to Map of LLamaSharp and llama.cpp versions, or use clone the submodule of LLamaSharp when cloning LLamaSharp.</p> <pre><code>git clone --recursive https://github.com/SciSharp/LLamaSharp.git\n</code></pre> <p>If you want to support cublas in the compilation, please make sure that you've installed it. If you are using Intel CPU, please check the highest AVX (Advanced Vector Extensions) level that is supported by your device.</p> <p>As shown in llama.cpp cmake file, which also includes llama.cpp/ggml cmake file, there are many options that could be enabled or disabled when building the library. The following ones are commonly used when using it as a native library of LLamaSharp.</p> <pre><code>option(BUILD_SHARED_LIBS                \"build shared libraries\") // Please always enable it \noption(LLAMA_BUILD_TESTS                \"llama: build tests\") // Please disable it.\noption(LLAMA_BUILD_EXAMPLES             \"llama: build examples\") // Please disable it.\noption(LLAMA_BUILD_SERVER               \"llama: build server example\")// Please disable it.\n\noption(GGML_NATIVE                      \"llama: enable -march=native flag\") // Could be disabled\noption(GGML_AVX                         \"ggml: enable AVX\") // Enable it if the highest supported avx level is AVX\noption(GGML_AVX2                        \"ggml: enable AVX2\") // Enable it if the highest supported avx level is AVX2\noption(GGML_AVX512                      \"ggml: enable AVX512\") // Enable it if the highest supported avx level is AVX512\noption(GGML_CUDA                        \"ggml: use CUDA\") // Enable it if you have CUDA device\noption(GGML_BLAS                        \"ggml: use BLAS\") // Enable it if you want to use BLAS library to acclerate the computation on CPU\noption(GGML_VULKAN                      \"ggml: use Vulkan\") // Enable it if you have a device with Vulkan support\noption(GGML_METAL                       \"ggml: use Metal\") // Enable it if you are using a MAC with Metal device.\n</code></pre> <p>Most importantly, <code>-DBUILD_SHARED_LIBS=ON</code> must be added to the cmake instruction and other options depends on you. For example, when building with CUDA, use the following instruction:</p> <pre><code>mkdir build &amp;&amp; cd build\ncmake .. -DGGML_CUDA=ON -DBUILD_SHARED_LIBS=ON\ncmake --build . --config Release\n</code></pre> <p>Now you could find the <code>llama.dll</code>, <code>libllama.so</code> or <code>llama.dylib</code> in <code>build/src</code>. </p> <p>To load the compiled native library, please add the following code to the very beginning of your code.</p> <pre><code>NativeLibraryConfig.Instance.WithLibrary(\"&lt;Your native library path&gt;\");\n</code></pre>"},{"location":"ContributingGuide/#add-a-new-feature-to-llamasharp","title":"Add a new feature to LLamaSharp","text":"<p>After refactoring the framework in <code>v0.4.0</code>, LLamaSharp will try to maintain the backward compatibility. However, in the following cases a breaking change will be required:</p> <ol> <li>Due to some break changes in llama.cpp, making a breaking change will help to maintain the good abstraction and friendly user APIs.</li> <li>An important feature cannot be implemented unless refactoring some parts.</li> <li>After some discussions, an agreement was reached that making the break change is reasonable.</li> </ol> <p>If a new feature could be added without introducing any break change, please open a PR rather than open an issue first. We will never refuse the PR but help to improve it, unless it's malicious.</p> <p>When adding the feature, please take care of the namespace and the naming convention. For example, if you are adding an integration for WPF, please put the code under namespace <code>LLama.WPF</code> or <code>LLama.Integration.WPF</code> instead of putting it under the root namespace. The naming convention of LLamaSharp follows the pascal naming convention, but in some parts that are invisible to users, you can do whatever you want.</p>"},{"location":"ContributingGuide/#find-the-problem-and-fix-the-bug","title":"Find the problem and fix the BUG","text":"<p>If the issue is related to the LLM internal behaviour, such as endless generating the response, the best way to find the problem is to do comparison test between llama.cpp and LLamaSharp.</p> <p>You could use exactly the same prompt, the same model and the same parameters to run the inference in llama.cpp and LLamaSharp respectively to see if it's really a problem caused by the implementation in LLamaSharp.</p> <p>If the experiment showed that it worked well in llama.cpp but didn't in LLamaSharp, a search for the problem could be started. While the reason of the problem could be various, the best way I think is to add log-print in the code of llama.cpp and use it in LLamaSharp after compilation. Thus, when running LLamaSharp, you could see what happened in the native library.</p> <p>During the BUG fix process, please don't hesitate to discuss together when you are blocked.</p>"},{"location":"ContributingGuide/#add-integrations","title":"Add integrations","text":"<p>All kinds of integration are welcomed here! Currently the following integrations have been added but still need improvement:</p> <ol> <li>semantic-kernel</li> <li>kernel-memory</li> <li>BotSharp (maintained in SciSharp/BotSharp repo)</li> <li>Langchain (maintained in tryAGI/LangChain repo)</li> </ol> <p>If you find another library that is good to be integrated, please open an issue to let us know!</p>"},{"location":"ContributingGuide/#add-examples","title":"Add examples","text":"<p>There're mainly two ways to add an example:</p> <ol> <li>Add the example to <code>LLama.Examples</code> of the repository.</li> <li>Put the example in another repository and add the link to the readme or docs of LLamaSharp.</li> </ol>"},{"location":"ContributingGuide/#add-documents","title":"Add documents","text":"<p>LLamaSharp uses mkdocs to build the documentation, please follow the tutorial of mkdocs to add or modify documents in LLamaSharp.</p>"},{"location":"FAQ/","title":"Frequently asked questions","text":"<p>Sometimes, your application with LLM and LLamaSharp may have unexpected behaviours. Here are some frequently asked questions, which may help you to deal with your problem.</p>"},{"location":"FAQ/#why-gpu-is-not-used-when-i-have-installed-cuda","title":"Why GPU is not used when I have installed CUDA","text":"<ol> <li>If you are using backend packages, please make sure you have installed the cuda backend package which matches the cuda version of your device. Please note that before LLamaSharp v0.10.0, only one backend package should be installed.</li> <li>Add <code>NativeLibraryConfig.Instance.WithLogs(LLamaLogLevel.Info)</code> to the very beginning of your code. The log will show which native library file is loaded. If the CPU library is loaded, please try to compile the native library yourself and open an issue for that. If the CUDA library is loaded, please check if <code>GpuLayerCount &gt; 0</code> when loading the model weight.</li> </ol>"},{"location":"FAQ/#why-the-inference-is-slow","title":"Why the inference is slow","text":"<p>Firstly, due to the large size of LLM models, it requires more time to generate outputs than other models, especially when you are using models larger than 30B.</p> <p>To see if that's a LLamaSharp performance issue, please follow the two tips below.</p> <ol> <li>If you are using CUDA, Metal or OpenCL, please set <code>GpuLayerCount</code> as large as possible.</li> <li>If it's still slower than you expect it to be, please try to run the same model with same setting in llama.cpp examples. If llama.cpp outperforms LLamaSharp significantly, it's likely a LLamaSharp BUG and please report us for that.</li> </ol>"},{"location":"FAQ/#why-the-program-crashes-before-any-output-is-generated","title":"Why the program crashes before any output is generated","text":"<p>Generally, there are two possible cases for this problem:</p> <ol> <li>The native library (backend) you are using is not compatible with the LLamaSharp version. If you compiled the native library yourself, please make sure you have checkouted llama.cpp to the corresponding commit of LLamaSharp, which could be found at the bottom of README.</li> <li>The model file you are using is not compatible with the backend. If you are using a GGUF file downloaded from huggingface, please check its publishing time.</li> </ol>"},{"location":"FAQ/#why-my-model-is-generating-output-infinitely","title":"Why my model is generating output infinitely","text":"<p>Please set anti-prompt or max-length when executing the inference.</p> <p>Anti-prompt can also be called as \"Stop-keyword\", which decides when to stop the response generation. Under interactive mode, the maximum tokens count is always not set, which makes the LLM generates responses infinitively. Therefore, setting anti-prompt correctly helps a lot to avoid the strange behaviours. For example, the prompt file <code>chat-with-bob.txt</code> has the following content:</p> <pre><code>Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\n\nUser: Hello, Bob.\nBob: Hello. How may I help you today?\nUser: Please tell me the largest city in Europe.\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\nUser:\n</code></pre> <p>Therefore, the anti-prompt should be set as \"User:\". If the last line of the prompt is removed, LLM will automatically generate a question (user) and a response (bob) for one time when running the chat session. Therefore, the antiprompt is suggested to be appended to the prompt when starting a chat session.</p> <p>What if an extra line is appended? The string \"User:\" in the prompt will be followed with a char \"\\n\". Thus when running the model, the automatic generation of a pair of question and response may appear because the anti-prompt is \"User:\" but the last token is \"User:\\n\". As for whether it will appear, it's an undefined behaviour, which depends on the implementation inside the <code>LLamaExecutor</code>. Anyway, since it may leads to unexpected behaviors, it's recommended to trim your prompt or carefully keep consistent with your anti-prompt.</p>"},{"location":"FAQ/#how-to-run-llm-with-non-english-languages","title":"How to run LLM with non-English languages","text":"<p>English is the most popular language in the world, and in the region of LLM. If you want to accept inputs and generate outputs of other languages, please follow the two tips below.</p> <ol> <li>Ensure the model you selected is well-trained with data of your language. For example, LLaMA (original) used few Chinese text during the pretrain, while Chinese-LLaMA-Alpaca finetuned LLaMA with a large amount of Chinese text data. Therefore, the quality of the output of Chinese-LLaMA-Alpaca is much better than that of LLaMA.</li> </ol>"},{"location":"FAQ/#pay-attention-to-the-length-of-prompt","title":"Pay attention to the length of prompt","text":"<p>Sometimes we want to input a long prompt to execute a task. However, the context size may limit the inference of LLama model. Please ensure the inequality below holds.</p> \\[ len(prompt) + len(response) &lt; len(context) \\] <p>In this inequality, <code>len(response)</code> refers to the expected tokens for LLM to generate.</p>"},{"location":"FAQ/#choose-models-weight-depending-on-you-task","title":"Choose models weight depending on you task","text":"<p>The differences between modes may lead to much different behaviours under the same task. For example, if you're building a chat bot with non-English, a fine-tuned model specially for the language you want to use will have huge effect on the performance.</p>"},{"location":"QuickStart/","title":"Quick start","text":""},{"location":"QuickStart/#installation","title":"Installation","text":"<p>To gain high performance, LLamaSharp interacts with a native library compiled from c++, which is called <code>backend</code>. We provide backend packages for Windows, Linux and MAC with CPU, Cuda, Metal and OpenCL. You don't need to handle anything about c++ but just install the backend packages.</p> <p>If no published backend match your device, please open an issue to let us know. If compiling c++ code is not difficult for you, you could also follow this guide to compile a backend and run LLamaSharp with it.</p> <ol> <li>Install LLamaSharp package on NuGet:</li> </ol> <pre><code>PM&gt; Install-Package LLamaSharp\n</code></pre> <ol> <li> <p>Install one or more of these backends, or use self-compiled backend.</p> </li> <li> <p><code>LLamaSharp.Backend.Cpu</code>: Pure CPU for Windows &amp; Linux &amp; MAC. Metal (GPU) support for MAC.</p> </li> <li><code>LLamaSharp.Backend.Cuda11</code>: CUDA11 for Windows &amp; Linux.</li> <li><code>LLamaSharp.Backend.Cuda12</code>: CUDA 12 for Windows &amp; Linux.</li> <li> <p><code>LLamaSharp.Backend.OpenCL</code>: OpenCL for Windows &amp; Linux.</p> </li> <li> <p>(optional) For Microsoft semantic-kernel integration, install the LLamaSharp.semantic-kernel package.</p> </li> <li>(optional) To enable RAG support, install the LLamaSharp.kernel-memory package (this package only supports <code>net6.0</code> or higher yet), which is based on Microsoft kernel-memory integration.</li> </ol>"},{"location":"QuickStart/#model-preparation","title":"Model preparation","text":"<p>There are two popular format of model file of LLM now, which are PyTorch format (.pth) and Huggingface format (.bin). LLamaSharp uses <code>GGUF</code> format file, which could be converted from these two formats. To get <code>GGUF</code> file, there are two options:</p> <ol> <li> <p>Search model name + 'gguf' in Huggingface, you will find lots of model files that have already been converted to GGUF format. Please take care of the publishing time of them because some old ones could only work with old version of LLamaSharp.</p> </li> <li> <p>Convert PyTorch or Huggingface format to GGUF format yourself. Please follow the instructions of this part of llama.cpp readme to convert them with the python scripts.</p> </li> </ol> <p>Generally, we recommend downloading models with quantization rather than fp16, because it significantly reduce the required memory size while only slightly impact on its generation quality.</p>"},{"location":"QuickStart/#example-of-llama-chat-session","title":"Example of LLaMA chat session","text":"<p>Here is a simple example to chat with bot based on LLM in LLamaSharp. Please replace the model path with yours.</p> <p></p> <pre><code>using LLama.Common;\nusing LLama;\n\nstring modelPath = @\"&lt;Your Model Path&gt;\"; // change it to your own model path.\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024, // The longest length of chat as memory.\nGpuLayerCount = 5 // How many layers to offload to GPU. Please adjust it according to your GPU memory.\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\n// Add chat histories as prompt to tell AI how to act.\nvar chatHistory = new ChatHistory();\nchatHistory.AddMessage(AuthorRole.System, \"Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\");\nchatHistory.AddMessage(AuthorRole.User, \"Hello, Bob.\");\nchatHistory.AddMessage(AuthorRole.Assistant, \"Hello. How may I help you today?\");\n\nChatSession session = new(executor, chatHistory);\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nMaxTokens = 256, // No more than 256 tokens should appear in answer. Remove it if antiprompt is enough for control.\nAntiPrompts = new List&lt;string&gt; { \"User:\" } // Stop generation once antiprompts appear.\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.Write(\"The chat session has started.\\nUser: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\nawait foreach ( // Generate the response streamingly.\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n}\n</code></pre>"},{"location":"QuickStart/#examples-of-chatting-with-llava","title":"Examples of chatting with LLaVA","text":"<p>This example shows chatting with LLaVA to ask it to describe the picture. </p> <pre><code>using System.Text.RegularExpressions;\nusing LLama;\nusing LLama.Common;\n\nstring multiModalProj = @\"&lt;Your multi-modal proj file path&gt;\";\nstring modelPath = @\"&lt;Your LLaVA model file path&gt;\";\nstring modelImage = @\"&lt;Your image path&gt;\";\nconst int maxTokens = 1024; // The max tokens that could be generated.\n\nvar prompt = $\"{{{modelImage}}}\\nUSER:\\nProvide a full description of the image.\\nASSISTANT:\\n\";\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 4096,\nSeed = 1337,\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\n\n// Llava Init\nusing var clipModel = LLavaWeights.LoadFromFile(multiModalProj);\n\nvar ex = new InteractiveExecutor(context, clipModel);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the prompt is printed, the maximum tokens is set to {0} and the context size is {1}.\", maxTokens, parameters.ContextSize);\nConsole.WriteLine(\"To send an image, enter its filename in curly braces, like this {c:/image.jpg}.\");\n\nvar inferenceParams = new InferenceParams() { Temperature = 0.1f, AntiPrompts = new List&lt;string&gt; { \"\\nUSER:\" }, MaxTokens = maxTokens };\n\ndo\n{\n\n// Evaluate if we have images\n//\nvar imageMatches = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Value);\nvar imageCount = imageMatches.Count();\nvar hasImages = imageCount &gt; 0;\nbyte[][] imageBytes = null;\n\nif (hasImages)\n{\nvar imagePathsWithCurlyBraces = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Value);\nvar imagePaths = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Groups[1].Value);\n\ntry\n{\nimageBytes = imagePaths.Select(File.ReadAllBytes).ToArray();\n}\ncatch (IOException exception)\n{\nConsole.ForegroundColor = ConsoleColor.Red;\nConsole.Write(\n$\"Could not load your {(imageCount == 1 ? \"image\" : \"images\")}:\");\nConsole.Write($\"{exception.Message}\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Please try again.\");\nbreak;\n}\n\n\nint index = 0;\nforeach (var path in imagePathsWithCurlyBraces)\n{\n// First image replace to tag &lt;image, the rest of the images delete the tag\nif (index++ == 0)\nprompt = prompt.Replace(path, \"&lt;image&gt;\");\nelse\nprompt = prompt.Replace(path, \"\");\n}\nConsole.WriteLine();\n\n\n// Initialize Images in executor\n//\nex.ImagePaths = imagePaths.ToList();\n}\n\nConsole.ForegroundColor = ConsoleColor.White;\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\nConsole.Write(\" \");\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.WriteLine();\n\n// let the user finish with exit\n//\nif (prompt.Equals(\"/exit\", StringComparison.OrdinalIgnoreCase))\nbreak;\n\n}\nwhile (true);\n</code></pre> <p>For more examples, please refer to LLamaSharp.Examples.</p>"},{"location":"AdvancedTutorials/CustomizeNativeLibraryLoading/","title":"Customize the native library loading process","text":"<p>In the tutorial of native library config, we introduces how to auto-select a best native library as backend with your configurations. In this tutorial, we're taking a step further, to customize the native library loading process in a much more flexible way.</p>"},{"location":"AdvancedTutorials/CustomizeNativeLibraryLoading/#customize-the-policy-of-the-selection","title":"Customize the policy of the selection","text":"<p><code>INativeLibrarySelectingPolicy</code> is responsible for selecting the native libraries to load and sort them in order of priority. You could implement this interface and register your implementation by calling <code>NativeLibraryConfig.WithSelectingPolicy</code>.</p> <p>Note that when you're using your own implementation, the native libraries returned from <code>INativeLibrarySelectingPolicy.Apply</code> are all the libraries that are possibly used. It might spend lots of time of you to complete a robust implementation, but you can always fallback to the default behavior by calling <code>DefaultNativeLibrarySelectingPolicy</code> when no suitable library is found.</p> <pre><code>public interface INativeLibrarySelectingPolicy\n{\n/// &lt;summary&gt;\n/// Select the native library.\n/// &lt;/summary&gt;\n/// &lt;param name=\"description\"&gt;The description of the user's configuration.&lt;/param&gt;\n/// &lt;param name=\"systemInfo\"&gt;The system information of the current machine.&lt;/param&gt;\n/// &lt;param name=\"logCallback\"&gt;The log callback.&lt;/param&gt;\n/// &lt;returns&gt;The information of the selected native library files, in order by priority from the beginning to the end.&lt;/returns&gt;\nIEnumerable&lt;INativeLibrary&gt; Apply(NativeLibraryConfig.Description description, SystemInfo systemInfo, NativeLogConfig.LLamaLogCallback? logCallback = null);\n}\n</code></pre>"},{"location":"AdvancedTutorials/CustomizeNativeLibraryLoading/#customize-the-native-library","title":"Customize the native library","text":"<p>The native library in LLamaSharp is not just a file path. Instead, it's abstracted to <code>INativeLibrary</code>, which contains the information of it and allows finer-grained control of the loading process.</p> <pre><code>public interface INativeLibrary\n{\n/// &lt;summary&gt;\n/// Metadata of this library.\n/// &lt;/summary&gt;\nNativeLibraryMetadata? Metadata { get; }\n\n/// &lt;summary&gt;\n/// Prepare the native library file and returns the local path of it.\n/// If it's a relative path, LLamaSharp will search the path in the search directies you set.\n/// &lt;/summary&gt;\n/// &lt;param name=\"systemInfo\"&gt;The system information of the current machine.&lt;/param&gt;\n/// &lt;param name=\"logCallback\"&gt;The log callback.&lt;/param&gt;\n/// &lt;returns&gt;\n/// The relative paths of the library. You could return multiple paths to try them one by one. If no file is available, please return an empty array.\n/// &lt;/returns&gt;\nIEnumerable&lt;string&gt; Prepare(SystemInfo systemInfo, NativeLogConfig.LLamaLogCallback? logCallback = null);\n}\n</code></pre> <p><code>INativeLibrary.Metadata</code> contains the information of the native library. When you implement the interface, you can ignore it and return <code>null</code>. However, the best practice will always include implemenbting this peoperty because it will provide much more information when there's an anexpected behavior.</p> <p><code>INativeLibrary.Prepare</code> is a method which allows you to do some preparations before loading the library. For example, you can move the file, download the file, output some logs, etc. It is supposed to return a list of file paths in order of priority. </p> <p>There are several implementations inside LLamaSharp, which are <code>NativeLibraryFromPath</code>, <code>NativeLibraryWithAvx</code>, <code>NativeLibraryWithCuda</code> and <code>NativeLibraryWithMacOrFallback</code>. Please refer to their implementations if you find it difficult to implement <code>INativeLibrary</code>.</p>"},{"location":"AdvancedTutorials/CustomizeNativeLibraryLoading/#how-does-the-loading-work","title":"How does the loading work","text":"<p>Here're the steps for the native library loading in LLamaSharp.</p> <ol> <li>Gather and validate the user's configuration for native library loading. (<code>NativeLibraryConfig</code>)</li> <li>Gather the system information, mainly including platform and device.</li> <li>Apply the selecting policy to get the list of native libraries to load.</li> <li>For each of the selected native library, call the <code>INativeLibrary.Prepare</code> method to get the file paths. Then Try to load the library from the paths in turn. If the library is loaded successfully with any path of them, the loop will be broken.</li> </ol>"},{"location":"AdvancedTutorials/CustomizeNativeLibraryLoading/#example-use-remote-library-files","title":"Example: use remote library files","text":"<p>Though the native library downloading feature will be introduced as discussed in https://github.com/SciSharp/LLamaSharp/issues/670, it might have some security concerns. Here's an example to implement the remote native library downloading.</p> <p>Firstly, the native library with downloading process should be implemented.</p> <pre><code>public class AutoDownloadedLibraries\n{\n// Wrap a cuda native library\npublic class Cuda: INativeLibrary\n{\n// the default cuda native library implementation in LLamaSHarp\nprivate NativeLibraryWithCuda _cudaLibrary;\n// Some download settings\nprivate NativeLibraryDownloadSettings _settings;\n\npublic Cuda(NativeLibraryWithCuda cudaLibrary, NativeLibraryDownloadSettings settings)\n{\n_cudaLibrary = cudaLibrary;\n_settings = settings;\n}\n\npublic NativeLibraryMetadata? Metadata =&gt; _cudaLibrary.Metadata;\n\npublic IEnumerable&lt;string&gt; Prepare(SystemInfo systemInfo, NativeLogConfig.LLamaLogCallback? logCallback = null)\n{\nforeach(var relativePath in _cudaLibrary.Prepare(systemInfo, logCallback))\n{\n// try to use the default path first. If loaded successfully, the download will not be triggered.\nyield return relativePath;\n// download the file.\n// NOTE: be sure to complete the downloading process here. You CANNOT make `Prepare` as an async method.\nvar path = NativeLibraryDownloader.DownloadLibraryFile(_settings, relativePath, logCallback).Result;\n// if the downloading is successful, return the path of the downloaded file.\nif (path is not null)\n{\nyield return path;\n}\n}\n}\n}\n}\n</code></pre> <p>Then, implement the selecting policy for the native libraries above</p> <pre><code>public class SelectingPolicyWithAutoDownload: INativeLibrarySelectingPolicy\n{\n// making this class a wrapper for the default policy.\nprivate DefaultNativeLibrarySelectingPolicy _defaultPolicy = new();\n// record the download settings.\nprivate NativeLibraryDownloadSettings _downloadSettings;\n\ninternal SelectingPolicyWithAutoDownload(NativeLibraryDownloadSettings downloadSettings)\n{\n_downloadSettings = downloadSettings;\n}\n\npublic IEnumerable&lt;INativeLibrary&gt; Apply(NativeLibraryConfig.Description description, SystemInfo systemInfo, NativeLogConfig.LLamaLogCallback? logCallback)\n{\nforeach(var library in _defaultPolicy.Apply(description, systemInfo, logCallback))\n{\n// check the return type and returns the corresponding wrapper\nif(library is NativeLibraryWithCuda cudaLibrary)\n{\nyield return new AutoDownloadedLibraries.Cuda(cudaLibrary, _downloadSettings);\n}\nelse if(library is NativeLibraryWithAvx avxLibrary)\n{\nyield return new AutoDownloadedLibraries.Avx(avxLibrary, _downloadSettings);\n}\nelse if(library is NativeLibraryWithMacOrFallback macLibrary)\n{\nyield return new AutoDownloadedLibraries.MacOrFallback(macLibrary, _downloadSettings);\n}\n// Generally, you don't need to download the DLL if the user specify a path.\n// But if you want, you can certainly add a wrapper for it.\nelse if(library is NativeLibraryFromPath)\n{\nyield return library;\n}\n// It's also reasonable to throw an exception here.\nelse\n{\nyield return library;\n}\n}\n}\n}\n</code></pre> <p>Finally, add an extension to make an API for users to enable this feature.</p> <pre><code> public static class NativeLibraryAutoDownloadExtension\n{\npublic static NativeLibraryConfig WithAutoDownload(this NativeLibraryConfig config, bool enable = true, NativeLibraryDownloadSettings? settings = null)\n{\nif (config.LibraryHasLoaded)\n{\nthrow new Exception(\"The library has already loaded, you can't change the configurations. \" +\n\"Please finish the configuration setting before any call to LLamaSharp native APIs.\" +\n\"Please use NativeLibraryConfig.DryRun if you want to see whether it's loaded successfully \" +\n\"but still have chance to modify the configurations.\");\n}\nif (enable)\n{\nif(settings is null)\n{\nsettings = NativeLibraryDownloadSettings.Create();\n}\n// If you want to return an relative path in `INativeLibrary.Prepare`, \n// be sure to add the downloading directory to the search directories.\nvar defaultLocalDir = NativeLibraryDownloadSettings.GetDefaultLocalDir(settings);\n\n// When using auto-download, this should be the only search this directory.\nList&lt;string&gt; searchDirectoriesForDownload = [settings.LocalDir!];\n// unless extra search paths are added by the user.\nsearchDirectoriesForDownload.AddRange(settings.ExtraSearchDirectories ?? []);\nconfig.WithSearchDirectories(searchDirectoriesForDownload);\n\n// register you selecting policy.\nconfig.WithSelectingPolicy(new SelectingPolicyWithAutoDownload(settings));\n}\nreturn config;\n}\n}\n</code></pre> <p>Now your users are now able to enable this feature by calling <code>NativeLibraryConfig.All.WithAutoDownload</code>!</p>"},{"location":"Examples/BatchedExecutorFork/","title":"BatchedExecutor Fork - Generate Multiple Completions With Shared Memory","text":"<p>This example demonstrates using the <code>BatchedExecutor</code> to split one sequence into multiple sequences. See the source code here.</p> <p>Sequences share memory up to the point they were split, meaning no extra memory is consumed by creating a fork. Inference runs for all sequences simultaneously, this means that running two sequences does not take twice as much time as running one.</p> <p>An example output, starting with the prompt <code>Not many people know that</code>:</p> <pre><code>Not many people know that\n\u2514\u2500\u2500 , in the 17th century, a military band led by Captain Charles\n    \u251c\u2500\u2500  Bossler of Baden, Germany, composed and played a music suite titled\n    \u2502   \u251c\u2500\u2500  the \"Civil Psalm,\" in order to rally German Protestants during\n    \u2502   \u2502   \u251c\u2500\u2500  the Thirty Years' War.  This tune became popular among German soldiers,\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500  and its popularity continued long after the war\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500  and, eventually, reached France. The\n    \u2502   \u2502   \u2514\u2500\u2500  the Thirty Years' War.This music, with its clear call\n    \u2502   \u2502       \u251c\u2500\u2500  to arms and strong Christian themes, helped\n    \u2502   \u2502       \u2514\u2500\u2500  to arms and unwavering belief\n    \u2502   \u2514\u2500\u2500  \"Baden's First National Symphony,\" with lyrics by a young Wol\n    \u2502       \u251c\u2500\u2500 fgang Amadeus Mozart. The story of the composition's creation\n    \u2502       \u2502   \u251c\u2500\u2500  has long been forgotten. But the B\n    \u2502       \u2502   \u2514\u2500\u2500  was popularized by a novelty book\n    \u2502       \u2514\u2500\u2500 fgang Amadeus Mozart. It's said that this music brought\n    \u2502           \u251c\u2500\u2500  peace to Europe, at least for a\n    \u2502           \u2514\u2500\u2500  the troops together during difficult times. It\n    \u2514\u2500\u2500  Newdick played a mournful dirge to accompany the procession of\n        \u251c\u2500\u2500  the head of King Charles I. It is the scene that opens my latest book\n        \u2502   \u251c\u2500\u2500 , \"Death and Taxes.\" The book follows a British army captain named\n        \u2502   \u2502   \u251c\u2500\u2500  Marcus as he seeks revenge for his wife\n        \u2502   \u2502   \u2514\u2500\u2500  William Darnay who becomes involved in\n        \u2502   \u2514\u2500\u2500 , A King, A Pawn and a Prince. The murder of the king\n        \u2502       \u251c\u2500\u2500  and the civil war that followed are the\n        \u2502       \u2514\u2500\u2500  is a watershed moment in the political\n        \u2514\u2500\u2500  the coffin of William Shakespeare, as it was carried to its final resting place\n            \u251c\u2500\u2500 . That is the least that can be said for a man who is often regarded\n            \u2502   \u251c\u2500\u2500  as the greatest writer in the English language\n            \u2502   \u2514\u2500\u2500  as the greatest writer the English language has\n            \u2514\u2500\u2500  at Stratford-upon-Avon.  Shakespeare, of course\n                \u251c\u2500\u2500 , was a famous English poet and play\n                \u2514\u2500\u2500 , was one of the greatest playwright\n</code></pre> <p>Forked sequences can be used for many possible things. For example  - Evaluating the system prompt once and forking for each independent conversation.  - Saving a \"checkpoint\" in a conversation to return to later.  - Beam Search.  - Splitting a conversation, generating completions from several different \"agents\", and taking the best response.</p>"},{"location":"Examples/BatchedExecutorGuidance/","title":"BatchedExecutor Guidance - Classifier Free Guidance / Negative Prompting","text":"<p>This example demonstrates using <code>Classifier Free Guidance</code> (a.k.a. negative prompting) with a custom sampling pipeline. Negative prompting is a way of steering the model output away from certain topics. See the source code here.</p> <p>Two conversations are created. The <code>guided</code> conversation starts with the prompt that should be completed as shown as the output, for example <code>\"my favourite colour is\"</code>. The <code>guidance</code> conversation contains the negative prompt at the start, for example <code>\"I hate the colour red. My favourite colour is\"</code>. Note that this is a negative prompt, so therefore this guidance will make the model answer as if it likes the colour red.</p> <p>A custom sampler samples the <code>guidance</code> conversation and uses that output to influence the output of the <code>guided</code> conversation. Once a token is selected both conversations are continued with this token.</p>"},{"location":"Examples/BatchedExecutorRewind/","title":"BatchedExecutor - Rewind","text":"<p>This example demonstrates using the <code>BatchedExecutor</code> to split one sequence into multiple sequences. See the source code here.</p> <p>A single conversation is prompted and then continued for 24 tokens, after that it is re-wound by 12 tokens and continued from there. Rewinding simply sets the conversation back to an earlier state and requires no extra computation.</p>"},{"location":"Examples/ChatChineseGB2312/","title":"Chinese LLM - with GB2312 encoding","text":"<pre><code>using System.Text;\nusing LLama.Common;\n\nnamespace LLama.Examples.Examples;\n\n// This example shows how to deal with Chinese input with gb2312 encoding.\npublic class ChatChineseGB2312\n{\nprivate static string ConvertEncoding(string input, Encoding original, Encoding target)\n{\nbyte[] bytes = original.GetBytes(input);\nvar convertedBytes = Encoding.Convert(original, target, bytes);\nreturn target.GetString(convertedBytes);\n}\n\npublic static async Task Run()\n{\n// Register provider for GB2312 encoding\nEncoding.RegisterProvider(CodePagesEncodingProvider.Instance);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"This example shows how to use Chinese with gb2312 encoding, which is common in windows. It's recommended\" +\n\" to use https://huggingface.co/hfl/chinese-alpaca-2-7b-gguf/blob/main/ggml-model-q5_0.gguf, which has been verified by LLamaSharp developers.\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5,\nEncoding = Encoding.UTF8\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\nChatSession session;\nif (Directory.Exists(\"Assets/chat-with-kunkun-chinese\"))\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Loading session from disk.\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nsession = new ChatSession(executor);\nsession.LoadSession(\"Assets/chat-with-kunkun-chinese\");\n}\nelse\n{\nvar chatHistoryJson = File.ReadAllText(\"Assets/chat-with-kunkun-chinese.json\");\nChatHistory chatHistory = ChatHistory.FromJson(chatHistoryJson) ?? new ChatHistory();\n\nsession = new ChatSession(executor, chatHistory);\n}\n\nsession\n.WithHistoryTransform(new LLamaTransforms.DefaultHistoryTransform(\"\u7528\u6237\", \"\u5764\u5764\"));\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"\u7528\u6237\uff1a\" }\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started.\");\n\n// show the prompt\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(\"\u7528\u6237\uff1a\");\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\n// Convert the encoding from gb2312 to utf8 for the language model\n// and later saving to the history json file.\nuserInput = ConvertEncoding(userInput, Encoding.GetEncoding(\"gb2312\"), Encoding.UTF8);\n\nif (userInput == \"save\")\n{\nsession.SaveSession(\"Assets/chat-with-kunkun-chinese\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Session saved.\");\n}\nelse if (userInput == \"regenerate\")\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Regenerating last response ...\");\n\nawait foreach (\nvar text\nin session.RegenerateAssistantMessageAsync(\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\n\n// Convert the encoding from utf8 to gb2312 for the console output.\nConsole.Write(ConvertEncoding(text, Encoding.UTF8, Encoding.GetEncoding(\"gb2312\")));\n}\n}\nelse\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n</code></pre>"},{"location":"Examples/ChatSessionStripRoleName/","title":"ChatSession - stripping role names","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples;\n\n// When using chatsession, it's a common case that you want to strip the role names\n// rather than display them. This example shows how to use transforms to strip them.\npublic class ChatSessionStripRoleName\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\nvar chatHistoryJson = File.ReadAllText(\"Assets/chat-with-bob.json\");\nChatHistory chatHistory = ChatHistory.FromJson(chatHistoryJson) ?? new ChatHistory();\n\nChatSession session = new(executor, chatHistory);\nsession.WithOutputTransform(new LLamaTransforms.KeywordTextOutputStreamTransform(\nnew string[] { \"User:\", \"Assistant:\" },\nredundancyLength: 8));\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"User:\" }\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started.\");\n\n// show the prompt\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n</code></pre>"},{"location":"Examples/ChatSessionWithHistory/","title":"ChatSession - with history","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples;\n\n// This example shows how to save the state and history of chat session and load it again.\npublic class ChatSessionWithHistory\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\nChatSession session;\nif (Directory.Exists(\"Assets/chat-with-bob\"))\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Loading session from disk.\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nsession = new ChatSession(executor);\nsession.LoadSession(\"Assets/chat-with-bob\");\n}\nelse\n{\nvar chatHistoryJson = File.ReadAllText(\"Assets/chat-with-bob.json\");\nChatHistory chatHistory = ChatHistory.FromJson(chatHistoryJson) ?? new ChatHistory();\n\nsession = new ChatSession(executor, chatHistory);\n}\n\nsession.WithOutputTransform(new LLamaTransforms.KeywordTextOutputStreamTransform(\nnew string[] { \"User:\", \"Assistant:\" },\nredundancyLength: 8));\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"User:\" }\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started.\");\nConsole.WriteLine(\"Type 'exit' to end the chat session.\");\nConsole.WriteLine(\"Type 'save' to save the chat session to disk.\");\nConsole.WriteLine(\"Type 'load' to load the chat session from disk.\");\nConsole.WriteLine(\"Type 'regenerate' to regenerate the last response.\");\n\n// show the prompt\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\n// Save the chat state to disk\nif (userInput == \"save\")\n{\nsession.SaveSession(\"Assets/chat-with-bob\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Session saved.\");\n}\n// Load the chat state from disk\nelse if (userInput == \"load\")\n{\nsession.LoadSession(\"Assets/chat-with-bob\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Session loaded.\");\n}\nelse if (userInput == \"regenerate\")\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Regenerating last response ...\");\n\nawait foreach (\nvar text\nin session.RegenerateAssistantMessageAsync(\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n}\nelse\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n</code></pre>"},{"location":"Examples/ChatSessionWithRestart/","title":"ChatSession - restarting","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples;\n\n// This example shows how to restart the chat session\npublic class ChatSessionWithRestart\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\nvar chatHistoryJson = File.ReadAllText(\"Assets/chat-with-bob.json\");\nChatHistory chatHistory = ChatHistory.FromJson(chatHistoryJson) ?? new ChatHistory();\nChatSession prototypeSession = await ChatSession.InitializeSessionFromHistoryAsync(executor, chatHistory);\nprototypeSession.WithOutputTransform(new LLamaTransforms.KeywordTextOutputStreamTransform(\nnew string[] { \"User:\", \"Assistant:\" },\nredundancyLength: 8));\nvar resetState = prototypeSession.GetSessionState();\n\nChatSession session = new ChatSession(executor);\nsession.LoadSession(resetState);\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"User:\" }\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started. Starting point saved.\");\nConsole.WriteLine(\"Type 'exit' to end the chat session.\");\nConsole.WriteLine(\"Type 'save' to save chat session state in memory.\");\nConsole.WriteLine(\"Type 'reset' to reset the chat session to its saved state.\");\nConsole.WriteLine(\"Type 'answer for assistant' to add and process provided user and assistant messages.\");\n\n// show the prompt\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\n// Load the session state from the reset state\nif(userInput == \"reset\")\n{\nsession.LoadSession(resetState);\nConsole.WriteLine($\"Reset to history:\\n{session.HistoryTransform.HistoryToText(session.History)}\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Session reset.\");\n}\n// Assign new reset state.\nelse if (userInput == \"save\")\n{\nresetState = session.GetSessionState();\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Session saved.\");\n}\n// Provide user and override assistant answer with your own.\nelse if (userInput == \"answer for assistant\")\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Provide user input: \");\n\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInputOverride = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Provide assistant input: \");\n\nConsole.ForegroundColor = ConsoleColor.Green;\nstring assistantInputOverride = Console.ReadLine() ?? \"\";\n\nawait session.AddAndProcessUserMessage(userInputOverride);\nawait session.AddAndProcessAssistantMessage(assistantInputOverride);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"User and assistant messages processed. Provide next user message:\");\n}\nelse\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n</code></pre>"},{"location":"Examples/ChatSessionWithRoleName/","title":"ChatSession - Basic","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples;\n\n// The basic example for using ChatSession\npublic class ChatSessionWithRoleName\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\nvar chatHistoryJson = File.ReadAllText(\"Assets/chat-with-bob.json\");\nChatHistory chatHistory = ChatHistory.FromJson(chatHistoryJson) ?? new ChatHistory();\n\nChatSession session = new(executor, chatHistory);\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"User:\" }\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started.\");\n\n// show the prompt\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n</code></pre>"},{"location":"Examples/CodingAssistant/","title":"Coding assistant","text":"<pre><code>namespace LLama.Examples.Examples\n{\nusing LLama.Common;\nusing System;\n\n// This example shows how to apply code completion as a coding assistant\ninternal class CodingAssistant\n{\n// Source paper with example prompts:\n// https://doi.org/10.48550/arXiv.2308.12950\nconst string InstructionPrefix = \"[INST]\";\nconst string InstructionSuffix = \"[/INST]\";\nconst string SystemInstruction = \"You're an intelligent, concise coding assistant. \" +\n\"Wrap code in ``` for readability. Don't repeat yourself. \" +\n\"Use best practice and good coding standards.\";\n\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\nif (!modelPath.Contains(\"codellama\", StringComparison.InvariantCultureIgnoreCase))\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"WARNING: the model you selected is not a Code LLama model!\");\nConsole.WriteLine(\"For this example we specifically recommend 'codellama-7b-instruct.Q4_K_S.gguf'\");\nConsole.WriteLine(\"Press ENTER to continue...\");\nConsole.ReadLine();\n}\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 4096\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InstructExecutor(context, InstructionPrefix, InstructionSuffix, null);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the LLM will follow your instructions.\" +\n\"\\nIt's a 7B Code Llama, so it's trained for programming tasks like \\\"Write a C# function reading \" +\n\"a file name from a given URI\\\" or \\\"Write some programming interview questions\\\".\" +\n\"\\nWrite 'exit' to exit\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nvar inferenceParams = new InferenceParams()\n{\nTemperature = 0.8f,\nMaxTokens = -1,\n};\n\nstring instruction = $\"{SystemInstruction}\\n\\n\";\nawait Console.Out.WriteAsync(\"Instruction: \");\ninstruction += Console.ReadLine() ?? \"Ask me for instructions.\";\nwhile (instruction != \"exit\")\n{\n\nConsole.ForegroundColor = ConsoleColor.Green;\nawait foreach (var text in executor.InferAsync(instruction + Environment.NewLine, inferenceParams))\n{\nConsole.Write(text);\n}\nConsole.ForegroundColor = ConsoleColor.White;\n\nawait Console.Out.WriteAsync(\"Instruction: \");\ninstruction = Console.ReadLine() ?? \"Ask me for instructions.\";\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/GetEmbeddings/","title":"Get embeddings","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\n// This example shows how to get embeddings from a text prompt.\npublic class GetEmbeddings\n{\npublic static void Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nConsole.ForegroundColor = ConsoleColor.DarkGray;\nvar @params = new ModelParams(modelPath) { EmbeddingMode = true };\nusing var weights = LLamaWeights.LoadFromFile(@params);\nvar embedder = new LLamaEmbedder(weights, @params);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\n\"\"\"\nThis example displays embeddings from a text prompt.\nEmbeddings are numerical codes that represent information like words, images, or concepts.\nThese codes capture important relationships between those objects, like how similar words are in meaning or how close images are visually.\nThis allows machine learning models to efficiently understand and process complex data.\nEmbeddings of a text in LLM is sometimes useful, for example, to train other MLP models.\n\"\"\"); // NOTE: this description was AI generated\n\nwhile (true)\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(\"Please input your text: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nvar text = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\n\nfloat[] embeddings = embedder.GetEmbeddings(text).Result;\nConsole.WriteLine($\"Embeddings contain {embeddings.Length:N0} floating point values:\");\nConsole.ForegroundColor = ConsoleColor.DarkGray;\nConsole.WriteLine(string.Join(\", \", embeddings.Take(20)) + \", ...\");\nConsole.WriteLine();\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/GrammarJsonResponse/","title":"Grammar - json response","text":"<pre><code>using LLama.Common;\nusing LLama.Grammars;\n\nnamespace LLama.Examples.Examples\n{\n// This example shows how to get response in json format using grammar.\npublic class GrammarJsonResponse\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar gbnf = File.ReadAllText(\"Assets/json.gbnf\").Trim();\nvar grammar = Grammar.Parse(gbnf, \"root\");\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nvar ex = new StatelessExecutor(model, parameters);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the LLM will follow your instructions and always respond in a JSON format. For example, you can input \\\"Tell me the attributes of a good dish\\\"\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nusing var grammarInstance = grammar.CreateInstance();\nvar inferenceParams = new InferenceParams()\n{\nTemperature = 0.6f,\nAntiPrompts = new List&lt;string&gt; { \"Question:\", \"#\", \"Question: \", \".\\n\" },\nMaxTokens = 50,\nGrammar = grammarInstance\n};\n\nwhile (true)\n{\nConsole.Write(\"\\nQuestion: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nvar prompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(\"Answer: \");\nprompt = $\"Question: {prompt?.Trim()} Answer: \";\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/InstructModeExecute/","title":"Instruct executor - basic","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\n// This example shows how to use InstructExecutor to generate the response.\npublic class InstructModeExecute\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar prompt = File.ReadAllText(\"Assets/dan.txt\").Trim();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InstructExecutor(context);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the LLM will follow your instructions. For example, you can input \\\"Write a story about a fox who want to \" +\n\"make friend with human, no less than 200 words.\\\"\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nvar inferenceParams = new InferenceParams() { Temperature = 0.8f, MaxTokens = 600 };\n\nwhile (true)\n{\nawait foreach (var text in executor.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/InteractiveModeExecute/","title":"Interactive executor - basic","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\n// This is an example which shows how to chat with LLM with InteractiveExecutor.\npublic class InteractiveModeExecute\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar prompt = (await File.ReadAllTextAsync(\"Assets/chat-with-bob.txt\")).Trim();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar ex = new InteractiveExecutor(context);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the prompt is printed, the maximum tokens is set to 128 and the context size is 256. (an example for small scale usage)\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nConsole.Write(prompt);\n\nvar inferenceParams = new InferenceParams() { Temperature = 0.6f, AntiPrompts = new List&lt;string&gt; { \"User:\" }, MaxTokens = 128 };\n\nwhile (true)\n{\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/KernelMemory/","title":"Kernel memory integration - basic","text":"<pre><code>using LLamaSharp.KernelMemory;\nusing Microsoft.KernelMemory;\nusing Microsoft.KernelMemory.Configuration;\nusing System.Diagnostics;\n\nnamespace LLama.Examples.Examples\n{\n// This example is from Microsoft's official kernel memory \"custom prompts\" example:\n// https://github.com/microsoft/kernel-memory/blob/6d516d70a23d50c6cb982e822e6a3a9b2e899cfa/examples/101-dotnet-custom-Prompts/Program.cs#L1-L86\n\n// Microsoft.KernelMemory has more features than Microsoft.SemanticKernel.\n// See https://microsoft.github.io/kernel-memory/ for details.\n\npublic class KernelMemory\n{\npublic static async Task Run()\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\n\"\"\"\n\nThis program uses the Microsoft.KernelMemory package to ingest documents\nand answer questions about them in an interactive chat prompt.\n\n\"\"\");\n\n// Setup the kernel memory with the LLM model\nstring modelPath = UserSettings.GetModelPath();\nIKernelMemory memory = CreateMemory(modelPath);\n\n// Ingest documents (format is automatically detected from the filename)\nstring[] filesToIngest = [\nPath.GetFullPath(@\"./Assets/sample-SK-Readme.pdf\"),\nPath.GetFullPath(@\"./Assets/sample-KM-Readme.pdf\"),\n];\n\nfor (int i = 0; i &lt; filesToIngest.Length; i++)\n{\nstring path = filesToIngest[i];\nStopwatch sw = Stopwatch.StartNew();\nConsole.ForegroundColor = ConsoleColor.Blue;\nConsole.WriteLine($\"Importing {i + 1} of {filesToIngest.Length}: {path}\");\nawait memory.ImportDocumentAsync(path, steps: Constants.PipelineWithoutSummary);\nConsole.WriteLine($\"Completed in {sw.Elapsed}\\n\");\n}\n\n// Ask a predefined question\nConsole.ForegroundColor = ConsoleColor.Green;\nstring question1 = \"What formats does KM support\";\nConsole.WriteLine($\"Question: {question1}\");\nawait AnswerQuestion(memory, question1);\n\n// Let the user ask additional questions\nwhile (true)\n{\nConsole.ForegroundColor = ConsoleColor.Green;\nConsole.Write(\"Question: \");\nstring question = Console.ReadLine()!;\nif (string.IsNullOrEmpty(question))\nreturn;\n\nawait AnswerQuestion(memory, question);\n}\n}\n\nprivate static IKernelMemory CreateMemory(string modelPath)\n{\nCommon.InferenceParams infParams = new() { AntiPrompts = [\"\\n\\n\"] };\n\nLLamaSharpConfig lsConfig = new(modelPath) { DefaultInferenceParams = infParams };\n\nSearchClientConfig searchClientConfig = new()\n{\nMaxMatchesCount = 1,\nAnswerTokens = 100,\n};\n\nTextPartitioningOptions parseOptions = new()\n{\nMaxTokensPerParagraph = 300,\nMaxTokensPerLine = 100,\nOverlappingTokens = 30\n};\n\nreturn new KernelMemoryBuilder()\n.WithLLamaSharpDefaults(lsConfig)\n.WithSearchClientConfig(searchClientConfig)\n.With(parseOptions)\n.Build();\n}\n\nprivate static async Task AnswerQuestion(IKernelMemory memory, string question)\n{\nStopwatch sw = Stopwatch.StartNew();\nConsole.ForegroundColor = ConsoleColor.DarkGray;\nConsole.WriteLine($\"Generating answer...\");\n\nMemoryAnswer answer = await memory.AskAsync(question);\nConsole.WriteLine($\"Answer generated in {sw.Elapsed}\");\n\nConsole.ForegroundColor = ConsoleColor.Gray;\nConsole.WriteLine($\"Answer: {answer.Result}\");\nforeach (var source in answer.RelevantSources)\n{\nConsole.WriteLine($\"Source: {source.SourceName}\");\n}\nConsole.WriteLine();\n}\n}\n}\n</code></pre>"},{"location":"Examples/KernelMemorySaveAndLoad/","title":"Kernel-memory - save &amp; load","text":"<pre><code>using LLamaSharp.KernelMemory;\nusing Microsoft.KernelMemory;\nusing Microsoft.KernelMemory.Configuration;\nusing Microsoft.KernelMemory.ContentStorage.DevTools;\nusing Microsoft.KernelMemory.FileSystem.DevTools;\nusing Microsoft.KernelMemory.MemoryStorage.DevTools;\nusing System.Diagnostics;\n\nnamespace LLama.Examples.Examples;\n\n// This example shows how to use kernel-memory integration with pre-saved embeddings.\npublic class KernelMemorySaveAndLoad\n{\nstatic string StorageFolder =&gt; Path.GetFullPath($\"./storage-{nameof(KernelMemorySaveAndLoad)}\");\nstatic bool StorageExists =&gt; Directory.Exists(StorageFolder) &amp;&amp; Directory.GetDirectories(StorageFolder).Length &gt; 0;\n\npublic static async Task Run()\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\n\"\"\"\n\nThis program uses the Microsoft.KernelMemory package to ingest documents\nand store the embeddings as local files so they can be quickly recalled\nwhen this application is launched again. \"\"\");\n\nstring modelPath = UserSettings.GetModelPath();\nIKernelMemory memory = CreateMemoryWithLocalStorage(modelPath);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nif (StorageExists)\n{\nConsole.WriteLine(\n\"\"\"\n\nKernel memory files have been located!\nInformation about previously analyzed documents has been loaded.\n\n\"\"\");\n}\nelse\n{\nConsole.WriteLine(\n$\"\"\"\n\nExisting kernel memory was not found.\nDocuments will be analyzed (slow) and information saved to disk.\nAnalysis will not be required the next time this program is run.\nPress ENTER to proceed...\n\n\"\"\");\nConsole.ReadLine();\nawait IngestDocuments(memory);\n}\n\nawait AskSingleQuestion(memory, \"What formats does KM support?\");\nawait StartUserChatSession(memory);\n}\n\nprivate static IKernelMemory CreateMemoryWithLocalStorage(string modelPath)\n{\nCommon.InferenceParams infParams = new() { AntiPrompts = [\"\\n\\n\"] };\n\nLLamaSharpConfig lsConfig = new(modelPath) { DefaultInferenceParams = infParams };\n\nSearchClientConfig searchClientConfig = new()\n{\nMaxMatchesCount = 1,\nAnswerTokens = 100,\n};\n\nTextPartitioningOptions parseOptions = new()\n{\nMaxTokensPerParagraph = 300,\nMaxTokensPerLine = 100,\nOverlappingTokens = 30\n};\n\nSimpleFileStorageConfig storageConfig = new()\n{\nDirectory = StorageFolder,\nStorageType = FileSystemTypes.Disk,\n};\n\nSimpleVectorDbConfig vectorDbConfig = new()\n{\nDirectory = StorageFolder,\nStorageType = FileSystemTypes.Disk,\n};\n\nConsole.ForegroundColor = ConsoleColor.Blue;\nConsole.WriteLine($\"Kernel memory folder: {StorageFolder}\");\n\nConsole.ForegroundColor = ConsoleColor.DarkGray;\nreturn new KernelMemoryBuilder()\n.WithSimpleFileStorage(storageConfig)\n.WithSimpleVectorDb(vectorDbConfig)\n.WithLLamaSharpDefaults(lsConfig)\n.WithSearchClientConfig(searchClientConfig)\n.With(parseOptions)\n.Build();\n}\n\nprivate static async Task AskSingleQuestion(IKernelMemory memory, string question)\n{\nConsole.ForegroundColor = ConsoleColor.Green;\nConsole.WriteLine($\"Question: {question}\");\nawait ShowAnswer(memory, question);\n}\n\nprivate static async Task StartUserChatSession(IKernelMemory memory)\n{\nwhile (true)\n{\nConsole.ForegroundColor = ConsoleColor.Green;\nConsole.Write(\"Question: \");\nstring question = Console.ReadLine()!;\nif (string.IsNullOrEmpty(question))\nreturn;\n\nawait ShowAnswer(memory, question);\n}\n}\n\nprivate static async Task IngestDocuments(IKernelMemory memory)\n{\nstring[] filesToIngest = [\nPath.GetFullPath(@\"./Assets/sample-SK-Readme.pdf\"),\nPath.GetFullPath(@\"./Assets/sample-KM-Readme.pdf\"),\n];\n\nfor (int i = 0; i &lt; filesToIngest.Length; i++)\n{\nstring path = filesToIngest[i];\nStopwatch sw = Stopwatch.StartNew();\nConsole.ForegroundColor = ConsoleColor.Blue;\nConsole.WriteLine($\"Importing {i + 1} of {filesToIngest.Length}: {path}\");\nawait memory.ImportDocumentAsync(path, steps: Constants.PipelineWithoutSummary);\nConsole.WriteLine($\"Completed in {sw.Elapsed}\\n\");\n}\n}\n\nprivate static async Task ShowAnswer(IKernelMemory memory, string question)\n{\nStopwatch sw = Stopwatch.StartNew();\nConsole.ForegroundColor = ConsoleColor.DarkGray;\nConsole.WriteLine($\"Generating answer...\");\n\nMemoryAnswer answer = await memory.AskAsync(question);\nConsole.WriteLine($\"Answer generated in {sw.Elapsed}\");\n\nConsole.ForegroundColor = ConsoleColor.Gray;\nConsole.WriteLine($\"Answer: {answer.Result}\");\nforeach (var source in answer.RelevantSources)\n{\nConsole.WriteLine($\"Source: {source.SourceName}\");\n}\nConsole.WriteLine();\n}\n}\n</code></pre>"},{"location":"Examples/LLavaInteractiveModeExecute/","title":"LLaVA - basic","text":"<pre><code>using System.Text.RegularExpressions;\nusing LLama.Common;\nusing Spectre.Console;\nusing LLama.Native;\n\nnamespace LLama.Examples.Examples\n{\n// This example shows how to chat with LLaVA model with both image and text as input.\n// It uses the interactive executor to inference.\npublic class LlavaInteractiveModeExecute\n{\npublic static async Task Run()\n{\nstring multiModalProj = UserSettings.GetMMProjPath();\nstring modelPath = UserSettings.GetModelPath();\nstring modelImage = UserSettings.GetImagePath();\nconst int maxTokens = 1024;\n\nvar prompt = $\"{{{modelImage}}}\\nUSER:\\nProvide a full description of the image.\\nASSISTANT:\\n\";\n\nvar parameters = new ModelParams(modelPath);\n\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\n\n// Llava Init\nusing var clipModel = LLavaWeights.LoadFromFile(multiModalProj);\n\nvar ex = new InteractiveExecutor(context, clipModel );\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the prompt is printed, the maximum tokens is set to {0} and the context size is {1}.\", maxTokens, parameters.ContextSize );\nConsole.WriteLine(\"To send an image, enter its filename in curly braces, like this {c:/image.jpg}.\");  var inferenceParams = new InferenceParams() { Temperature = 0.1f, AntiPrompts = new List&lt;string&gt; { \"\\nUSER:\" }, MaxTokens = maxTokens };\n\ndo\n{\n\n// Evaluate if we have images\n//\nvar imageMatches = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Value);\nvar imageCount = imageMatches.Count();\nvar hasImages = imageCount &gt; 0;\n\nif (hasImages)\n{\nvar imagePathsWithCurlyBraces = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Value);\nvar imagePaths = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Groups[1].Value).ToList();\n\nList&lt;byte[]&gt; imageBytes;\ntry\n{\nimageBytes = imagePaths.Select(File.ReadAllBytes).ToList();\n}\ncatch (IOException exception)\n{\nConsole.ForegroundColor = ConsoleColor.Red;\nConsole.Write(\n$\"Could not load your {(imageCount == 1 ? \"image\" : \"images\")}:\");\nConsole.Write($\"{exception.Message}\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Please try again.\");\nbreak;\n}\n\n// Each prompt with images we clear cache\n// When the prompt contains images we clear KV_CACHE to restart conversation\n// See:\n// https://github.com/ggerganov/llama.cpp/discussions/3620\nex.Context.NativeHandle.KvCacheRemove( LLamaSeqId.Zero, -1, -1 );\n\nint index = 0;\nforeach (var path in imagePathsWithCurlyBraces)\n{\n// First image replace to tag &lt;image, the rest of the images delete the tag\nprompt = prompt.Replace(path, index++ == 0 ? \"&lt;image&gt;\" : \"\");\n}\n\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine($\"Here are the images, that are sent to the chat model in addition to your message.\");\nConsole.WriteLine();\n\nforeach (var consoleImage in imageBytes?.Select(bytes =&gt; new CanvasImage(bytes)))\n{\nconsoleImage.MaxWidth = 50;\nAnsiConsole.Write(consoleImage);\n}\n\nConsole.WriteLine();\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine($\"The images were scaled down for the console only, the model gets full versions.\");\nConsole.WriteLine($\"Write /exit or press Ctrl+c to return to main menu.\");\nConsole.WriteLine();\n\n\n// Initialize Images in executor\n//\nforeach (var image in imagePaths)\n{\nex.Images.Add(await File.ReadAllBytesAsync(image));\n}\n}\n\nConsole.ForegroundColor = Color.White;\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\nConsole.Write(\" \");\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.WriteLine();\n\n// let the user finish with exit\n//\nif (prompt != null &amp;&amp; prompt.Equals(\"/exit\", StringComparison.OrdinalIgnoreCase))\nbreak;\n\n}\nwhile(true);\n}\n}\n}\n</code></pre>"},{"location":"Examples/LoadAndSaveSession/","title":"ChatSession - load &amp; save","text":"<p>\u26a0\ufe0fWarning: this example has been outdated for the latest version of LLamaSharp, please refer to this example to see how to save and load state for <code>ChatSession</code>. If you are using some old versions of LLamaSharp, this example may help you.</p> <pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\npublic class SaveAndLoadSession\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar prompt = (await File.ReadAllTextAsync(\"Assets/chat-with-bob.txt\")).Trim();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar ex = new InteractiveExecutor(context);\n\nvar session = new ChatSession(ex);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started. In this example, the prompt is printed for better visual result. Input \\\"save\\\" to save and reload the session.\");\nConsole.ForegroundColor = ConsoleColor.White;\n\n// show the prompt\nConsole.Write(prompt);\nwhile (true)\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, prompt),\nnew InferenceParams()\n{\nTemperature = 0.6f,\nAntiPrompts = new List&lt;string&gt; { \"User:\" }\n}))\n{\nConsole.Write(text);\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\nif (prompt == \"save\")\n{\nConsole.Write(\"Preparing to save the state, please input the path you want to save it: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nvar statePath = Console.ReadLine();\nsession.SaveSession(statePath);\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Saved session!\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nex.Context.Dispose();\nex = new(new LLamaContext(model, parameters));\nsession = new ChatSession(ex);\nsession.LoadSession(statePath);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Loaded session!\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nConsole.Write(\"Now you can continue your session: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/LoadAndSaveState/","title":"Executor - save/load state","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\n// This example shows how to save/load state of the executor.\npublic class LoadAndSaveState\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar prompt = (await File.ReadAllTextAsync(\"Assets/chat-with-bob.txt\")).Trim();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar ex = new InteractiveExecutor(context);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the prompt is printed, \" +\n\"the maximum tokens is set to 64 and the context size is 256. (an example for small scale usage)\");\n\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(prompt);\n\nvar inferenceParams = new InferenceParams() { Temperature = 0.6f, AntiPrompts = new List&lt;string&gt; { \"User:\" } };\n\nwhile (true)\n{\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\n\nprompt = Console.ReadLine();\nif (prompt == \"save\")\n{\nConsole.Write(\"Your path to save model state: \");\nvar modelStatePath = Console.ReadLine();\nex.Context.SaveState(modelStatePath);\n\nConsole.Write(\"Your path to save executor state: \");\nvar executorStatePath = Console.ReadLine();\nawait ex.SaveState(executorStatePath);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"All states saved!\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nvar ctx = ex.Context;\nctx.LoadState(modelStatePath);\nex = new InteractiveExecutor(ctx);\nawait ex.LoadState(executorStatePath);\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Loaded state!\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nConsole.Write(\"Now you can continue your session: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/QuantizeModel/","title":"Quantization","text":"<pre><code>namespace LLama.Examples.Examples\n{\npublic class QuantizeModel\n{\npublic static void Run()\n{\nstring inputPath = UserSettings.GetModelPath();\n\nConsole.Write(\"Please input your output model path: \");\nvar outputPath = Console.ReadLine();\n\nConsole.Write(\"Please input the quantize type (one of q4_0, q4_1, q5_0, q5_1, q8_0): \");\nvar quantizeType = Console.ReadLine();\n\nif (LLamaQuantizer.Quantize(inputPath, outputPath, quantizeType))\n{\nConsole.WriteLine(\"Quantization succeeded!\");\n}\nelse\n{\nConsole.WriteLine(\"Quantization failed!\");\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/SemanticKernelChat/","title":"Semantic-kernel - chat","text":"<pre><code>using LLama.Common;\nusing LLamaSharp.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel.ChatCompletion;\n\nnamespace LLama.Examples.Examples\n{\npublic class SemanticKernelChat\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"This example is from: \\n\" +\n\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/KernelSyntaxExamples/Example17_ChatGPT.cs\");\n\n// Load weights into memory\nvar parameters = new ModelParams(modelPath);\nusing var model = LLamaWeights.LoadFromFile(parameters);\nvar ex = new StatelessExecutor(model, parameters);\n\nvar chatGPT = new LLamaSharpChatCompletion(ex);\n\nvar chatHistory = chatGPT.CreateNewChat(\"This is a conversation between the \" +\n\"assistant and the user. \\n\\n You are a librarian, expert about books. \");\n\nConsole.WriteLine(\"Chat content:\");\nConsole.WriteLine(\"------------------------\");\n\nchatHistory.AddUserMessage(\"Hi, I'm looking for book suggestions\");\nawait MessageOutputAsync(chatHistory);\n\n// First bot assistant message\nvar reply = await chatGPT.GetChatMessageContentAsync(chatHistory);\nchatHistory.AddAssistantMessage(reply.Content);\nawait MessageOutputAsync(chatHistory);\n\n// Second user message\nchatHistory.AddUserMessage(\"I love history and philosophy, I'd like to learn \" +\n\"something new about Greece, any suggestion\");\nawait MessageOutputAsync(chatHistory);\n\n// Second bot assistant message\nreply = await chatGPT.GetChatMessageContentAsync(chatHistory);\nchatHistory.AddAssistantMessage(reply.Content);\nawait MessageOutputAsync(chatHistory);\n}\n\n/// &lt;summary&gt;\n/// Outputs the last message of the chat history\n/// &lt;/summary&gt;\nprivate static Task MessageOutputAsync(Microsoft.SemanticKernel.ChatCompletion.ChatHistory chatHistory)\n{\nvar message = chatHistory.Last();\n\nConsole.WriteLine($\"{message.Role}: {message.Content}\");\nConsole.WriteLine(\"------------------------\");\n\nreturn Task.CompletedTask;\n}\n}\n}\n</code></pre>"},{"location":"Examples/SemanticKernelMemory/","title":"Semantic-kernel - with kernel-memory","text":"<p>Semantic Memory allows to store your data like traditional DBs, adding the ability to query it using natural language.</p> <pre><code>using LLama.Common;\nusing Microsoft.SemanticKernel.Memory;\nusing LLamaSharp.SemanticKernel.TextEmbedding;\n\nnamespace LLama.Examples.Examples\n{\npublic class SemanticKernelMemory\n{\nprivate const string MemoryCollectionName = \"SKGitHub\";\n\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nConsole.WriteLine(\"This example is from: \\n\" +\n\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/KernelSyntaxExamples/Example14_SemanticMemory.cs\");\n\nvar seed = 1337u;\n// Load weights into memory\nvar parameters = new ModelParams(modelPath)\n{\nSeed = seed,\nEmbeddingMode = true\n};\n\nusing var model = LLamaWeights.LoadFromFile(parameters);\nvar embedding = new LLamaEmbedder(model, parameters);\n\nConsole.WriteLine(\"====================================================\");\nConsole.WriteLine(\"======== Semantic Memory (volatile, in RAM) ========\");\nConsole.WriteLine(\"====================================================\");\n\n/* You can build your own semantic memory combining an Embedding Generator\n             * with a Memory storage that supports search by similarity (ie semantic search).\n             *\n             * In this example we use a volatile memory, a local simulation of a vector DB.\n             *\n             * You can replace VolatileMemoryStore with Qdrant (see QdrantMemoryStore connector)\n             * or implement your connectors for Pinecone, Vespa, Postgres + pgvector, SQLite VSS, etc.\n             */\n\nvar memory = new MemoryBuilder()\n.WithTextEmbeddingGeneration(new LLamaSharpEmbeddingGeneration(embedding))\n.WithMemoryStore(new VolatileMemoryStore())\n.Build();\n\nawait RunExampleAsync(memory);\n}\n\nprivate static async Task RunExampleAsync(ISemanticTextMemory memory)\n{\nawait StoreMemoryAsync(memory);\n\nawait SearchMemoryAsync(memory, \"How do I get started?\");\n\n/*\n            Output:\n\n            Query: How do I get started?\n\n            Result 1:\n              URL:     : https://github.com/microsoft/semantic-kernel/blob/main/README.md\n              Title    : README: Installation, getting started, and how to contribute\n\n            Result 2:\n              URL:     : https://github.com/microsoft/semantic-kernel/blob/main/samples/dotnet-jupyter-notebooks/00-getting-started.ipynb\n              Title    : Jupyter notebook describing how to get started with the Semantic Kernel\n\n            */\n\nawait SearchMemoryAsync(memory, \"Can I build a chat with SK?\");\n\n/*\n            Output:\n\n            Query: Can I build a chat with SK?\n\n            Result 1:\n              URL:     : https://github.com/microsoft/semantic-kernel/tree/main/samples/skills/ChatSkill/ChatGPT\n              Title    : Sample demonstrating how to create a chat skill interfacing with ChatGPT\n\n            Result 2:\n              URL:     : https://github.com/microsoft/semantic-kernel/blob/main/samples/apps/chat-summary-webapp-react/README.md\n              Title    : README: README associated with a sample chat summary react-based webapp\n\n            */\n\nawait SearchMemoryAsync(memory, \"Jupyter notebook\");\n\nawait SearchMemoryAsync(memory, \"README: README associated with a sample chat summary react-based webapp\");\n\nawait SearchMemoryAsync(memory, \"Jupyter notebook describing how to pass prompts from a file to a semantic skill or function\");\n}\n\nprivate static async Task SearchMemoryAsync(ISemanticTextMemory memory, string query)\n{\nConsole.WriteLine(\"\\nQuery: \" + query + \"\\n\");\n\nvar memories = memory.SearchAsync(MemoryCollectionName, query, limit: 10, minRelevanceScore: 0.5);\n\nint i = 0;\nawait foreach (MemoryQueryResult result in memories)\n{\nConsole.WriteLine($\"Result {++i}:\");\nConsole.WriteLine(\"  URL:     : \" + result.Metadata.Id);\nConsole.WriteLine(\"  Title    : \" + result.Metadata.Description);\nConsole.WriteLine(\"  Relevance: \" + result.Relevance);\nConsole.WriteLine();\n}\n\nConsole.WriteLine(\"----------------------\");\n}\n\nprivate static async Task StoreMemoryAsync(ISemanticTextMemory memory)\n{\n/* Store some data in the semantic memory.\n             *\n             * When using Azure Cognitive Search the data is automatically indexed on write.\n             *\n             * When using the combination of VolatileStore and Embedding generation, SK takes\n             * care of creating and storing the index\n             */\n\nConsole.WriteLine(\"\\nAdding some GitHub file URLs and their descriptions to the semantic memory.\");\nvar githubFiles = SampleData();\nvar i = 0;\nforeach (var entry in githubFiles)\n{\nvar result = await memory.SaveReferenceAsync(\ncollection: MemoryCollectionName,\nexternalSourceName: \"GitHub\",\nexternalId: entry.Key,\ndescription: entry.Value,\ntext: entry.Value);\n\nConsole.WriteLine($\"#{++i} saved.\");\nConsole.WriteLine(result);\n}\n\nConsole.WriteLine(\"\\n----------------------\");\n}\n\nprivate static Dictionary&lt;string, string&gt; SampleData()\n{\nreturn new Dictionary&lt;string, string&gt;\n{\n[\"https://github.com/microsoft/semantic-kernel/blob/main/README.md\"]\n= \"README: Installation, getting started, and how to contribute\",\n[\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/notebooks/02-running-prompts-from-file.ipynb\"]\n= \"Jupyter notebook describing how to pass prompts from a file to a semantic skill or function\",\n[\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/notebooks//00-getting-started.ipynb\"]\n= \"Jupyter notebook describing how to get started with the Semantic Kernel\",\n[\"https://github.com/microsoft/semantic-kernel/tree/main/samples/skills/ChatSkill/ChatGPT\"]\n= \"Sample demonstrating how to create a chat skill interfacing with ChatGPT\",\n[\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/SemanticKernel/Memory/VolatileMemoryStore.cs\"]\n= \"C# class that defines a volatile embedding store\",\n[\"https://github.com/microsoft/semantic-kernel/blob/main/samples/dotnet/KernelHttpServer/README.md\"]\n= \"README: How to set up a Semantic Kernel Service API using Azure Function Runtime v4\",\n[\"https://github.com/microsoft/semantic-kernel/blob/main/samples/apps/chat-summary-webapp-react/README.md\"]\n= \"README: README associated with a sample chat summary react-based webapp\",\n};\n}\n}\n}\n</code></pre>"},{"location":"Examples/SemanticKernelPrompt/","title":"Semantic-kernel - basic","text":"<pre><code>using LLama.Common;\nusing LLamaSharp.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel;\nusing LLamaSharp.SemanticKernel.TextCompletion;\nusing Microsoft.SemanticKernel.TextGeneration;\nusing Microsoft.Extensions.DependencyInjection;\n\nnamespace LLama.Examples.Examples\n{\n// The basic example for using the semantic-kernel integration\npublic class SemanticKernelPrompt\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"This example is from: \" +\n\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/README.md\");\n\n// Load weights into memory\nvar parameters = new ModelParams(modelPath);\nusing var model = LLamaWeights.LoadFromFile(parameters);\nvar ex = new StatelessExecutor(model, parameters);\n\nvar builder = Kernel.CreateBuilder();\nbuilder.Services.AddKeyedSingleton&lt;ITextGenerationService&gt;(\"local-llama\", new LLamaSharpTextCompletion(ex));\n\nvar kernel = builder.Build();\n\nvar prompt = @\"{{$input}}\n\nOne line TLDR with the fewest words.\";\n\nChatRequestSettings settings = new() { MaxTokens = 100 };\nvar summarize = kernel.CreateFunctionFromPrompt(prompt, settings);\n\nstring text1 = @\"\n1st Law of Thermodynamics - Energy cannot be created or destroyed.\n2nd Law of Thermodynamics - For a spontaneous process, the entropy of the universe increases.\n3rd Law of Thermodynamics - A perfect crystal at zero Kelvin has zero entropy.\";\n\nstring text2 = @\"\n1. An object at rest remains at rest, and an object in motion remains in motion at constant speed and in a straight line unless acted on by an unbalanced force.\n2. The acceleration of an object depends on the mass of the object and the amount of force applied.\n3. Whenever one object exerts a force on another object, the second object exerts an equal and opposite on the first.\";\n\nConsole.WriteLine((await kernel.InvokeAsync(summarize, new() { [\"input\"] = text1 })).GetValue&lt;string&gt;());\n\nConsole.WriteLine((await kernel.InvokeAsync(summarize, new() { [\"input\"] = text2 })).GetValue&lt;string&gt;());\n}\n}\n}\n</code></pre>"},{"location":"Examples/StatelessModeExecute/","title":"Stateless executor","text":"<pre><code>using LLama.Common;\nusing LLama.Examples.Extensions;\n\nnamespace LLama.Examples.Examples\n{\n// Basic usage of the stateless executor.\npublic class StatelessModeExecute\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nvar ex = new StatelessExecutor(model, parameters);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the inference is an one-time job. That says, the previous input and response has \" +\n\"no impact on the current response. Now you can ask it questions. Note that in this example, no prompt was set for LLM and the maximum response tokens is 50. \" +\n\"It may not perform well because of lack of prompt. This is also an example that could indicate the importance of prompt in LLM. To improve it, you can add \" +\n\"a prompt for it yourself!\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nvar inferenceParams = new InferenceParams() { Temperature = 0.6f, AntiPrompts = new List&lt;string&gt; { \"Question:\", \"#\", \"Question: \", \".\\n\" }, MaxTokens = 50 };\n\nwhile (true)\n{\nConsole.Write(\"\\nQuestion: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nvar prompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(\"Answer: \");\nprompt = $\"Question: {prompt?.Trim()} Answer: \";\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams).Spinner())\n{\nConsole.Write(text);\n}\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/TalkToYourself/","title":"Talk to yourself","text":"<pre><code>using System.Text;\nusing LLama.Abstractions;\nusing LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\n// Let two bots chat with each other.\npublic class TalkToYourself\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\n// Load weights into memory\nvar @params = new ModelParams(modelPath);\nusing var weights = LLamaWeights.LoadFromFile(@params);\n\n// Create 2 contexts sharing the same weights\nusing var aliceCtx = weights.CreateContext(@params);\nvar alice = new InteractiveExecutor(aliceCtx);\nusing var bobCtx = weights.CreateContext(@params);\nvar bob = new InteractiveExecutor(bobCtx);\n\n// Initial alice prompt\nvar alicePrompt = \"Transcript of a dialog, where the Alice interacts a person named Bob. Alice is friendly, kind, honest and good at writing.\\nAlice: Hello\";\nvar aliceResponse = await Prompt(alice, ConsoleColor.Green, alicePrompt, false, false);\n\n// Initial bob prompt\nvar bobPrompt = $\"Transcript of a dialog, where the Bob interacts a person named Alice. Bob is smart, intellectual and good at writing.\\nAlice: Hello{aliceResponse}\";\nvar bobResponse = await Prompt(bob, ConsoleColor.Red, bobPrompt, true, true);\n\n// swap back and forth from Alice to Bob\nwhile (true)\n{\naliceResponse = await Prompt(alice, ConsoleColor.Green, bobResponse, false, true);\nbobResponse = await Prompt(bob, ConsoleColor.Red, aliceResponse, false, true);\n\nif (Console.KeyAvailable)\nbreak;\n}\n}\n\nprivate static async Task&lt;string&gt; Prompt(ILLamaExecutor executor, ConsoleColor color, string prompt, bool showPrompt, bool showResponse)\n{\nvar inferenceParams = new InferenceParams\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"Alice:\", \"Bob:\", \"User:\" },\nMaxTokens = 128,\nMirostat = MirostatType.Mirostat2,\nMirostatTau = 10,\n};\n\nConsole.ForegroundColor = ConsoleColor.White;\nif (showPrompt)\nConsole.Write(prompt);\n\nConsole.ForegroundColor = color;\nvar builder = new StringBuilder();\nawait foreach (var text in executor.InferAsync(prompt, inferenceParams))\n{\nbuilder.Append(text);\nif (showResponse)\nConsole.Write(text);\n}\n\nreturn builder.ToString();\n}\n}\n}\n</code></pre>"},{"location":"Integrations/bot-sharp/","title":"BotSharp integration","text":"<p>The document is under work, please have a wait. Thank you for your support! :)</p>"},{"location":"Integrations/kernel-memory/","title":"LLamaSharp.kernel-memory","text":"<p>The document is under work, please have a wait. Thank you for your support! :)</p>"},{"location":"Integrations/langchain/","title":"Langchain integration","text":"<p>The document is under work, please have a wait. Thank you for your support! :)</p>"},{"location":"Integrations/semantic-kernel/","title":"LLamaSharp.SemanticKernel","text":"<p>LLamaSharp.SemanticKernel are connections for SemanticKernel: an SDK for integrating various LLM interfaces into a single implementation. With this, you can add local LLaMa queries as another connection point with your existing connections.</p> <p>For reference on how to implement it, view the following examples: </p> <ul> <li>SemanticKernelChat</li> <li>SemanticKernelPrompt</li> <li>SemanticKernelMemory</li> </ul>"},{"location":"Integrations/semantic-kernel/#itextcompletion","title":"ITextCompletion","text":"<pre><code>using var model = LLamaWeights.LoadFromFile(parameters);\n// LLamaSharpTextCompletion can accept ILLamaExecutor. \nvar ex = new StatelessExecutor(model, parameters);\nvar builder = new KernelBuilder();\nbuilder.WithAIService&lt;ITextCompletion&gt;(\"local-llama\", new LLamaSharpTextCompletion(ex), true);\n</code></pre>"},{"location":"Integrations/semantic-kernel/#ichatcompletion","title":"IChatCompletion","text":"<pre><code>using var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\n// LLamaSharpChatCompletion requires InteractiveExecutor, as it's the best fit for the given command.\nvar ex = new InteractiveExecutor(context);\nvar chatGPT = new LLamaSharpChatCompletion(ex);\n</code></pre>"},{"location":"Integrations/semantic-kernel/#itextembeddinggeneration","title":"ITextEmbeddingGeneration","text":"<pre><code>using var model = LLamaWeights.LoadFromFile(parameters);\nvar embedding = new LLamaEmbedder(model, parameters);\nvar kernelWithCustomDb = Kernel.Builder\n.WithLoggerFactory(ConsoleLogger.LoggerFactory)\n.WithAIService&lt;ITextEmbeddingGeneration&gt;(\"local-llama-embed\", new LLamaSharpEmbeddingGeneration(embedding), true)\n.WithMemoryStorage(new VolatileMemoryStore())\n.Build();\n</code></pre>"},{"location":"More/log/","title":"The Logger in LLamaSharp","text":"<p>LLamaSharp supports customized logger because it could be used in many kinds of applications, like Winform/WPF, WebAPI and Blazor, so that the preference of logger varies.</p>"},{"location":"More/log/#define-customized-logger","title":"Define customized logger","text":"<p>What you need to do is to implement the <code>ILogger</code> interface. </p> <pre><code>public interface ILLamaLogger\n{\npublic enum LogLevel\n{\nInfo,\nDebug,\nWarning,\nError\n}\nvoid Log(string source, string message, LogLevel level);\n}\n</code></pre> <p>The <code>source</code> specifies where the log message is from, which could be a function, a class, etc..</p> <p>The <code>message</code> is the log message itself.</p> <p>The <code>level</code> is the level of the information in the log. As shown above, there're four levels, which are <code>info</code>, <code>debug</code>, <code>warning</code> and <code>error</code> respectively.</p> <p>The following is a simple example of the logger implementation:</p> <pre><code>public sealed class LLamaDefaultLogger : ILLamaLogger\n{\nprivate static readonly Lazy&lt;LLamaDefaultLogger&gt; _instance = new Lazy&lt;LLamaDefaultLogger&gt;(() =&gt; new LLamaDefaultLogger());\n\nprivate bool _toConsole = true;\nprivate bool _toFile = false;\n\nprivate FileStream? _fileStream = null;\nprivate StreamWriter _fileWriter = null;\n\npublic static LLamaDefaultLogger Default =&gt; _instance.Value;\n\nprivate LLamaDefaultLogger()\n{\n\n}\n\npublic LLamaDefaultLogger EnableConsole()\n{\n_toConsole = true;\nreturn this;\n}\n\npublic LLamaDefaultLogger DisableConsole()\n{\n_toConsole = false;\nreturn this;\n}\n\npublic LLamaDefaultLogger EnableFile(string filename, FileMode mode = FileMode.Append)\n{\n_fileStream = new FileStream(filename, mode, FileAccess.Write);\n_fileWriter = new StreamWriter(_fileStream);\n_toFile = true;\nreturn this;\n}\n\npublic LLamaDefaultLogger DisableFile(string filename)\n{\nif (_fileWriter is not null)\n{\n_fileWriter.Close();\n_fileWriter = null;\n}\nif (_fileStream is not null)\n{\n_fileStream.Close();\n_fileStream = null;\n}\n_toFile = false;\nreturn this;\n}\n\npublic void Log(string source, string message, LogLevel level)\n{\nif (level == LogLevel.Info)\n{\nInfo(message);\n}\nelse if (level == LogLevel.Debug)\n{\n\n}\nelse if (level == LogLevel.Warning)\n{\nWarn(message);\n}\nelse if (level == LogLevel.Error)\n{\nError(message);\n}\n}\n\npublic void Info(string message)\n{\nmessage = MessageFormat(\"info\", message);\nif (_toConsole)\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.WriteLine(message);\nConsole.ResetColor();\n}\nif (_toFile)\n{\nDebug.Assert(_fileStream is not null);\nDebug.Assert(_fileWriter is not null);\n_fileWriter.WriteLine(message);\n}\n}\n\npublic void Warn(string message)\n{\nmessage = MessageFormat(\"warn\", message);\nif (_toConsole)\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(message);\nConsole.ResetColor();\n}\nif (_toFile)\n{\nDebug.Assert(_fileStream is not null);\nDebug.Assert(_fileWriter is not null);\n_fileWriter.WriteLine(message);\n}\n}\n\npublic void Error(string message)\n{\nmessage = MessageFormat(\"error\", message);\nif (_toConsole)\n{\nConsole.ForegroundColor = ConsoleColor.Red;\nConsole.WriteLine(message);\nConsole.ResetColor();\n}\nif (_toFile)\n{\nDebug.Assert(_fileStream is not null);\nDebug.Assert(_fileWriter is not null);\n_fileWriter.WriteLine(message);\n}\n}\n\nprivate string MessageFormat(string level, string message)\n{\nDateTime now = DateTime.Now;\nstring formattedDate = now.ToString(\"yyyy.MM.dd HH:mm:ss\");\nreturn $\"[{formattedDate}][{level}]: {message}\";\n}\n}\n</code></pre>"},{"location":"Tutorials/ChatSession/","title":"LLamaSharp chat session","text":""},{"location":"Tutorials/ChatSession/#basic-usages-of-chatsession","title":"Basic usages of ChatSession","text":"<p><code>ChatSession</code> is a higher-level abstraction than the executors. In the context of a chat application like ChatGPT, a \"chat session\" refers to an interactive conversation or exchange of messages between the user and the chatbot. It represents a continuous flow of communication where the user enters input or asks questions, and the chatbot responds accordingly. A chat session typically starts when the user initiates a conversation with the chatbot and continues until the interaction comes to a natural end or is explicitly terminated by either the user or the system. During a chat session, the chatbot maintains the context of the conversation, remembers previous messages, and generates appropriate responses based on the user's inputs and the ongoing dialogue.</p>"},{"location":"Tutorials/ChatSession/#initialize-a-session","title":"Initialize a session","text":"<p>Currently, the only parameter that is accepted is an <code>ILLamaExecutor</code>, because this is the only parameter that we're sure to exist in all the future versions. Since it's the high-level abstraction, we're conservative to the API designs. In the future, there may be more kinds of constructors added.</p> <pre><code>InteractiveExecutor ex = new(new LLamaModel(new ModelParams(modelPath)));\nChatSession session = new ChatSession(ex);\n</code></pre>"},{"location":"Tutorials/ChatSession/#chat-with-the-bot","title":"Chat with the bot","text":"<p>There'll be two kinds of input accepted by the <code>Chat</code> API, which are <code>ChatHistory</code> and <code>String</code>. The API with string is quite similar to that of the executors. Meanwhile, the API with <code>ChatHistory</code> is aimed to provide more flexible usages. For example, you have had a chat with the bot in session A before you open the session B. Now session B has no memory for what you said before. Therefore, you can feed the history of A to B.</p> <pre><code>string prompt = \"What is C#?\";\n\nawait foreach (var text in session.ChatAsync(prompt, new InferenceParams() { Temperature = 0.6f, AntiPrompts = new List&lt;string&gt; { \"User:\" } })) // the inference params should be changed depending on your statement\n{\nConsole.Write(text);\n}\n</code></pre>"},{"location":"Tutorials/ChatSession/#get-the-history","title":"Get the history","text":"<p>Currently <code>History</code> is a property of <code>ChatSession</code>.</p> <pre><code>foreach(var rec in session.History.Messages)\n{\nConsole.WriteLine($\"{rec.AuthorRole}: {rec.Content}\");\n}\n</code></pre>"},{"location":"Tutorials/ChatSession/#saveload-chat-session","title":"Save/Load Chat Session","text":"<p>Generally, the chat session could be switched, which requires the ability of loading and saving session.</p> <p>The API is also quite simple, the files will be saved into a directory you specified. If the path does not exist, a new directory will be created.</p> <pre><code>string savePath = \"&lt;save dir&gt;\";\nsession.SaveSession(savePath);\n\nsession.LoadSession(savePath, loadTransforms:true);\nsession.LoadSession(savePath, loadTransforms:false);\n</code></pre> <p>You could also keep the state in memory and load them with the following APIs.</p> <pre><code>var sessionState = session.GetSessionState();\nsession.LoadSession(sessionState, loadTransforms:true);\nsession.LoadSession(sessionState, loadTransforms:false);\n\n## Transforms in Chat Session\n\nThere's three important elements in `ChatSession`, which are input, output and history. Besides, there're some conversions between them. Since the process of them under different conditions varies, LLamaSharp hands over this part of the power to the users.\n\nCurrently, there're three kinds of process that could be customized, as introduced below.\n\n### Input transform\n\nIn general, the input of the chat API is a text (without stream), therefore `ChatSession` processes it in a pipeline. If you want to use your customized transform, you need to define a transform that implements `ITextTransform` and add it to the pipeline of `ChatSession`.\n\n```cs\npublic interface ITextTransform\n{\nstring Transform(string text);\n}\n</code></pre> <pre><code>public class MyInputTransform1 : ITextTransform\n{\npublic string Transform(string text)\n{\nreturn $\"Question: {text}\\n\";\n}\n}\n\npublic class MyInputTransform2 : ITextTransform\n{\npublic string Transform(string text)\n{\nreturn text + \"Answer: \";\n}\n}\n\nsession.AddInputTransform(new MyInputTransform1()).AddInputTransform(new MyInputTransform2());\n</code></pre>"},{"location":"Tutorials/ChatSession/#output-transform","title":"Output transform","text":"<p>Different from the input, the output of chat API is a text stream. Therefore you need to process it word by word, instead of getting the full text at once.</p> <p>The interface of it has an <code>IEnumerable&lt;string&gt;</code> as input, which is actually a yield sequence.</p> <pre><code>public interface ITextStreamTransform\n{\nIEnumerable&lt;string&gt; Transform(IEnumerable&lt;string&gt; tokens);\nIAsyncEnumerable&lt;string&gt; TransformAsync(IAsyncEnumerable&lt;string&gt; tokens);\n}\n</code></pre> <p>When implementing it, you could throw a not-implemented exception in one of them if you only need to use the chat API in synchronously or asynchronously.</p> <p>Different from the input transform pipeline, the output transform only supports one transform.</p> <pre><code>session.WithOutputTransform(new MyOutputTransform());\n</code></pre> <p>Here's an example of how to implement the interface. In this example, the transform detects whether there's some keywords in the response and removes them.</p> <pre><code>/// &lt;summary&gt;\n/// A text output transform that removes the keywords from the response.\n/// &lt;/summary&gt;\npublic class KeywordTextOutputStreamTransform : ITextStreamTransform\n{\nHashSet&lt;string&gt; _keywords;\nint _maxKeywordLength;\nbool _removeAllMatchedTokens;\n\n/// &lt;summary&gt;\n/// \n/// &lt;/summary&gt;\n/// &lt;param name=\"keywords\"&gt;Keywords that you want to remove from the response.&lt;/param&gt;\n/// &lt;param name=\"redundancyLength\"&gt;The extra length when searching for the keyword. For example, if your only keyword is \"highlight\", \n/// maybe the token you get is \"\\r\\nhighligt\". In this condition, if redundancyLength=0, the token cannot be successfully matched because the length of \"\\r\\nhighligt\" (10)\n/// has already exceeded the maximum length of the keywords (8). On the contrary, setting redundancyLengyh &gt;= 2 leads to successful match.\n/// The larger the redundancyLength is, the lower the processing speed. But as an experience, it won't introduce too much performance impact when redundancyLength &lt;= 5 &lt;/param&gt;\n/// &lt;param name=\"removeAllMatchedTokens\"&gt;If set to true, when getting a matched keyword, all the related tokens will be removed. Otherwise only the part of keyword will be removed.&lt;/param&gt;\npublic KeywordTextOutputStreamTransform(IEnumerable&lt;string&gt; keywords, int redundancyLength = 3, bool removeAllMatchedTokens = false)\n{\n_keywords = new(keywords);\n_maxKeywordLength = keywords.Select(x =&gt; x.Length).Max() + redundancyLength;\n_removeAllMatchedTokens = removeAllMatchedTokens;\n}\n/// &lt;inheritdoc /&gt;\npublic IEnumerable&lt;string&gt; Transform(IEnumerable&lt;string&gt; tokens)\n{\nvar window = new Queue&lt;string&gt;();\n\nforeach (var s in tokens)\n{\nwindow.Enqueue(s);\nvar current = string.Join(\"\", window);\nif (_keywords.Any(x =&gt; current.Contains(x)))\n{\nvar matchedKeyword = _keywords.First(x =&gt; current.Contains(x));\nint total = window.Count;\nfor (int i = 0; i &lt; total; i++)\n{\nwindow.Dequeue();\n}\nif (!_removeAllMatchedTokens)\n{\nyield return current.Replace(matchedKeyword, \"\");\n}\n}\nif (current.Length &gt;= _maxKeywordLength)\n{\nif (_keywords.Any(x =&gt; current.Contains(x)))\n{\nvar matchedKeyword = _keywords.First(x =&gt; current.Contains(x));\nint total = window.Count;\nfor (int i = 0; i &lt; total; i++)\n{\nwindow.Dequeue();\n}\nif (!_removeAllMatchedTokens)\n{\nyield return current.Replace(matchedKeyword, \"\");\n}\n}\nelse\n{\nint total = window.Count;\nfor (int i = 0; i &lt; total; i++)\n{\nyield return window.Dequeue();\n}\n}\n}\n}\nint totalCount = window.Count;\nfor (int i = 0; i &lt; totalCount; i++)\n{\nyield return window.Dequeue();\n}\n}\n/// &lt;inheritdoc /&gt;\npublic async IAsyncEnumerable&lt;string&gt; TransformAsync(IAsyncEnumerable&lt;string&gt; tokens)\n{\nthrow new NotImplementedException(); // This is implemented in `LLamaTransforms` but we ignore it here.\n}\n}\n</code></pre>"},{"location":"Tutorials/ChatSession/#history-transform","title":"History transform","text":"<p>The chat history could be converted to or from a text, which is exactly what the interface of it.</p> <pre><code>public interface IHistoryTransform\n{\nstring HistoryToText(ChatHistory history);\nChatHistory TextToHistory(AuthorRole role, string text);\n}\n</code></pre> <p>Similar to the output transform, the history transform is added in the following way:</p> <pre><code>session.WithHistoryTransform(new MyHistoryTransform());\n</code></pre> <p>The implementation is quite flexible, depending on what you want the history message to be like. Here's an example, which is the default history transform in LLamaSharp.</p> <pre><code>/// &lt;summary&gt;\n/// The default history transform.\n/// Uses plain text with the following format:\n/// [Author]: [Message]\n/// &lt;/summary&gt;\npublic class DefaultHistoryTransform : IHistoryTransform\n{\nprivate readonly string defaultUserName = \"User\";\nprivate readonly string defaultAssistantName = \"Assistant\";\nprivate readonly string defaultSystemName = \"System\";\nprivate readonly string defaultUnknownName = \"??\";\n\nstring _userName;\nstring _assistantName;\nstring _systemName;\nstring _unknownName;\nbool _isInstructMode;\npublic DefaultHistoryTransform(string? userName = null, string? assistantName = null, string? systemName = null, string? unknownName = null, bool isInstructMode = false)\n{\n_userName = userName ?? defaultUserName;\n_assistantName = assistantName ?? defaultAssistantName;\n_systemName = systemName ?? defaultSystemName;\n_unknownName = unknownName ?? defaultUnknownName;\n_isInstructMode = isInstructMode;\n}\n\npublic virtual string HistoryToText(ChatHistory history)\n{\nStringBuilder sb = new();\nforeach (var message in history.Messages)\n{\nif (message.AuthorRole == AuthorRole.User)\n{\nsb.AppendLine($\"{_userName}: {message.Content}\");\n}\nelse if (message.AuthorRole == AuthorRole.System)\n{\nsb.AppendLine($\"{_systemName}: {message.Content}\");\n}\nelse if (message.AuthorRole == AuthorRole.Unknown)\n{\nsb.AppendLine($\"{_unknownName}: {message.Content}\");\n}\nelse if (message.AuthorRole == AuthorRole.Assistant)\n{\nsb.AppendLine($\"{_assistantName}: {message.Content}\");\n}\n}\nreturn sb.ToString();\n}\n\npublic virtual ChatHistory TextToHistory(AuthorRole role, string text)\n{\nChatHistory history = new ChatHistory();\nhistory.AddMessage(role, TrimNamesFromText(text, role));\nreturn history;\n}\n\npublic virtual string TrimNamesFromText(string text, AuthorRole role)\n{\nif (role == AuthorRole.User &amp;&amp; text.StartsWith($\"{_userName}:\"))\n{\ntext = text.Substring($\"{_userName}:\".Length).TrimStart();\n}\nelse if (role == AuthorRole.Assistant &amp;&amp; text.EndsWith($\"{_assistantName}:\"))\n{\ntext = text.Substring(0, text.Length - $\"{_assistantName}:\".Length).TrimEnd();\n}\nif (_isInstructMode &amp;&amp; role == AuthorRole.Assistant &amp;&amp; text.EndsWith(\"\\n&gt; \"))\n{\ntext = text.Substring(0, text.Length - \"\\n&gt; \".Length).TrimEnd();\n}\nreturn text;\n}\n}\n</code></pre>"},{"location":"Tutorials/Executors/","title":"LLamaSharp executors","text":"<p>LLamaSharp executor defines the behavior of the model when it is called. Currently, there are four kinds of executors, which are <code>InteractiveExecutor</code>, <code>InstructExecutor</code>, <code>StatelessExecutor</code> and <code>BatchedExecutor</code>.</p> <p>In a word, <code>InteractiveExecutor</code> is suitable for getting answer of your questions from LLM continuously. <code>InstructExecutor</code> let LLM execute your instructions, such as \"continue writing\". <code>StatelessExecutor</code> is best for one-time job because the previous inference has no impact on the current inference. <code>BatchedExecutor</code> could accept multiple inputs and generate multiple outputs of different sessions at the same time, significantly improving the throughput of the program.</p>"},{"location":"Tutorials/Executors/#text-to-text-apis-of-the-executors","title":"Text-to-Text APIs of the executors","text":"<p>All the executors implements the interface <code>ILLamaExecutor</code>, which provides two APIs to execute text-to-text tasks.</p> <pre><code>public interface ILLamaExecutor\n{\n/// &lt;summary&gt;\n/// The loaded context for this executor.\n/// &lt;/summary&gt;\npublic LLamaContext Context { get; }\n\n// LLava Section\n//\n/// &lt;summary&gt;\n/// Identify if it's a multi-modal model and there is a image to process.\n/// &lt;/summary&gt;\npublic bool IsMultiModal { get; }\n/// &lt;summary&gt;\n/// Multi-Modal Projections / Clip Model weights\n/// &lt;/summary&gt;\npublic LLavaWeights? ClipModel { get;  }        /// &lt;summary&gt;\n/// List of images: List of images in byte array format.\n/// &lt;/summary&gt;\npublic List&lt;byte[]&gt; Images { get; }\n\n\n/// &lt;summary&gt;\n/// Asynchronously infers a response from the model.\n/// &lt;/summary&gt;\n/// &lt;param name=\"text\"&gt;Your prompt&lt;/param&gt;\n/// &lt;param name=\"inferenceParams\"&gt;Any additional parameters&lt;/param&gt;\n/// &lt;param name=\"token\"&gt;A cancellation token.&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nIAsyncEnumerable&lt;string&gt; InferAsync(string text, IInferenceParams? inferenceParams = null, CancellationToken token = default);\n}\n</code></pre> <p>The output of both two APIs are yield enumerable. Therefore, when receiving the output, you can directly use <code>foreach</code> to take actions on each word you get by order, instead of waiting for the whole process completed.</p>"},{"location":"Tutorials/Executors/#interactiveexecutor--instructexecutor","title":"InteractiveExecutor &amp; InstructExecutor","text":"<p>Both of them are taking \"completing the prompt\" as the goal to generate the response. For example, if you input <code>Long long ago, there was a fox who wanted to make friend with humen. One day</code>, then the LLM will continue to write the story.</p> <p>Under interactive mode, you serve a role of user and the LLM serves the role of assistant. Then it will help you with your question or request. </p> <p>Under instruct mode, you give LLM some instructions and it follows.</p> <p>Though the behaviors of them sounds similar, it could introduce many differences depending on your prompt. For example, \"chat-with-bob\" has good performance under interactive mode and <code>alpaca</code> does well with instruct mode.</p> <pre><code>// chat-with-bob\n\nTranscript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\n\nUser: Hello, Bob.\nBob: Hello. How may I help you today?\nUser: Please tell me the largest city in Europe.\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\nUser:\n</code></pre> <pre><code>// alpaca\n\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n</code></pre> <p>Therefore, please modify the prompt correspondingly when switching from one mode to the other.</p>"},{"location":"Tutorials/Executors/#statelessexecutor","title":"StatelessExecutor.","text":"<p>Despite the differences between interactive mode and instruct mode, both of them are stateful mode. That is, your previous question/instruction will impact on the current response from LLM. On the contrary, the stateless executor does not have such a \"memory\". No matter how many times you talk to it, it will only concentrate on what you say in this time. It is very useful when you want a clean context, without being affected by previous inputs.</p> <p>Since the stateless executor has no memory of conversations before, you need to input your question with the whole prompt into it to get the better answer.</p> <p>For example, if you feed <code>Q: Who is Trump? A:</code> to the stateless executor, it may give the following answer with the antiprompt <code>Q:</code>.</p> <pre><code>Donald J. Trump, born June 14, 1946, is an American businessman, television personality, politician and the 45th President of the United States (2017-2021). # Anexo:Torneo de Hamburgo 2022 (individual masculino)\n\n## Presentaci\u00f3n previa\n\n* Defensor del t\u00edtulo:  Daniil Medv\u00e9dev\n</code></pre> <p>It seems that things went well at first. However, after answering the question itself, LLM began to talk about some other things until the answer reached the token count limit. The reason of this strange behavior is the anti-prompt cannot be match. With the input, LLM cannot decide whether to append a string \"A: \" at the end of the response.</p> <p>As an improvement, let's take the following text as the input:</p> <pre><code>Q: What is the capital of the USA? A: Washingtong. Q: What is the sum of 1 and 2? A: 3. Q: Who is Trump? A: \n</code></pre> <p>Then, I got the following answer with the anti-prompt <code>Q:</code>.</p> <pre><code>45th president of the United States.\n</code></pre> <p>At this time, by repeating the same mode of <code>Q: xxx? A: xxx.</code>, LLM outputs the anti-prompt we want to help to decide where to stop the generation.</p>"},{"location":"Tutorials/Executors/#batchedexecutor","title":"BatchedExecutor","text":"<p>Different from other executors, <code>BatchedExecutor</code> could accept multiple inputs from different sessions and generate outputs for them at the same time. Here is an example to use it.</p> <pre><code>using LLama.Batched;\nusing LLama.Common;\nusing LLama.Native;\nusing LLama.Sampling;\nusing Spectre.Console;\n\nnamespace LLama.Examples.Examples;\n\n/// &lt;summary&gt;\n/// This demonstrates using a batch to generate two sequences and then using one\n/// sequence as the negative guidance (\"classifier free guidance\") for the other.\n/// &lt;/summary&gt;\npublic class BatchedExecutorGuidance\n{\nprivate const int n_len = 32;\n\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath);\nusing var model = LLamaWeights.LoadFromFile(parameters);\n\nvar positivePrompt = AnsiConsole.Ask(\"Positive Prompt (or ENTER for default):\", \"My favourite colour is\").Trim();\nvar negativePrompt = AnsiConsole.Ask(\"Negative Prompt (or ENTER for default):\", \"I hate the colour red. My favourite colour is\").Trim();\nvar weight = AnsiConsole.Ask(\"Guidance Weight (or ENTER for default):\", 2.0f);\n\n// Create an executor that can evaluate a batch of conversations together\nusing var executor = new BatchedExecutor(model, parameters);\n\n// Print some info\nvar name = executor.Model.Metadata.GetValueOrDefault(\"general.name\", \"unknown model name\");\nConsole.WriteLine($\"Created executor with model: {name}\");\n\n// Load the two prompts into two conversations\nusing var guided = executor.Create();\nguided.Prompt(positivePrompt);\nusing var guidance = executor.Create();\nguidance.Prompt(negativePrompt);\n\n// Run inference to evaluate prompts\nawait AnsiConsole\n.Status()\n.Spinner(Spinner.Known.Line)\n.StartAsync(\"Evaluating Prompts...\", _ =&gt; executor.Infer());\n\n// Fork the \"guided\" conversation. We'll run this one without guidance for comparison\nusing var unguided = guided.Fork();\n\n// Run inference loop\nvar unguidedSampler = new GuidedSampler(null, weight);\nvar unguidedDecoder = new StreamingTokenDecoder(executor.Context);\nvar guidedSampler = new GuidedSampler(guidance, weight);\nvar guidedDecoder = new StreamingTokenDecoder(executor.Context);\nawait AnsiConsole\n.Progress()\n.StartAsync(async progress =&gt;\n{\nvar reporter = progress.AddTask(\"Running Inference\", maxValue: n_len);\n\nfor (var i = 0; i &lt; n_len; i++)\n{\nif (i != 0)\nawait executor.Infer();\n\n// Sample from the \"unguided\" conversation. This is just a conversation using the same prompt, without any\n// guidance. This serves as a comparison to show the effect of guidance.\nvar u = unguidedSampler.Sample(executor.Context.NativeHandle, unguided.Sample(), Array.Empty&lt;LLamaToken&gt;());\nunguidedDecoder.Add(u);\nunguided.Prompt(u);\n\n// Sample from the \"guided\" conversation. This sampler will internally use the \"guidance\" conversation\n// to steer the conversation. See how this is done in GuidedSampler.ProcessLogits (bottom of this file).\nvar g = guidedSampler.Sample(executor.Context.NativeHandle, guided.Sample(), Array.Empty&lt;LLamaToken&gt;());\nguidedDecoder.Add(g);\n\n// Use this token to advance both guided _and_ guidance. Keeping them in sync (except for the initial prompt).\nguided.Prompt(g);\nguidance.Prompt(g);\n\n// Early exit if we reach the natural end of the guided sentence\nif (g == model.EndOfSentenceToken)\nbreak;\n\n// Update progress bar\nreporter.Increment(1);\n}\n});\n\nAnsiConsole.MarkupLine($\"[green]Unguided:[/][white]{unguidedDecoder.Read().ReplaceLineEndings(\" \")}[/]\");\nAnsiConsole.MarkupLine($\"[green]Guided:[/][white]{guidedDecoder.Read().ReplaceLineEndings(\" \")}[/]\");\n}\n\nprivate class GuidedSampler(Conversation? guidance, float weight)\n: BaseSamplingPipeline\n{\npublic override void Accept(SafeLLamaContextHandle ctx, LLamaToken token)\n{\n}\n\npublic override ISamplingPipeline Clone()\n{\nthrow new NotSupportedException();\n}\n\nprotected override void ProcessLogits(SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n{\nif (guidance == null)\nreturn;\n\n// Get the logits generated by the guidance sequences\nvar guidanceLogits = guidance.Sample();\n\n// Use those logits to guide this sequence\nNativeApi.llama_sample_apply_guidance(ctx, logits, guidanceLogits, weight);\n}\n\nprotected override LLamaToken ProcessTokenDataArray(SafeLLamaContextHandle ctx, LLamaTokenDataArray candidates, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n{\ncandidates.Temperature(ctx, 0.8f);\ncandidates.TopK(ctx, 25);\n\nreturn candidates.SampleToken(ctx);\n}\n}\n}\n</code></pre>"},{"location":"Tutorials/Executors/#inference-parameters","title":"Inference parameters","text":"<p>Different from context parameters, which is indicated in understand-llama-context, executors accept parameters when you call its API to execute the inference. That means you could change the parameters every time you ask the model to generate the outputs.</p> <p>Here is the parameters for LLamaSharp executors.</p> <pre><code>/// &lt;summary&gt;\n/// The parameters used for inference.\n/// &lt;/summary&gt;\npublic record InferenceParams\n: IInferenceParams\n{\n/// &lt;summary&gt;\n/// number of tokens to keep from initial prompt\n/// &lt;/summary&gt;\npublic int TokensKeep { get; set; } = 0;\n\n/// &lt;summary&gt;\n/// how many new tokens to predict (n_predict), set to -1 to inifinitely generate response\n/// until it complete.\n/// &lt;/summary&gt;\npublic int MaxTokens { get; set; } = -1;\n\n/// &lt;summary&gt;\n/// logit bias for specific tokens\n/// &lt;/summary&gt;\npublic Dictionary&lt;LLamaToken, float&gt;? LogitBias { get; set; } = null;\n\n/// &lt;summary&gt;\n/// Sequences where the model will stop generating further tokens.\n/// &lt;/summary&gt;\npublic IReadOnlyList&lt;string&gt; AntiPrompts { get; set; } = Array.Empty&lt;string&gt;();\n\n/// &lt;inheritdoc /&gt;\npublic int TopK { get; set; } = 40;\n\n/// &lt;inheritdoc /&gt;\npublic float TopP { get; set; } = 0.95f;\n\n/// &lt;inheritdoc /&gt;\npublic float MinP { get; set; } = 0.05f;\n\n/// &lt;inheritdoc /&gt;\npublic float TfsZ { get; set; } = 1.0f;\n\n/// &lt;inheritdoc /&gt;\npublic float TypicalP { get; set; } = 1.0f;\n\n/// &lt;inheritdoc /&gt;\npublic float Temperature { get; set; } = 0.8f;\n\n/// &lt;inheritdoc /&gt;\npublic float RepeatPenalty { get; set; } = 1.1f;\n\n/// &lt;inheritdoc /&gt;\npublic int RepeatLastTokensCount { get; set; } = 64;\n\n/// &lt;inheritdoc /&gt;\npublic float FrequencyPenalty { get; set; } = .0f;\n\n/// &lt;inheritdoc /&gt;\npublic float PresencePenalty { get; set; } = .0f;\n\n/// &lt;inheritdoc /&gt;\npublic MirostatType Mirostat { get; set; } = MirostatType.Disable;\n\n/// &lt;inheritdoc /&gt;\npublic float MirostatTau { get; set; } = 5.0f;\n\n/// &lt;inheritdoc /&gt;\npublic float MirostatEta { get; set; } = 0.1f;\n\n/// &lt;inheritdoc /&gt;\npublic bool PenalizeNL { get; set; } = true;\n\n/// &lt;inheritdoc /&gt;\npublic SafeLLamaGrammarHandle? Grammar { get; set; }\n\n/// &lt;inheritdoc /&gt;\npublic ISamplingPipeline? SamplingPipeline { get; set; }\n}\n</code></pre>"},{"location":"Tutorials/Executors/#save-and-load-executor-state","title":"Save and load executor state","text":"<p>An executor also has its state, which can be saved and loaded. That means a lot when you want to support restore a previous session for the user in your application.</p> <p>The following code shows how to use save and load executor state.</p> <pre><code>InteractiveExecutor executor = new InteractiveExecutor(model);\n// do some things...\nexecutor.SaveState(\"executor.st\");\nvar stateData = executor.GetStateData();\n\nInteractiveExecutor executor2 = new InteractiveExecutor(model);\nexecutor2.LoadState(stateData);\n// do some things...\n\nInteractiveExecutor executor3 = new InteractiveExecutor(model);\nexecutor3.LoadState(\"executor.st\");\n// do some things...\n</code></pre>"},{"location":"Tutorials/GetEmbeddings/","title":"Get embeddings","text":"<p>Getting the embeddings of a text in LLM is sometimes useful, for example, to train other MLP models.</p> <p>To get the embeddings, please initialize a <code>LLamaEmbedder</code> and then call <code>GetEmbeddings</code>.</p> <pre><code>var embedder = new LLamaEmbedder(new ModelParams(\"&lt;modelPath&gt;\"));\nstring text = \"hello, LLM.\";\nfloat[] embeddings = embedder.GetEmbeddings(text);\n</code></pre> <p>The output is a float array. Note that the length of the array is related with the model you load. If you just want to get a smaller size embedding, please consider changing a model.</p>"},{"location":"Tutorials/NativeLibraryConfig/","title":"Configure the native library loading","text":"<p>As indicated in Architecture, LLamaSharp uses the native library to run the LLM models. Sometimes you may want to compile the native library yourself, or just dynamically load the library due to the environment of your user of your application. Luckily, since version 0.7.0, dynamic loading of native library has been supported! That allows you to customize the native library loading process.</p>"},{"location":"Tutorials/NativeLibraryConfig/#when-you-should-compile-the-native-library-yourself","title":"When you should compile the native library yourself","text":"<p>Before introducing the way to customize native library loading, please follow the tips below to see if you need to compile the native library yourself, rather than use the published backend packages, which contain native library files for multiple targets.</p> <ol> <li>Your device/environment has not been supported by any published backend packages. For example, vulkan has not been supported yet. In this case, it will mean a lot to open an issue to tell us you are using it. Since our support for new backend will have a delay, you could compile yourself before that.</li> <li>You want to gain the best performance of LLamaSharp. Because LLamaSharp offloads the model to both GPU and CPU, the performance is significantly related with CPU if your GPU memory size is small. AVX (Advanced Vector Extensions) and BLAS (Basic Linear Algebra Subprograms) are the most important ways to accelerate the CPU computation. By default, LLamaSharp disables the support for BLAS and use AVX2 for CUDA backend yet. If you would like to enable BLAS or use AVX 512 along with CUDA, please compile the native library yourself, following the instructions here.</li> <li>You want to debug the c++ code.</li> </ol>"},{"location":"Tutorials/NativeLibraryConfig/#use-nativelibraryconfig","title":"Use NativeLibraryConfig","text":"<p>We provide <code>LLama.Native.NativeLibraryConfig</code> class with singleton mode to allow users to customize the loading process of the native library. Any method of it should be called before the model loading, because a native library file must be decided before any model is loaded.</p>"},{"location":"Tutorials/NativeLibraryConfig/#load-specified-native-library-file","title":"Load specified native library file","text":"<p>All you need to do is adding the following code to the very beginning of your code.</p> <pre><code>NativeLibraryConfig.All.WithLibrary(\"&lt;Your native library path&gt;\");\n</code></pre> <p>If you want to configure the loading for LLama library or llava library respectively, please call the following APIs.</p> <pre><code>NativeLibraryConfig.LLama.WithLibrary(\"&lt;Your llama native library path&gt;\");\nNativeLibraryConfig.LLava.WithLibrary(\"&lt;Your llava native library path&gt;\");\n</code></pre>"},{"location":"Tutorials/NativeLibraryConfig/#automatically-select-one-from-multiple-native-library-files","title":"Automatically select one from multiple native library files","text":"<p>Let's consider this case: you don't know your user's device when distributing your application, so you put all the possible native libraries in a folder and want to select the best one depending on the user's device. LLamaSharp allows you to define the strategy to do it.</p> <ul> <li><code>NativeLibraryConfig.All.WithCuda</code>: decide if you want to use cuda if possible.</li> <li><code>NativeLibraryConfig.All.WithAvx</code>: decide the highest AVX level you want to use if possible.</li> <li><code>NativeLibraryConfig.All.WithSearchDirectory</code>: specify the directory to search the native library files.</li> <li><code>NativeLibraryConfig.All.WithAutoFallback</code>: whether to allow fall back to other options if no native library that matches your specified settings could be found.</li> </ul>"},{"location":"Tutorials/NativeLibraryConfig/#skip-the-check-in-case-of-false-trigger-of-validation","title":"Skip the check in case of false trigger of validation","text":"<p><code>NativeLibraryConfig.All.SkipCheck</code> allows you to skip the checks of the device/environment compatibility. This API will be very useful if your users have an unusual environment. For example, if the user installed the cuda toolkit in a customized path and didn't set the environment variables correctly, it might be regarded as invalid even though the loading could actually succeed.</p>"},{"location":"Tutorials/NativeLibraryConfig/#dry-run-your-configuration","title":"Dry run your configuration","text":"<p>In some conditions, you or your users are not sure if the native library could be successfully loaded with the configuration. To address this issue, <code>NativeLibraryConfig.All.DryRun</code> is provided to try to run the native library loading with the current configuration. It will return whether the loading is successfully, and the loaded library. However, the loaded library is not assigned to the handle of <code>NativeApi</code>. So you could try different configurations until one of them could be loaded successfully.</p> <p>NOTE: Similar to other APIs, <code>NativeLibraryConfig.All.DryRun</code> is only available before calling an arbitrary API in <code>NativeApi</code>, too.</p>"},{"location":"Tutorials/NativeLibraryConfig/#set-the-log-level-of-native-library-loading","title":"Set the log level of native library loading","text":"<pre><code>NativeLibraryConfig.All.WithLogs();\n</code></pre> <p>There are four log levels, which are error, warning, info and debug. If you are not sure if the correct library is selected, please set log level to <code>info</code> to see the full logs.</p>"},{"location":"Tutorials/Quantization/","title":"Quantization","text":"<p>Quantization is significant to accelerate the model inference. Since there's little accuracy (performance) reduction when quantizing the model, get it easy to quantize it!</p> <p>To quantize the model, please call <code>Quantize</code> from <code>LLamaQuantizer</code>, which is a static method.</p> <pre><code>string srcPath = \"&lt;model.bin&gt;\";\nstring dstPath = \"&lt;model_q4_0.bin&gt;\";\nLLamaQuantizer.Quantize(srcPath, dstPath, \"q4_0\");\n// The following overload is also okay.\n// LLamaQuantizer.Quantize(srcPath, dstPath, LLamaFtype.LLAMA_FTYPE_MOSTLY_Q4_0);\n</code></pre> <p>After calling it, a quantized model file will be saved.</p> <p>There're currently the following types of quantization supported:</p> <pre><code>{ \"Q4_0\",   LLAMA_FTYPE_MOSTLY_Q4_0,   \" 3.56G, +0.2166 ppl @ LLaMA-v1-7B\", },\n{ \"Q4_1\",   LLAMA_FTYPE_MOSTLY_Q4_1,   \" 3.90G, +0.1585 ppl @ LLaMA-v1-7B\", },\n{ \"Q5_0\",   LLAMA_FTYPE_MOSTLY_Q5_0,   \" 4.33G, +0.0683 ppl @ LLaMA-v1-7B\", },\n{ \"Q5_1\",   LLAMA_FTYPE_MOSTLY_Q5_1,   \" 4.70G, +0.0349 ppl @ LLaMA-v1-7B\", },\n{ \"IQ2_XXS\",LLAMA_FTYPE_MOSTLY_IQ2_XXS,\" 2.06 bpw quantization\",            },\n{ \"IQ2_XS\", LLAMA_FTYPE_MOSTLY_IQ2_XS, \" 2.31 bpw quantization\",            },\n{ \"IQ2_S\",  LLAMA_FTYPE_MOSTLY_IQ2_S,  \" 2.5  bpw quantization\",            },\n{ \"IQ2_M\",  LLAMA_FTYPE_MOSTLY_IQ2_M,  \" 2.7  bpw quantization\",            },\n{ \"IQ1_S\",  LLAMA_FTYPE_MOSTLY_IQ1_S,  \" 1.56 bpw quantization\",            },\n{ \"IQ1_M\",  LLAMA_FTYPE_MOSTLY_IQ1_M,  \" 1.75 bpw quantization\",            },\n{ \"Q2_K\",   LLAMA_FTYPE_MOSTLY_Q2_K,   \" 2.63G, +0.6717 ppl @ LLaMA-v1-7B\", },\n{ \"Q2_K_S\", LLAMA_FTYPE_MOSTLY_Q2_K_S, \" 2.16G, +9.0634 ppl @ LLaMA-v1-7B\", },\n{ \"IQ3_XXS\",LLAMA_FTYPE_MOSTLY_IQ3_XXS,\" 3.06 bpw quantization\",            },\n{ \"IQ3_S\",  LLAMA_FTYPE_MOSTLY_IQ3_S,  \" 3.44 bpw quantization\",            },\n{ \"IQ3_M\",  LLAMA_FTYPE_MOSTLY_IQ3_M,  \" 3.66 bpw quantization mix\",        },\n{ \"Q3_K\",   LLAMA_FTYPE_MOSTLY_Q3_K_M, \"alias for Q3_K_M\" },\n{ \"IQ3_XS\", LLAMA_FTYPE_MOSTLY_IQ3_XS, \" 3.3 bpw quantization\"   ,          },\n{ \"Q3_K_S\", LLAMA_FTYPE_MOSTLY_Q3_K_S, \" 2.75G, +0.5551 ppl @ LLaMA-v1-7B\", },\n{ \"Q3_K_M\", LLAMA_FTYPE_MOSTLY_Q3_K_M, \" 3.07G, +0.2496 ppl @ LLaMA-v1-7B\", },\n{ \"Q3_K_L\", LLAMA_FTYPE_MOSTLY_Q3_K_L, \" 3.35G, +0.1764 ppl @ LLaMA-v1-7B\", },\n{ \"IQ4_NL\", LLAMA_FTYPE_MOSTLY_IQ4_NL, \" 4.50 bpw non-linear quantization\", },\n{ \"IQ4_XS\", LLAMA_FTYPE_MOSTLY_IQ4_XS, \" 4.25 bpw non-linear quantization\", },\n{ \"Q4_K\",   LLAMA_FTYPE_MOSTLY_Q4_K_M, \"alias for Q4_K_M\", },\n{ \"Q4_K_S\", LLAMA_FTYPE_MOSTLY_Q4_K_S, \" 3.59G, +0.0992 ppl @ LLaMA-v1-7B\", },\n{ \"Q4_K_M\", LLAMA_FTYPE_MOSTLY_Q4_K_M, \" 3.80G, +0.0532 ppl @ LLaMA-v1-7B\", },\n{ \"Q5_K\",   LLAMA_FTYPE_MOSTLY_Q5_K_M, \"alias for Q5_K_M\", },\n{ \"Q5_K_S\", LLAMA_FTYPE_MOSTLY_Q5_K_S, \" 4.33G, +0.0400 ppl @ LLaMA-v1-7B\", },\n{ \"Q5_K_M\", LLAMA_FTYPE_MOSTLY_Q5_K_M, \" 4.45G, +0.0122 ppl @ LLaMA-v1-7B\", },\n{ \"Q6_K\",   LLAMA_FTYPE_MOSTLY_Q6_K,   \" 5.15G, +0.0008 ppl @ LLaMA-v1-7B\", },\n{ \"Q8_0\",   LLAMA_FTYPE_MOSTLY_Q8_0,   \" 6.70G, +0.0004 ppl @ LLaMA-v1-7B\", },\n{ \"F16\",    LLAMA_FTYPE_MOSTLY_F16,    \"13.00G              @ 7B\", },\n{ \"F32\",    LLAMA_FTYPE_ALL_F32,       \"26.00G              @ 7B\", },\n// Note: Ensure COPY comes after F32 to avoid ftype 0 from matching.\n{ \"COPY\",   LLAMA_FTYPE_ALL_F32,       \"only copy tensors, no quantizing\", },\n</code></pre>"},{"location":"Tutorials/UnderstandLLamaContext/","title":"Understand LLamaSharp context","text":"<p><code>LLamaContext</code> is the most important component as a link between native APIs and higher-level APIs. It contains the basic settings for model inference and holds the kv-cache, which could significantly accelerate the model inference. Since <code>LLamaContext</code> is not coupled with <code>LLamaWeights</code>, it's possible to create multiple context based on one piece of model weight. Each <code>ILLamaExecutor</code> will hold a <code>LLamaContext</code> instance, but it's possible to switch to different context in an executor.</p> <p>If your application has multiple sessions, please take care of managing <code>LLamaContext</code>.</p> <p><code>LLamaContext</code> takes the following parameters as its settings. Note that the parameters could not be changed once the context has been created.</p> <pre><code>public interface IContextParams\n{\n/// &lt;summary&gt;\n/// Model context size (n_ctx)\n/// &lt;/summary&gt;\nuint? ContextSize { get; }\n\n/// &lt;summary&gt;\n/// batch size for prompt processing (must be &gt;=32 to use BLAS) (n_batch)\n/// &lt;/summary&gt;\nuint BatchSize { get; }\n\n/// &lt;summary&gt;\n/// Seed for the random number generator (seed)\n/// &lt;/summary&gt;\nuint Seed { get; }\n\n/// &lt;summary&gt;\n/// Whether to use embedding mode. (embedding) Note that if this is set to true, \n/// The LLamaModel won't produce text response anymore.\n/// &lt;/summary&gt;\nbool EmbeddingMode { get; }\n\n/// &lt;summary&gt;\n/// RoPE base frequency (null to fetch from the model)\n/// &lt;/summary&gt;\nfloat? RopeFrequencyBase { get; }\n\n/// &lt;summary&gt;\n/// RoPE frequency scaling factor (null to fetch from the model)\n/// &lt;/summary&gt;\nfloat? RopeFrequencyScale { get; }\n\n/// &lt;summary&gt;\n/// The encoding to use for models\n/// &lt;/summary&gt;\nEncoding Encoding { get; }\n\n/// &lt;summary&gt;\n/// Number of threads (null = autodetect) (n_threads)\n/// &lt;/summary&gt;\nuint? Threads { get; }\n\n/// &lt;summary&gt;\n/// Number of threads to use for batch processing (null = autodetect) (n_threads)\n/// &lt;/summary&gt;\nuint? BatchThreads { get; }\n\n/// &lt;summary&gt;\n/// YaRN extrapolation mix factor (null = from model)\n/// &lt;/summary&gt;\nfloat? YarnExtrapolationFactor { get; }\n\n/// &lt;summary&gt;\n/// YaRN magnitude scaling factor (null = from model)\n/// &lt;/summary&gt;\nfloat? YarnAttentionFactor { get; }\n\n/// &lt;summary&gt;\n/// YaRN low correction dim (null = from model)\n/// &lt;/summary&gt;\nfloat? YarnBetaFast { get; }\n\n/// &lt;summary&gt;\n/// YaRN high correction dim (null = from model)\n/// &lt;/summary&gt;\nfloat? YarnBetaSlow { get; }\n\n/// &lt;summary&gt;\n/// YaRN original context length (null = from model)\n/// &lt;/summary&gt;\nuint? YarnOriginalContext { get; }\n\n/// &lt;summary&gt;\n/// YaRN scaling method to use.\n/// &lt;/summary&gt;\nRopeScalingType? YarnScalingType { get; }\n\n/// &lt;summary&gt;\n/// Override the type of the K cache\n/// &lt;/summary&gt;\nGGMLType? TypeK { get; }\n\n/// &lt;summary&gt;\n/// Override the type of the V cache\n/// &lt;/summary&gt;\nGGMLType? TypeV { get; }\n\n/// &lt;summary&gt;\n/// Whether to disable offloading the KQV cache to the GPU\n/// &lt;/summary&gt;\nbool NoKqvOffload { get; }\n\n/// &lt;summary&gt;\n/// defragment the KV cache if holes/size &amp;gt; defrag_threshold, Set to &amp;lt; 0 to disable (default)\n/// &lt;/summary&gt;\nfloat DefragThreshold { get; }\n\n/// &lt;summary&gt;\n/// Whether to pool (sum) embedding results by sequence id (ignored if no pooling layer)\n/// &lt;/summary&gt;\nbool DoPooling { get; }\n}\n</code></pre> <p><code>LLamaContext</code> has its state, which could be saved and loaded.</p> <pre><code>LLamaContext.SaveState(string filename)\nLLamaContext.GetState()\n</code></pre>"},{"location":"xmldocs/","title":"LLamaSharp","text":""},{"location":"xmldocs/#llama","title":"LLama","text":"<p>AntipromptProcessor</p> <p>ChatSession</p> <p>InstructExecutor</p> <p>InteractiveExecutor</p> <p>LLamaContext</p> <p>LLamaEmbedder</p> <p>LLamaQuantizer</p> <p>LLamaTransforms</p> <p>LLamaWeights</p> <p>LLavaWeights</p> <p>SessionState</p> <p>StatefulExecutorBase</p> <p>StatelessExecutor</p> <p>StreamingTokenDecoder</p>"},{"location":"xmldocs/#llamaabstractions","title":"LLama.Abstractions","text":"<p>AdapterCollection</p> <p>IContextParams</p> <p>IHistoryTransform</p> <p>IInferenceParams</p> <p>ILLamaExecutor</p> <p>ILLamaParams</p> <p>IModelParams</p> <p>ITextStreamTransform</p> <p>ITextTransform</p> <p>LoraAdapter</p> <p>MetadataOverride</p> <p>MetadataOverrideConverter</p> <p>TensorSplitsCollection</p> <p>TensorSplitsCollectionConverter</p>"},{"location":"xmldocs/#llamabatched","title":"LLama.Batched","text":"<p>AlreadyPromptedConversationException</p> <p>BatchedExecutor</p> <p>CannotForkWhileRequiresInferenceException</p> <p>CannotModifyWhileRequiresInferenceException</p> <p>CannotSampleRequiresInferenceException</p> <p>CannotSampleRequiresPromptException</p> <p>Conversation</p> <p>ConversationExtensions</p> <p>ExperimentalBatchedExecutorException</p>"},{"location":"xmldocs/#llamacommon","title":"LLama.Common","text":"<p>AuthorRole</p> <p>ChatHistory</p> <p>FixedSizeQueue&lt;T&gt;</p> <p>InferenceParams</p> <p>MirostatType</p> <p>ModelParams</p>"},{"location":"xmldocs/#llamaexceptions","title":"LLama.Exceptions","text":"<p>GrammarExpectedName</p> <p>GrammarExpectedNext</p> <p>GrammarExpectedPrevious</p> <p>GrammarFormatException</p> <p>GrammarUnexpectedCharAltElement</p> <p>GrammarUnexpectedCharRngElement</p> <p>GrammarUnexpectedEndElement</p> <p>GrammarUnexpectedEndOfInput</p> <p>GrammarUnexpectedHexCharsCount</p> <p>GrammarUnknownEscapeCharacter</p> <p>LLamaDecodeError</p> <p>LoadWeightsFailedException</p> <p>RuntimeError</p>"},{"location":"xmldocs/#llamaextensions","title":"LLama.Extensions","text":"<p>IContextParamsExtensions</p> <p>IModelParamsExtensions</p>"},{"location":"xmldocs/#llamagrammars","title":"LLama.Grammars","text":"<p>Grammar</p> <p>GrammarRule</p>"},{"location":"xmldocs/#llamanative","title":"LLama.Native","text":"<p>DecodeResult</p> <p>GGMLType</p> <p>GPUSplitMode</p> <p>LLamaBatch</p> <p>LLamaBeamsState</p> <p>LLamaBeamView</p> <p>LLamaChatMessage</p> <p>LLamaContextParams</p> <p>LLamaFtype</p> <p>LLamaGrammarElement</p> <p>LLamaGrammarElementType</p> <p>LLamaKvCacheView</p> <p>LLamaKvCacheViewCell</p> <p>LLamaKvCacheViewSafeHandle</p> <p>LLamaLogLevel</p> <p>LLamaModelKvOverrideType</p> <p>LLamaModelMetadataOverride</p> <p>LLamaModelParams</p> <p>LLamaModelQuantizeParams</p> <p>LLamaNativeBatch</p> <p>LLamaPoolingType</p> <p>LLamaPos</p> <p>LLamaRopeType</p> <p>LLamaSeqId</p> <p>LLamaToken</p> <p>LLamaTokenData</p> <p>LLamaTokenDataArray</p> <p>LLamaTokenDataArrayNative</p> <p>LLamaTokenType</p> <p>LLamaVocabType</p> <p>LLavaImageEmbed</p> <p>NativeApi</p> <p>NativeLibraryConfig</p> <p>RopeScalingType</p> <p>SafeLLamaContextHandle</p> <p>SafeLLamaGrammarHandle</p> <p>SafeLLamaHandleBase</p> <p>SafeLlamaModelHandle</p> <p>SafeLlavaImageEmbedHandle</p> <p>SafeLlavaModelHandle</p>"},{"location":"xmldocs/#llamasampling","title":"LLama.Sampling","text":"<p>BaseSamplingPipeline</p> <p>DefaultSamplingPipeline</p> <p>GreedySamplingPipeline</p> <p>ISamplingPipeline</p> <p>ISamplingPipelineExtensions</p> <p>Mirostate2SamplingPipeline</p> <p>MirostateSamplingPipeline</p>"},{"location":"xmldocs/llama.abstractions.adaptercollection/","title":"AdapterCollection","text":"<p>Namespace: LLama.Abstractions</p> <p>A list of LoraAdapter objects</p> <pre><code>public sealed class AdapterCollection : System.Collections.Generic.List`1[[LLama.Abstractions.LoraAdapter, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]], System.Collections.Generic.IList`1[[LLama.Abstractions.LoraAdapter, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]], System.Collections.Generic.ICollection`1[[LLama.Abstractions.LoraAdapter, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]], System.Collections.Generic.IEnumerable`1[[LLama.Abstractions.LoraAdapter, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]], System.Collections.IEnumerable, System.Collections.IList, System.Collections.ICollection, System.Collections.Generic.IReadOnlyList`1[[LLama.Abstractions.LoraAdapter, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]], System.Collections.Generic.IReadOnlyCollection`1[[LLama.Abstractions.LoraAdapter, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]], System.IEquatable`1[[LLama.Abstractions.AdapterCollection, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 List&lt;LoraAdapter&gt; \u2192 AdapterCollection Implements IList&lt;LoraAdapter&gt;, ICollection&lt;LoraAdapter&gt;, IEnumerable&lt;LoraAdapter&gt;, IEnumerable, IList, ICollection, IReadOnlyList&lt;LoraAdapter&gt;, IReadOnlyCollection&lt;LoraAdapter&gt;, IEquatable&lt;AdapterCollection&gt;</p>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.adaptercollection/#capacity","title":"Capacity","text":"<pre><code>public int Capacity { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#count","title":"Count","text":"<pre><code>public int Count { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#item","title":"Item","text":"<pre><code>public LoraAdapter Item { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#property-value_2","title":"Property Value","text":"<p>LoraAdapter</p>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.abstractions.adaptercollection/#adaptercollection_1","title":"AdapterCollection()","text":"<pre><code>public AdapterCollection()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.adaptercollection/#equalsadaptercollection","title":"Equals(AdapterCollection)","text":"<pre><code>public bool Equals(AdapterCollection other)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#parameters","title":"Parameters","text":"<p><code>other</code> AdapterCollection</p>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#parameters_1","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.adaptercollection/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/","title":"IContextParams","text":"<p>Namespace: LLama.Abstractions</p> <p>The parameters for initializing a LLama context from a model.</p> <pre><code>public interface IContextParams\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.icontextparams/#contextsize","title":"ContextSize","text":"<p>Model context size (n_ctx)</p> <pre><code>public abstract Nullable&lt;uint&gt; ContextSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#batchsize","title":"BatchSize","text":"<p>batch size for prompt processing (must be &gt;=32 to use BLAS) (n_batch)</p> <pre><code>public abstract uint BatchSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_1","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#seed","title":"Seed","text":"<p>Seed for the random number generator (seed)</p> <pre><code>public abstract uint Seed { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_2","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#embeddingmode","title":"EmbeddingMode","text":"<p>Whether to use embedding mode. (embedding) Note that if this is set to true,   The LLamaModel won't produce text response anymore.</p> <pre><code>public abstract bool EmbeddingMode { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#ropefrequencybase","title":"RopeFrequencyBase","text":"<p>RoPE base frequency (null to fetch from the model)</p> <pre><code>public abstract Nullable&lt;float&gt; RopeFrequencyBase { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_4","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#ropefrequencyscale","title":"RopeFrequencyScale","text":"<p>RoPE frequency scaling factor (null to fetch from the model)</p> <pre><code>public abstract Nullable&lt;float&gt; RopeFrequencyScale { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_5","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#encoding","title":"Encoding","text":"<p>The encoding to use for models</p> <pre><code>public abstract Encoding Encoding { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_6","title":"Property Value","text":"<p>Encoding</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#threads","title":"Threads","text":"<p>Number of threads (null = autodetect) (n_threads)</p> <pre><code>public abstract Nullable&lt;uint&gt; Threads { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_7","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#batchthreads","title":"BatchThreads","text":"<p>Number of threads to use for batch processing (null = autodetect) (n_threads)</p> <pre><code>public abstract Nullable&lt;uint&gt; BatchThreads { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_8","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnextrapolationfactor","title":"YarnExtrapolationFactor","text":"<p>YaRN extrapolation mix factor (null = from model)</p> <pre><code>public abstract Nullable&lt;float&gt; YarnExtrapolationFactor { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_9","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnattentionfactor","title":"YarnAttentionFactor","text":"<p>YaRN magnitude scaling factor (null = from model)</p> <pre><code>public abstract Nullable&lt;float&gt; YarnAttentionFactor { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_10","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnbetafast","title":"YarnBetaFast","text":"<p>YaRN low correction dim (null = from model)</p> <pre><code>public abstract Nullable&lt;float&gt; YarnBetaFast { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_11","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnbetaslow","title":"YarnBetaSlow","text":"<p>YaRN high correction dim (null = from model)</p> <pre><code>public abstract Nullable&lt;float&gt; YarnBetaSlow { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_12","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnoriginalcontext","title":"YarnOriginalContext","text":"<p>YaRN original context length (null = from model)</p> <pre><code>public abstract Nullable&lt;uint&gt; YarnOriginalContext { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_13","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnscalingtype","title":"YarnScalingType","text":"<p>YaRN scaling method to use.</p> <pre><code>public abstract Nullable&lt;RopeScalingType&gt; YarnScalingType { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_14","title":"Property Value","text":"<p>Nullable&lt;RopeScalingType&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#typek","title":"TypeK","text":"<p>Override the type of the K cache</p> <pre><code>public abstract Nullable&lt;GGMLType&gt; TypeK { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_15","title":"Property Value","text":"<p>Nullable&lt;GGMLType&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#typev","title":"TypeV","text":"<p>Override the type of the V cache</p> <pre><code>public abstract Nullable&lt;GGMLType&gt; TypeV { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_16","title":"Property Value","text":"<p>Nullable&lt;GGMLType&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#nokqvoffload","title":"NoKqvOffload","text":"<p>Whether to disable offloading the KQV cache to the GPU</p> <pre><code>public abstract bool NoKqvOffload { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_17","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#defragthreshold","title":"DefragThreshold","text":"<p>defragment the KV cache if holes/size &gt; defrag_threshold, Set to &lt; 0 to disable (default)</p> <pre><code>public abstract float DefragThreshold { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_18","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#dopooling","title":"DoPooling","text":"<p>Whether to pool (sum) embedding results by sequence id (ignored if no pooling layer)</p> <pre><code>public abstract bool DoPooling { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_19","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/","title":"IHistoryTransform","text":"<p>Namespace: LLama.Abstractions</p> <p>Transform history to plain text and vice versa.</p> <pre><code>public interface IHistoryTransform\n</code></pre>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.ihistorytransform/#historytotextchathistory","title":"HistoryToText(ChatHistory)","text":"<p>Convert a ChatHistory instance to plain text.</p> <pre><code>string HistoryToText(ChatHistory history)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#parameters","title":"Parameters","text":"<p><code>history</code> ChatHistory The ChatHistory instance</p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#texttohistoryauthorrole-string","title":"TextToHistory(AuthorRole, String)","text":"<p>Converts plain text to a ChatHistory instance.</p> <pre><code>ChatHistory TextToHistory(AuthorRole role, string text)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#parameters_1","title":"Parameters","text":"<p><code>role</code> AuthorRole The role for the author.</p> <p><code>text</code> String The chat history as plain text.</p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#returns_1","title":"Returns","text":"<p>ChatHistory The updated history.</p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#clone","title":"Clone()","text":"<p>Copy the transform.</p> <pre><code>IHistoryTransform Clone()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#returns_2","title":"Returns","text":"<p>IHistoryTransform</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/","title":"IInferenceParams","text":"<p>Namespace: LLama.Abstractions</p> <p>The parameters used for inference.</p> <pre><code>public interface IInferenceParams\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.iinferenceparams/#tokenskeep","title":"TokensKeep","text":"<p>number of tokens to keep from initial prompt</p> <pre><code>public abstract int TokensKeep { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#maxtokens","title":"MaxTokens","text":"<p>how many new tokens to predict (n_predict), set to -1 to inifinitely generate response  until it complete.</p> <pre><code>public abstract int MaxTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#logitbias","title":"LogitBias","text":"<p>logit bias for specific tokens</p> <pre><code>public abstract Dictionary&lt;LLamaToken, float&gt; LogitBias { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_2","title":"Property Value","text":"<p>Dictionary&lt;LLamaToken, Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#antiprompts","title":"AntiPrompts","text":"<p>Sequences where the model will stop generating further tokens.</p> <pre><code>public abstract IReadOnlyList&lt;string&gt; AntiPrompts { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_3","title":"Property Value","text":"<p>IReadOnlyList&lt;String&gt;</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#topk","title":"TopK","text":"<p>0 or lower to use vocab size</p> <pre><code>public abstract int TopK { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_4","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#topp","title":"TopP","text":"<p>1.0 = disabled</p> <pre><code>public abstract float TopP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_5","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#minp","title":"MinP","text":"<p>0.0 = disabled</p> <pre><code>public abstract float MinP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_6","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#tfsz","title":"TfsZ","text":"<p>1.0 = disabled</p> <pre><code>public abstract float TfsZ { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_7","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#typicalp","title":"TypicalP","text":"<p>1.0 = disabled</p> <pre><code>public abstract float TypicalP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_8","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#temperature","title":"Temperature","text":"<p>1.0 = disabled</p> <pre><code>public abstract float Temperature { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_9","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#repeatpenalty","title":"RepeatPenalty","text":"<p>1.0 = disabled</p> <pre><code>public abstract float RepeatPenalty { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_10","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#repeatlasttokenscount","title":"RepeatLastTokensCount","text":"<p>last n tokens to penalize (0 = disable penalty, -1 = context size) (repeat_last_n)</p> <pre><code>public abstract int RepeatLastTokensCount { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_11","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#frequencypenalty","title":"FrequencyPenalty","text":"<p>frequency penalty coefficient  0.0 = disabled</p> <pre><code>public abstract float FrequencyPenalty { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_12","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#presencepenalty","title":"PresencePenalty","text":"<p>presence penalty coefficient  0.0 = disabled</p> <pre><code>public abstract float PresencePenalty { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_13","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#mirostat","title":"Mirostat","text":"<p>Mirostat uses tokens instead of words.  algorithm described in the paper https://arxiv.org/abs/2007.14966.  0 = disabled, 1 = mirostat, 2 = mirostat 2.0</p> <pre><code>public abstract MirostatType Mirostat { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_14","title":"Property Value","text":"<p>MirostatType</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#mirostattau","title":"MirostatTau","text":"<p>target entropy</p> <pre><code>public abstract float MirostatTau { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_15","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#mirostateta","title":"MirostatEta","text":"<p>learning rate</p> <pre><code>public abstract float MirostatEta { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_16","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#penalizenl","title":"PenalizeNL","text":"<p>consider newlines as a repeatable token (penalize_nl)</p> <pre><code>public abstract bool PenalizeNL { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_17","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#grammar","title":"Grammar","text":"<p>Grammar to constrain possible tokens</p> <pre><code>public abstract SafeLLamaGrammarHandle Grammar { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_18","title":"Property Value","text":"<p>SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#samplingpipeline","title":"SamplingPipeline","text":"<p>Set a custom sampling pipeline to use. If this is set All other sampling parameters are ignored!</p> <pre><code>public abstract ISamplingPipeline SamplingPipeline { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_19","title":"Property Value","text":"<p>ISamplingPipeline</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/","title":"ILLamaExecutor","text":"<p>Namespace: LLama.Abstractions</p> <p>A high level interface for LLama models.</p> <pre><code>public interface ILLamaExecutor\n</code></pre>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.illamaexecutor/#context","title":"Context","text":"<p>The loaded context for this executor.</p> <pre><code>public abstract LLamaContext Context { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#property-value","title":"Property Value","text":"<p>LLamaContext</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#ismultimodal","title":"IsMultiModal","text":"<p>Identify if it's a multi-modal model and there is a image to process.</p> <pre><code>public abstract bool IsMultiModal { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#clipmodel","title":"ClipModel","text":"<p>Muti-Modal Projections / Clip Model weights</p> <pre><code>public abstract LLavaWeights ClipModel { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#property-value_2","title":"Property Value","text":"<p>LLavaWeights</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#imagepaths","title":"ImagePaths","text":"<p>List of images: Image filename and path (jpeg images).</p> <pre><code>public abstract List&lt;string&gt; ImagePaths { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#property-value_3","title":"Property Value","text":"<p>List&lt;String&gt;</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.illamaexecutor/#inferasyncstring-iinferenceparams-cancellationtoken","title":"InferAsync(String, IInferenceParams, CancellationToken)","text":"<p>Asynchronously infers a response from the model.</p> <pre><code>IAsyncEnumerable&lt;string&gt; InferAsync(string text, IInferenceParams inferenceParams, CancellationToken token)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#parameters","title":"Parameters","text":"<p><code>text</code> String Your prompt</p> <p><code>inferenceParams</code> IInferenceParams Any additional parameters</p> <p><code>token</code> CancellationToken A cancellation token.</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#returns","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.abstractions.illamaparams/","title":"ILLamaParams","text":"<p>Namespace: LLama.Abstractions</p> <p>Convenience interface for implementing both type of parameters.</p> <pre><code>public interface ILLamaParams : IModelParams, IContextParams\n</code></pre> <p>Implements IModelParams, IContextParams</p> <p>Remarks:</p> <p>Mostly exists for backwards compatibility reasons, when these two were not split.</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/","title":"IModelParams","text":"<p>Namespace: LLama.Abstractions</p> <p>The parameters for initializing a LLama model.</p> <pre><code>public interface IModelParams\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.imodelparams/#maingpu","title":"MainGpu","text":"<p>main_gpu interpretation depends on split_mode:  NoneThe GPU that is used for the entire mode.RowThe GPU that is used for small tensors and intermediate results.LayerIgnored.</p> <pre><code>public abstract int MainGpu { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#splitmode","title":"SplitMode","text":"<p>How to split the model across multiple GPUs</p> <pre><code>public abstract GPUSplitMode SplitMode { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_1","title":"Property Value","text":"<p>GPUSplitMode</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#gpulayercount","title":"GpuLayerCount","text":"<p>Number of layers to run in VRAM / GPU memory (n_gpu_layers)</p> <pre><code>public abstract int GpuLayerCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#usememorymap","title":"UseMemorymap","text":"<p>Use mmap for faster loads (use_mmap)</p> <pre><code>public abstract bool UseMemorymap { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#usememorylock","title":"UseMemoryLock","text":"<p>Use mlock to keep model in memory (use_mlock)</p> <pre><code>public abstract bool UseMemoryLock { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_4","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#modelpath","title":"ModelPath","text":"<p>Model path (model)</p> <pre><code>public abstract string ModelPath { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#tensorsplits","title":"TensorSplits","text":"<p>how split tensors should be distributed across GPUs</p> <pre><code>public abstract TensorSplitsCollection TensorSplits { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_6","title":"Property Value","text":"<p>TensorSplitsCollection</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#vocabonly","title":"VocabOnly","text":"<p>Load vocab only (no weights)</p> <pre><code>public abstract bool VocabOnly { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_7","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#loraadapters","title":"LoraAdapters","text":"<p>List of LoRA adapters to apply</p> <pre><code>public abstract AdapterCollection LoraAdapters { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_8","title":"Property Value","text":"<p>AdapterCollection</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#lorabase","title":"LoraBase","text":"<p>base model path for the lora adapter (lora_base)</p> <pre><code>public abstract string LoraBase { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_9","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#metadataoverrides","title":"MetadataOverrides","text":"<p>Override specific metadata items in the model</p> <pre><code>public abstract List&lt;MetadataOverride&gt; MetadataOverrides { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_10","title":"Property Value","text":"<p>List&lt;MetadataOverride&gt;</p>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/","title":"ITextStreamTransform","text":"<p>Namespace: LLama.Abstractions</p> <p>Takes a stream of tokens and transforms them.</p> <pre><code>public interface ITextStreamTransform\n</code></pre>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#transformasynciasyncenumerablestring","title":"TransformAsync(IAsyncEnumerable&lt;String&gt;)","text":"<p>Takes a stream of tokens and transforms them, returning a new stream of tokens asynchronously.</p> <pre><code>IAsyncEnumerable&lt;string&gt; TransformAsync(IAsyncEnumerable&lt;string&gt; tokens)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#parameters","title":"Parameters","text":"<p><code>tokens</code> IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#returns","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#clone","title":"Clone()","text":"<p>Copy the transform.</p> <pre><code>ITextStreamTransform Clone()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#returns_1","title":"Returns","text":"<p>ITextStreamTransform</p>"},{"location":"xmldocs/llama.abstractions.itexttransform/","title":"ITextTransform","text":"<p>Namespace: LLama.Abstractions</p> <p>An interface for text transformations.  These can be used to compose a pipeline of text transformations, such as:  - Tokenization  - Lowercasing  - Punctuation removal  - Trimming  - etc.</p> <pre><code>public interface ITextTransform\n</code></pre>"},{"location":"xmldocs/llama.abstractions.itexttransform/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.itexttransform/#transformstring","title":"Transform(String)","text":"<p>Takes a string and transforms it.</p> <pre><code>string Transform(string text)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.itexttransform/#parameters","title":"Parameters","text":"<p><code>text</code> String</p>"},{"location":"xmldocs/llama.abstractions.itexttransform/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.itexttransform/#clone","title":"Clone()","text":"<p>Copy the transform.</p> <pre><code>ITextTransform Clone()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.itexttransform/#returns_1","title":"Returns","text":"<p>ITextTransform</p>"},{"location":"xmldocs/llama.abstractions.loraadapter/","title":"LoraAdapter","text":"<p>Namespace: LLama.Abstractions</p> <p>A LoRA adapter to apply to a model</p> <pre><code>public struct LoraAdapter\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LoraAdapter Implements IEquatable&lt;LoraAdapter&gt;</p>"},{"location":"xmldocs/llama.abstractions.loraadapter/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.loraadapter/#path","title":"Path","text":"<p>Path to the LoRA file</p> <pre><code>public string Path { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.loraadapter/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.loraadapter/#scale","title":"Scale","text":"<p>Strength of this LoRA</p> <pre><code>public float Scale { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.loraadapter/#property-value_1","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.loraadapter/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.abstractions.loraadapter/#loraadapterstring-single","title":"LoraAdapter(String, Single)","text":"<p>A LoRA adapter to apply to a model</p> <pre><code>LoraAdapter(string Path, float Scale)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.loraadapter/#parameters","title":"Parameters","text":"<p><code>Path</code> String Path to the LoRA file</p> <p><code>Scale</code> Single Strength of this LoRA</p>"},{"location":"xmldocs/llama.abstractions.loraadapter/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.loraadapter/#tostring","title":"ToString()","text":"<pre><code>string ToString()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.loraadapter/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.loraadapter/#gethashcode","title":"GetHashCode()","text":"<pre><code>int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.loraadapter/#returns_1","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.loraadapter/#equalsobject","title":"Equals(Object)","text":"<pre><code>bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.loraadapter/#parameters_1","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.abstractions.loraadapter/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.loraadapter/#equalsloraadapter","title":"Equals(LoraAdapter)","text":"<pre><code>bool Equals(LoraAdapter other)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.loraadapter/#parameters_2","title":"Parameters","text":"<p><code>other</code> LoraAdapter</p>"},{"location":"xmldocs/llama.abstractions.loraadapter/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.loraadapter/#deconstructstring-single","title":"Deconstruct(String&amp;, Single&amp;)","text":"<pre><code>void Deconstruct(String&amp; Path, Single&amp; Scale)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.loraadapter/#parameters_3","title":"Parameters","text":"<p><code>Path</code> String&amp;</p> <p><code>Scale</code> Single&amp;</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/","title":"MetadataOverride","text":"<p>Namespace: LLama.Abstractions</p> <p>An override for a single key/value pair in model metadata</p> <pre><code>public sealed class MetadataOverride : System.IEquatable`1[[LLama.Abstractions.MetadataOverride, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 MetadataOverride Implements IEquatable&lt;MetadataOverride&gt;</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.metadataoverride/#key","title":"Key","text":"<p>Get the key being overriden by this override</p> <pre><code>public string Key { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.abstractions.metadataoverride/#metadataoverridestring-int32","title":"MetadataOverride(String, Int32)","text":"<p>Create a new override for an int key</p> <pre><code>public MetadataOverride(string key, int value)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters","title":"Parameters","text":"<p><code>key</code> String</p> <p><code>value</code> Int32</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#metadataoverridestring-single","title":"MetadataOverride(String, Single)","text":"<p>Create a new override for a float key</p> <pre><code>public MetadataOverride(string key, float value)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters_1","title":"Parameters","text":"<p><code>key</code> String</p> <p><code>value</code> Single</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#metadataoverridestring-boolean","title":"MetadataOverride(String, Boolean)","text":"<p>Create a new override for a boolean key</p> <pre><code>public MetadataOverride(string key, bool value)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters_2","title":"Parameters","text":"<p><code>key</code> String</p> <p><code>value</code> Boolean</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.metadataoverride/#writevaluellamamodelmetadataoverride","title":"WriteValue(LLamaModelMetadataOverride&amp;)","text":"<pre><code>internal void WriteValue(LLamaModelMetadataOverride&amp; dest)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters_3","title":"Parameters","text":"<p><code>dest</code> LLamaModelMetadataOverride&amp;</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#writevalueutf8jsonwriter","title":"WriteValue(Utf8JsonWriter)","text":"<pre><code>internal void WriteValue(Utf8JsonWriter writer)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters_4","title":"Parameters","text":"<p><code>writer</code> Utf8JsonWriter</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#returns_1","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters_5","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#equalsmetadataoverride","title":"Equals(MetadataOverride)","text":"<pre><code>public bool Equals(MetadataOverride other)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters_6","title":"Parameters","text":"<p><code>other</code> MetadataOverride</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public MetadataOverride &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#returns_4","title":"Returns","text":"<p>MetadataOverride</p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/","title":"MetadataOverrideConverter","text":"<p>Namespace: LLama.Abstractions</p> <p>A JSON converter for MetadataOverride</p> <pre><code>public class MetadataOverrideConverter : System.Text.Json.Serialization.JsonConverter`1[[LLama.Abstractions.MetadataOverride, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 JsonConverter \u2192 JsonConverter&lt;MetadataOverride&gt; \u2192 MetadataOverrideConverter</p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#handlenull","title":"HandleNull","text":"<pre><code>public bool HandleNull { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#metadataoverrideconverter_1","title":"MetadataOverrideConverter()","text":"<pre><code>public MetadataOverrideConverter()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#readutf8jsonreader-type-jsonserializeroptions","title":"Read(Utf8JsonReader&amp;, Type, JsonSerializerOptions)","text":"<pre><code>public MetadataOverride Read(Utf8JsonReader&amp; reader, Type typeToConvert, JsonSerializerOptions options)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#parameters","title":"Parameters","text":"<p><code>reader</code> Utf8JsonReader&amp;</p> <p><code>typeToConvert</code> Type</p> <p><code>options</code> JsonSerializerOptions</p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#returns","title":"Returns","text":"<p>MetadataOverride</p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#writeutf8jsonwriter-metadataoverride-jsonserializeroptions","title":"Write(Utf8JsonWriter, MetadataOverride, JsonSerializerOptions)","text":"<pre><code>public void Write(Utf8JsonWriter writer, MetadataOverride value, JsonSerializerOptions options)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#parameters_1","title":"Parameters","text":"<p><code>writer</code> Utf8JsonWriter</p> <p><code>value</code> MetadataOverride</p> <p><code>options</code> JsonSerializerOptions</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/","title":"TensorSplitsCollection","text":"<p>Namespace: LLama.Abstractions</p> <p>A fixed size array to set the tensor splits across multiple GPUs</p> <pre><code>public sealed class TensorSplitsCollection : System.Collections.Generic.IEnumerable`1[[System.Single, System.Private.CoreLib, Version=6.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Collections.IEnumerable\n</code></pre> <p>Inheritance Object \u2192 TensorSplitsCollection Implements IEnumerable&lt;Single&gt;, IEnumerable</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#length","title":"Length","text":"<p>The size of this array</p> <pre><code>public int Length { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#item","title":"Item","text":"<pre><code>public float Item { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#property-value_1","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#tensorsplitscollectionsingle","title":"TensorSplitsCollection(Single[])","text":"<p>Create a new tensor splits collection, copying the given values</p> <pre><code>public TensorSplitsCollection(Single[] splits)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#parameters","title":"Parameters","text":"<p><code>splits</code> Single[]</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#exceptions","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#tensorsplitscollection_1","title":"TensorSplitsCollection()","text":"<p>Create a new tensor splits collection with all values initialised to the default</p> <pre><code>public TensorSplitsCollection()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#clear","title":"Clear()","text":"<p>Set all values to zero</p> <pre><code>public void Clear()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#pin","title":"Pin()","text":"<pre><code>internal MemoryHandle Pin()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#returns","title":"Returns","text":"<p>MemoryHandle</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#getenumerator","title":"GetEnumerator()","text":"<pre><code>public IEnumerator&lt;float&gt; GetEnumerator()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#returns_1","title":"Returns","text":"<p>IEnumerator&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/","title":"TensorSplitsCollectionConverter","text":"<p>Namespace: LLama.Abstractions</p> <p>A JSON converter for TensorSplitsCollection</p> <pre><code>public class TensorSplitsCollectionConverter : System.Text.Json.Serialization.JsonConverter`1[[LLama.Abstractions.TensorSplitsCollection, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 JsonConverter \u2192 JsonConverter&lt;TensorSplitsCollection&gt; \u2192 TensorSplitsCollectionConverter</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#handlenull","title":"HandleNull","text":"<pre><code>public bool HandleNull { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#tensorsplitscollectionconverter_1","title":"TensorSplitsCollectionConverter()","text":"<pre><code>public TensorSplitsCollectionConverter()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#readutf8jsonreader-type-jsonserializeroptions","title":"Read(Utf8JsonReader&amp;, Type, JsonSerializerOptions)","text":"<pre><code>public TensorSplitsCollection Read(Utf8JsonReader&amp; reader, Type typeToConvert, JsonSerializerOptions options)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#parameters","title":"Parameters","text":"<p><code>reader</code> Utf8JsonReader&amp;</p> <p><code>typeToConvert</code> Type</p> <p><code>options</code> JsonSerializerOptions</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#returns","title":"Returns","text":"<p>TensorSplitsCollection</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#writeutf8jsonwriter-tensorsplitscollection-jsonserializeroptions","title":"Write(Utf8JsonWriter, TensorSplitsCollection, JsonSerializerOptions)","text":"<pre><code>public void Write(Utf8JsonWriter writer, TensorSplitsCollection value, JsonSerializerOptions options)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#parameters_1","title":"Parameters","text":"<p><code>writer</code> Utf8JsonWriter</p> <p><code>value</code> TensorSplitsCollection</p> <p><code>options</code> JsonSerializerOptions</p>"},{"location":"xmldocs/llama.antipromptprocessor/","title":"AntipromptProcessor","text":"<p>Namespace: LLama</p> <p>AntipromptProcessor keeps track of past tokens looking for any set Anti-Prompts</p> <pre><code>public sealed class AntipromptProcessor\n</code></pre> <p>Inheritance Object \u2192 AntipromptProcessor</p>"},{"location":"xmldocs/llama.antipromptprocessor/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.antipromptprocessor/#antipromptprocessorienumerablestring","title":"AntipromptProcessor(IEnumerable&lt;String&gt;)","text":"<p>Initializes a new instance of the AntipromptProcessor class.</p> <pre><code>public AntipromptProcessor(IEnumerable&lt;string&gt; antiprompts)\n</code></pre>"},{"location":"xmldocs/llama.antipromptprocessor/#parameters","title":"Parameters","text":"<p><code>antiprompts</code> IEnumerable&lt;String&gt; The antiprompts.</p>"},{"location":"xmldocs/llama.antipromptprocessor/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.antipromptprocessor/#addantipromptstring","title":"AddAntiprompt(String)","text":"<p>Add an antiprompt to the collection</p> <pre><code>public void AddAntiprompt(string antiprompt)\n</code></pre>"},{"location":"xmldocs/llama.antipromptprocessor/#parameters_1","title":"Parameters","text":"<p><code>antiprompt</code> String</p>"},{"location":"xmldocs/llama.antipromptprocessor/#setantipromptsienumerablestring","title":"SetAntiprompts(IEnumerable&lt;String&gt;)","text":"<p>Overwrite all current antiprompts with a new set</p> <pre><code>public void SetAntiprompts(IEnumerable&lt;string&gt; antiprompts)\n</code></pre>"},{"location":"xmldocs/llama.antipromptprocessor/#parameters_2","title":"Parameters","text":"<p><code>antiprompts</code> IEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.antipromptprocessor/#addstring","title":"Add(String)","text":"<p>Add some text and check if the buffer now ends with any antiprompt</p> <pre><code>public bool Add(string text)\n</code></pre>"},{"location":"xmldocs/llama.antipromptprocessor/#parameters_3","title":"Parameters","text":"<p><code>text</code> String</p>"},{"location":"xmldocs/llama.antipromptprocessor/#returns","title":"Returns","text":"<p>Boolean true if the text buffer ends with any antiprompt</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/","title":"AlreadyPromptedConversationException","text":"<p>Namespace: LLama.Batched</p> <p>This exception is thrown when \"Prompt()\" is called on a Conversation which has  already been prompted and before \"Infer()\" has been called on the associated  BatchedExecutor.</p> <pre><code>public class AlreadyPromptedConversationException : ExperimentalBatchedExecutorException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException \u2192 AlreadyPromptedConversationException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/","title":"BatchedExecutor","text":"<p>Namespace: LLama.Batched</p> <p>A batched executor that can infer multiple separate \"conversations\" simultaneously.</p> <pre><code>public sealed class BatchedExecutor : System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 BatchedExecutor Implements IDisposable</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.batchedexecutor/#context","title":"Context","text":"<p>The LLamaContext this executor is using</p> <pre><code>public LLamaContext Context { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#property-value","title":"Property Value","text":"<p>LLamaContext</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#model","title":"Model","text":"<p>The LLamaWeights this executor is using</p> <pre><code>public LLamaWeights Model { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#property-value_1","title":"Property Value","text":"<p>LLamaWeights</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#batchedtokencount","title":"BatchedTokenCount","text":"<p>Get the number of tokens in the batch, waiting for BatchedExecutor.Infer(CancellationToken) to be called</p> <pre><code>public int BatchedTokenCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#isdisposed","title":"IsDisposed","text":"<p>Check if this executor has been disposed.</p> <pre><code>public bool IsDisposed { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.batched.batchedexecutor/#batchedexecutorllamaweights-icontextparams","title":"BatchedExecutor(LLamaWeights, IContextParams)","text":"<p>Create a new batched executor</p> <pre><code>public BatchedExecutor(LLamaWeights model, IContextParams contextParams)\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#parameters","title":"Parameters","text":"<p><code>model</code> LLamaWeights The model to use</p> <p><code>contextParams</code> IContextParams Parameters to create a new context</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.batched.batchedexecutor/#promptstring","title":"Prompt(String)","text":""},{"location":"xmldocs/llama.batched.batchedexecutor/#caution","title":"Caution","text":"<p>Use BatchedExecutor.Create instead</p> <p>Start a new Conversation with the given prompt</p> <pre><code>public Conversation Prompt(string prompt)\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#parameters_1","title":"Parameters","text":"<p><code>prompt</code> String</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#returns","title":"Returns","text":"<p>Conversation</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#create","title":"Create()","text":"<p>Start a new Conversation</p> <pre><code>public Conversation Create()\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#returns_1","title":"Returns","text":"<p>Conversation</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#infercancellationtoken","title":"Infer(CancellationToken)","text":"<p>Run inference for all conversations in the batch which have pending tokens.</p> <p>If the result is <code>NoKvSlot</code> then there is not enough memory for inference, try disposing some conversation  threads and running inference again.</p> <pre><code>public Task&lt;DecodeResult&gt; Infer(CancellationToken cancellation)\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#parameters_2","title":"Parameters","text":"<p><code>cancellation</code> CancellationToken</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#returns_2","title":"Returns","text":"<p>Task&lt;DecodeResult&gt;</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#dispose","title":"Dispose()","text":"<pre><code>public void Dispose()\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#getnextsequenceid","title":"GetNextSequenceId()","text":"<pre><code>internal LLamaSeqId GetNextSequenceId()\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#returns_3","title":"Returns","text":"<p>LLamaSeqId</p>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/","title":"CannotForkWhileRequiresInferenceException","text":"<p>Namespace: LLama.Batched</p> <p>This exception is thrown when Conversation.Fork() is called when Conversation.RequiresInference = true</p> <pre><code>public class CannotForkWhileRequiresInferenceException : ExperimentalBatchedExecutorException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException \u2192 CannotForkWhileRequiresInferenceException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/","title":"CannotModifyWhileRequiresInferenceException","text":"<p>Namespace: LLama.Batched</p> <p>This exception is thrown when Conversation.Modify(ModifyKvCache) is called when Conversation.RequiresInference = true</p> <pre><code>public class CannotModifyWhileRequiresInferenceException : ExperimentalBatchedExecutorException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException \u2192 CannotModifyWhileRequiresInferenceException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/","title":"CannotSampleRequiresInferenceException","text":"<p>Namespace: LLama.Batched</p> <p>This exception is thrown when \"Sample()\" is called on a Conversation which has  already been prompted and before \"Infer()\" has been called on the associated  BatchedExecutor.</p> <pre><code>public class CannotSampleRequiresInferenceException : ExperimentalBatchedExecutorException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException \u2192 CannotSampleRequiresInferenceException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/","title":"CannotSampleRequiresPromptException","text":"<p>Namespace: LLama.Batched</p> <p>This exception is thrown when \"Sample()\" is called on a Conversation which was not  first prompted.  BatchedExecutor.</p> <pre><code>public class CannotSampleRequiresPromptException : ExperimentalBatchedExecutorException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException \u2192 CannotSampleRequiresPromptException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.conversation/","title":"Conversation","text":"<p>Namespace: LLama.Batched</p> <p>A single conversation thread that can be prompted (adding tokens from the user) or inferred (extracting a token from the LLM)</p> <pre><code>public sealed class Conversation : System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 Conversation Implements IDisposable</p>"},{"location":"xmldocs/llama.batched.conversation/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.conversation/#executor","title":"Executor","text":"<p>The executor which this conversation belongs to</p> <pre><code>public BatchedExecutor Executor { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value","title":"Property Value","text":"<p>BatchedExecutor</p>"},{"location":"xmldocs/llama.batched.conversation/#conversationid","title":"ConversationId","text":"<p>Unique ID for this conversation</p> <pre><code>public LLamaSeqId ConversationId { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value_1","title":"Property Value","text":"<p>LLamaSeqId</p>"},{"location":"xmldocs/llama.batched.conversation/#tokencount","title":"TokenCount","text":"<p>Total number of tokens in this conversation, cannot exceed the context length.</p> <pre><code>public int TokenCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.conversation/#isdisposed","title":"IsDisposed","text":"<p>Indicates if this conversation has been disposed, nothing can be done with a disposed conversation</p> <pre><code>public bool IsDisposed { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.batched.conversation/#requiresinference","title":"RequiresInference","text":"<p>Indicates if this conversation is waiting for inference to be run on the executor. \"Prompt\" and \"Sample\" cannot be called when this is true.</p> <pre><code>public bool RequiresInference { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value_4","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.batched.conversation/#requiressampling","title":"RequiresSampling","text":"<p>Indicates that this conversation should be sampled.</p> <pre><code>public bool RequiresSampling { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value_5","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.batched.conversation/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.batched.conversation/#finalize","title":"Finalize()","text":"<p>Finalizer for Conversation</p> <pre><code>protected void Finalize()\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#dispose","title":"Dispose()","text":"<p>End this conversation, freeing all resources used by it</p> <pre><code>public void Dispose()\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#exceptions","title":"Exceptions","text":"<p>ObjectDisposedException</p>"},{"location":"xmldocs/llama.batched.conversation/#fork","title":"Fork()","text":"<p>Create a copy of the current conversation</p> <pre><code>public Conversation Fork()\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#returns","title":"Returns","text":"<p>Conversation</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_1","title":"Exceptions","text":"<p>ObjectDisposedException</p> <p>Remarks:</p> <p>The copy shares internal state, so consumes very little extra memory.</p>"},{"location":"xmldocs/llama.batched.conversation/#sample","title":"Sample()","text":"<p>Get the logits from this conversation, ready for sampling</p> <pre><code>public Span&lt;float&gt; Sample()\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#returns_1","title":"Returns","text":"<p>Span&lt;Single&gt;</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_2","title":"Exceptions","text":"<p>ObjectDisposedException</p> <p>CannotSampleRequiresPromptException Thrown if this conversation was not prompted before the previous call to infer</p> <p>CannotSampleRequiresInferenceException Thrown if Infer() must be called on the executor</p>"},{"location":"xmldocs/llama.batched.conversation/#promptstring","title":"Prompt(String)","text":"<p>Add tokens to this conversation</p> <pre><code>public void Prompt(string input)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters","title":"Parameters","text":"<p><code>input</code> String</p>"},{"location":"xmldocs/llama.batched.conversation/#promptlistllamatoken","title":"Prompt(List&lt;LLamaToken&gt;)","text":"<p>Add tokens to this conversation</p> <pre><code>public void Prompt(List&lt;LLamaToken&gt; tokens)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_1","title":"Parameters","text":"<p><code>tokens</code> List&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_3","title":"Exceptions","text":"<p>ObjectDisposedException</p> <p>AlreadyPromptedConversationException</p>"},{"location":"xmldocs/llama.batched.conversation/#promptreadonlyspanllamatoken","title":"Prompt(ReadOnlySpan&lt;LLamaToken&gt;)","text":"<p>Add tokens to this conversation</p> <pre><code>public void Prompt(ReadOnlySpan&lt;LLamaToken&gt; tokens)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_2","title":"Parameters","text":"<p><code>tokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_4","title":"Exceptions","text":"<p>ObjectDisposedException</p> <p>AlreadyPromptedConversationException</p>"},{"location":"xmldocs/llama.batched.conversation/#promptllamatoken","title":"Prompt(LLamaToken)","text":"<p>Add a single token to this conversation</p> <pre><code>public void Prompt(LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_3","title":"Parameters","text":"<p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_5","title":"Exceptions","text":"<p>ObjectDisposedException</p> <p>AlreadyPromptedConversationException</p>"},{"location":"xmldocs/llama.batched.conversation/#modifymodifykvcache","title":"Modify(ModifyKvCache)","text":"<p>Directly modify the KV cache of this conversation</p> <pre><code>public void Modify(ModifyKvCache modifier)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_4","title":"Parameters","text":"<p><code>modifier</code> ModifyKvCache</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_6","title":"Exceptions","text":"<p>CannotModifyWhileRequiresInferenceException Thrown if this method is called while Conversation.RequiresInference == true</p>"},{"location":"xmldocs/llama.batched.conversationextensions/","title":"ConversationExtensions","text":"<p>Namespace: LLama.Batched</p> <p>Extension method for Conversation</p> <pre><code>public static class ConversationExtensions\n</code></pre> <p>Inheritance Object \u2192 ConversationExtensions</p>"},{"location":"xmldocs/llama.batched.conversationextensions/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.batched.conversationextensions/#rewindconversation-int32","title":"Rewind(Conversation, Int32)","text":"<p>Rewind a Conversation back to an earlier state by removing tokens from the end</p> <pre><code>public static void Rewind(Conversation conversation, int tokens)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversationextensions/#parameters","title":"Parameters","text":"<p><code>conversation</code> Conversation The conversation to rewind</p> <p><code>tokens</code> Int32 The number of tokens to rewind</p>"},{"location":"xmldocs/llama.batched.conversationextensions/#exceptions","title":"Exceptions","text":"<p>ArgumentOutOfRangeException Thrown if <code>tokens</code> parameter is larger than TokenCount</p>"},{"location":"xmldocs/llama.batched.conversationextensions/#shiftleftconversation-int32-int32","title":"ShiftLeft(Conversation, Int32, Int32)","text":"<p>Shift all tokens over to the left, removing \"count\" tokens from the start and shifting everything over.  Leaves \"keep\" tokens at the start completely untouched. This can be used to free up space when the context  gets full, keeping the prompt at the start intact.</p> <pre><code>public static void ShiftLeft(Conversation conversation, int count, int keep)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversationextensions/#parameters_1","title":"Parameters","text":"<p><code>conversation</code> Conversation The conversation to rewind</p> <p><code>count</code> Int32 How much to shift tokens over by</p> <p><code>keep</code> Int32 The number of tokens at the start which should not be shifted</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/","title":"ExperimentalBatchedExecutorException","text":"<p>Namespace: LLama.Batched</p> <p>Base class for exceptions thrown from BatchedExecutor</p> <pre><code>public class ExperimentalBatchedExecutorException : System.Exception, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.chatsession-1/","title":"ChatSession&lt;T&gt;","text":"<p>Namespace: LLama</p> <pre><code>public class ChatSession&lt;T&gt;\n</code></pre>"},{"location":"xmldocs/llama.chatsession-1/#type-parameters","title":"Type Parameters","text":"<p><code>T</code></p> <p>Inheritance Object \u2192 ChatSession&lt;T&gt;</p>"},{"location":"xmldocs/llama.chatsession-1/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.chatsession-1/#chatsessiont_1","title":"ChatSession(T)","text":"<pre><code>public ChatSession(T model)\n</code></pre>"},{"location":"xmldocs/llama.chatsession-1/#parameters","title":"Parameters","text":"<p><code>model</code> T</p>"},{"location":"xmldocs/llama.chatsession-1/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.chatsession-1/#chatstring-string","title":"Chat(String, String)","text":"<pre><code>public IEnumerable&lt;string&gt; Chat(string text, string prompt)\n</code></pre>"},{"location":"xmldocs/llama.chatsession-1/#parameters_1","title":"Parameters","text":"<p><code>text</code> String</p> <p><code>prompt</code> String</p>"},{"location":"xmldocs/llama.chatsession-1/#returns","title":"Returns","text":"<p>IEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.chatsession-1/#withpromptstring","title":"WithPrompt(String)","text":"<pre><code>public ChatSession&lt;T&gt; WithPrompt(string prompt)\n</code></pre>"},{"location":"xmldocs/llama.chatsession-1/#parameters_2","title":"Parameters","text":"<p><code>prompt</code> String</p>"},{"location":"xmldocs/llama.chatsession-1/#returns_1","title":"Returns","text":"<p>ChatSession&lt;T&gt;</p>"},{"location":"xmldocs/llama.chatsession-1/#withpromptfilestring","title":"WithPromptFile(String)","text":"<pre><code>public ChatSession&lt;T&gt; WithPromptFile(string promptFilename)\n</code></pre>"},{"location":"xmldocs/llama.chatsession-1/#parameters_3","title":"Parameters","text":"<p><code>promptFilename</code> String</p>"},{"location":"xmldocs/llama.chatsession-1/#returns_2","title":"Returns","text":"<p>ChatSession&lt;T&gt;</p>"},{"location":"xmldocs/llama.chatsession-1/#withantipromptstring","title":"WithAntiprompt(String[])","text":"<pre><code>public ChatSession&lt;T&gt; WithAntiprompt(String[] antiprompt)\n</code></pre>"},{"location":"xmldocs/llama.chatsession-1/#parameters_4","title":"Parameters","text":"<p><code>antiprompt</code> String[]</p>"},{"location":"xmldocs/llama.chatsession-1/#returns_3","title":"Returns","text":"<p>ChatSession&lt;T&gt;</p>"},{"location":"xmldocs/llama.chatsession/","title":"ChatSession","text":"<p>Namespace: LLama</p> <p>The main chat session class.</p> <pre><code>public class ChatSession\n</code></pre> <p>Inheritance Object \u2192 ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.chatsession/#outputtransform","title":"OutputTransform","text":"<p>The output transform used in this session.</p> <pre><code>public ITextStreamTransform OutputTransform;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#model_state_filename","title":"MODEL_STATE_FILENAME","text":"<p>The filename for the serialized model state (KV cache, etc).</p> <pre><code>public static string MODEL_STATE_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#executor_state_filename","title":"EXECUTOR_STATE_FILENAME","text":"<p>The filename for the serialized executor state.</p> <pre><code>public static string EXECUTOR_STATE_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#history_state_filename","title":"HISTORY_STATE_FILENAME","text":"<p>The filename for the serialized chat history.</p> <pre><code>public static string HISTORY_STATE_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#input_transform_filename","title":"INPUT_TRANSFORM_FILENAME","text":"<p>The filename for the serialized input transform pipeline.</p> <pre><code>public static string INPUT_TRANSFORM_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#output_transform_filename","title":"OUTPUT_TRANSFORM_FILENAME","text":"<p>The filename for the serialized output transform.</p> <pre><code>public static string OUTPUT_TRANSFORM_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#history_transform_filename","title":"HISTORY_TRANSFORM_FILENAME","text":"<p>The filename for the serialized history transform.</p> <pre><code>public static string HISTORY_TRANSFORM_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.chatsession/#executor","title":"Executor","text":"<p>The executor for this session.</p> <pre><code>public ILLamaExecutor Executor { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#property-value","title":"Property Value","text":"<p>ILLamaExecutor</p>"},{"location":"xmldocs/llama.chatsession/#history","title":"History","text":"<p>The chat history for this session.</p> <pre><code>public ChatHistory History { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#property-value_1","title":"Property Value","text":"<p>ChatHistory</p>"},{"location":"xmldocs/llama.chatsession/#historytransform","title":"HistoryTransform","text":"<p>The history transform used in this session.</p> <pre><code>public IHistoryTransform HistoryTransform { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#property-value_2","title":"Property Value","text":"<p>IHistoryTransform</p>"},{"location":"xmldocs/llama.chatsession/#inputtransformpipeline","title":"InputTransformPipeline","text":"<p>The input transform pipeline used in this session.</p> <pre><code>public List&lt;ITextTransform&gt; InputTransformPipeline { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#property-value_3","title":"Property Value","text":"<p>List&lt;ITextTransform&gt;</p>"},{"location":"xmldocs/llama.chatsession/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.chatsession/#chatsessionillamaexecutor","title":"ChatSession(ILLamaExecutor)","text":"<p>Create a new chat session.</p> <pre><code>public ChatSession(ILLamaExecutor executor)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters","title":"Parameters","text":"<p><code>executor</code> ILLamaExecutor The executor for this session</p>"},{"location":"xmldocs/llama.chatsession/#chatsessionillamaexecutor-chathistory","title":"ChatSession(ILLamaExecutor, ChatHistory)","text":"<p>Create a new chat session with a custom history.</p> <pre><code>public ChatSession(ILLamaExecutor executor, ChatHistory history)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_1","title":"Parameters","text":"<p><code>executor</code> ILLamaExecutor</p> <p><code>history</code> ChatHistory</p>"},{"location":"xmldocs/llama.chatsession/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.chatsession/#initializesessionfromhistoryasyncillamaexecutor-chathistory","title":"InitializeSessionFromHistoryAsync(ILLamaExecutor, ChatHistory)","text":"<p>Create a new chat session and preprocess history.</p> <pre><code>public static Task&lt;ChatSession&gt; InitializeSessionFromHistoryAsync(ILLamaExecutor executor, ChatHistory history)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_2","title":"Parameters","text":"<p><code>executor</code> ILLamaExecutor The executor for this session</p> <p><code>history</code> ChatHistory History for this session</p>"},{"location":"xmldocs/llama.chatsession/#returns","title":"Returns","text":"<p>Task&lt;ChatSession&gt;</p>"},{"location":"xmldocs/llama.chatsession/#withhistorytransformihistorytransform","title":"WithHistoryTransform(IHistoryTransform)","text":"<p>Use a custom history transform.</p> <pre><code>public ChatSession WithHistoryTransform(IHistoryTransform transform)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_3","title":"Parameters","text":"<p><code>transform</code> IHistoryTransform</p>"},{"location":"xmldocs/llama.chatsession/#returns_1","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#addinputtransformitexttransform","title":"AddInputTransform(ITextTransform)","text":"<p>Add a text transform to the input transform pipeline.</p> <pre><code>public ChatSession AddInputTransform(ITextTransform transform)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_4","title":"Parameters","text":"<p><code>transform</code> ITextTransform</p>"},{"location":"xmldocs/llama.chatsession/#returns_2","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#withoutputtransformitextstreamtransform","title":"WithOutputTransform(ITextStreamTransform)","text":"<p>Use a custom output transform.</p> <pre><code>public ChatSession WithOutputTransform(ITextStreamTransform transform)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_5","title":"Parameters","text":"<p><code>transform</code> ITextStreamTransform</p>"},{"location":"xmldocs/llama.chatsession/#returns_3","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#savesessionstring","title":"SaveSession(String)","text":"<p>Save a session from a directory.</p> <pre><code>public void SaveSession(string path)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_6","title":"Parameters","text":"<p><code>path</code> String</p>"},{"location":"xmldocs/llama.chatsession/#exceptions","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.chatsession/#getsessionstate","title":"GetSessionState()","text":"<p>Get the session state.</p> <pre><code>public SessionState GetSessionState()\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#returns_4","title":"Returns","text":"<p>SessionState SessionState object representing session state in-memory</p>"},{"location":"xmldocs/llama.chatsession/#loadsessionsessionstate-boolean","title":"LoadSession(SessionState, Boolean)","text":"<p>Load a session from a session state.</p> <pre><code>public void LoadSession(SessionState state, bool loadTransforms)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_7","title":"Parameters","text":"<p><code>state</code> SessionState</p> <p><code>loadTransforms</code> Boolean If true loads transforms saved in the session state.</p>"},{"location":"xmldocs/llama.chatsession/#exceptions_1","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.chatsession/#loadsessionstring-boolean","title":"LoadSession(String, Boolean)","text":"<p>Load a session from a directory.</p> <pre><code>public void LoadSession(string path, bool loadTransforms)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_8","title":"Parameters","text":"<p><code>path</code> String</p> <p><code>loadTransforms</code> Boolean If true loads transforms saved in the session state.</p>"},{"location":"xmldocs/llama.chatsession/#exceptions_2","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.chatsession/#addmessagemessage","title":"AddMessage(Message)","text":"<p>Add a message to the chat history.</p> <pre><code>public ChatSession AddMessage(Message message)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_9","title":"Parameters","text":"<p><code>message</code> Message</p>"},{"location":"xmldocs/llama.chatsession/#returns_5","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#addsystemmessagestring","title":"AddSystemMessage(String)","text":"<p>Add a system message to the chat history.</p> <pre><code>public ChatSession AddSystemMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_10","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_6","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#addassistantmessagestring","title":"AddAssistantMessage(String)","text":"<p>Add an assistant message to the chat history.</p> <pre><code>public ChatSession AddAssistantMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_11","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_7","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#addusermessagestring","title":"AddUserMessage(String)","text":"<p>Add a user message to the chat history.</p> <pre><code>public ChatSession AddUserMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_12","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_8","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#removelastmessage","title":"RemoveLastMessage()","text":"<p>Remove the last message from the chat history.</p> <pre><code>public ChatSession RemoveLastMessage()\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#returns_9","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#addandprocessmessagemessage","title":"AddAndProcessMessage(Message)","text":"<p>Compute KV cache for the message and add it to the chat history.</p> <pre><code>public Task&lt;ChatSession&gt; AddAndProcessMessage(Message message)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_13","title":"Parameters","text":"<p><code>message</code> Message</p>"},{"location":"xmldocs/llama.chatsession/#returns_10","title":"Returns","text":"<p>Task&lt;ChatSession&gt;</p>"},{"location":"xmldocs/llama.chatsession/#addandprocesssystemmessagestring","title":"AddAndProcessSystemMessage(String)","text":"<p>Compute KV cache for the system message and add it to the chat history.</p> <pre><code>public Task&lt;ChatSession&gt; AddAndProcessSystemMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_14","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_11","title":"Returns","text":"<p>Task&lt;ChatSession&gt;</p>"},{"location":"xmldocs/llama.chatsession/#addandprocessusermessagestring","title":"AddAndProcessUserMessage(String)","text":"<p>Compute KV cache for the user message and add it to the chat history.</p> <pre><code>public Task&lt;ChatSession&gt; AddAndProcessUserMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_15","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_12","title":"Returns","text":"<p>Task&lt;ChatSession&gt;</p>"},{"location":"xmldocs/llama.chatsession/#addandprocessassistantmessagestring","title":"AddAndProcessAssistantMessage(String)","text":"<p>Compute KV cache for the assistant message and add it to the chat history.</p> <pre><code>public Task&lt;ChatSession&gt; AddAndProcessAssistantMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_16","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_13","title":"Returns","text":"<p>Task&lt;ChatSession&gt;</p>"},{"location":"xmldocs/llama.chatsession/#replaceusermessagemessage-message","title":"ReplaceUserMessage(Message, Message)","text":"<p>Replace a user message with a new message and remove all messages after the new message.  This is useful when the user wants to edit a message. And regenerate the response.</p> <pre><code>public ChatSession ReplaceUserMessage(Message oldMessage, Message newMessage)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_17","title":"Parameters","text":"<p><code>oldMessage</code> Message</p> <p><code>newMessage</code> Message</p>"},{"location":"xmldocs/llama.chatsession/#returns_14","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#chatasyncmessage-boolean-iinferenceparams-cancellationtoken","title":"ChatAsync(Message, Boolean, IInferenceParams, CancellationToken)","text":"<p>Chat with the model.</p> <pre><code>public IAsyncEnumerable&lt;string&gt; ChatAsync(Message message, bool applyInputTransformPipeline, IInferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_18","title":"Parameters","text":"<p><code>message</code> Message</p> <p><code>applyInputTransformPipeline</code> Boolean</p> <p><code>inferenceParams</code> IInferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.chatsession/#returns_15","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.chatsession/#exceptions_3","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.chatsession/#chatasyncmessage-iinferenceparams-cancellationtoken","title":"ChatAsync(Message, IInferenceParams, CancellationToken)","text":"<p>Chat with the model.</p> <pre><code>public IAsyncEnumerable&lt;string&gt; ChatAsync(Message message, IInferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_19","title":"Parameters","text":"<p><code>message</code> Message</p> <p><code>inferenceParams</code> IInferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.chatsession/#returns_16","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.chatsession/#chatasyncchathistory-boolean-iinferenceparams-cancellationtoken","title":"ChatAsync(ChatHistory, Boolean, IInferenceParams, CancellationToken)","text":"<p>Chat with the model.</p> <pre><code>public IAsyncEnumerable&lt;string&gt; ChatAsync(ChatHistory history, bool applyInputTransformPipeline, IInferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_20","title":"Parameters","text":"<p><code>history</code> ChatHistory</p> <p><code>applyInputTransformPipeline</code> Boolean</p> <p><code>inferenceParams</code> IInferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.chatsession/#returns_17","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.chatsession/#exceptions_4","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.chatsession/#chatasyncchathistory-iinferenceparams-cancellationtoken","title":"ChatAsync(ChatHistory, IInferenceParams, CancellationToken)","text":"<p>Chat with the model.</p> <pre><code>public IAsyncEnumerable&lt;string&gt; ChatAsync(ChatHistory history, IInferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_21","title":"Parameters","text":"<p><code>history</code> ChatHistory</p> <p><code>inferenceParams</code> IInferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.chatsession/#returns_18","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.chatsession/#regenerateassistantmessageasyncinferenceparams-cancellationtoken","title":"RegenerateAssistantMessageAsync(InferenceParams, CancellationToken)","text":"<p>Regenerate the last assistant message.</p> <pre><code>public IAsyncEnumerable&lt;string&gt; RegenerateAssistantMessageAsync(InferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_22","title":"Parameters","text":"<p><code>inferenceParams</code> InferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.chatsession/#returns_19","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.chatsession/#exceptions_5","title":"Exceptions","text":"<p>InvalidOperationException</p>"},{"location":"xmldocs/llama.common.authorrole/","title":"AuthorRole","text":"<p>Namespace: LLama.Common</p> <p>Role of the message author, e.g. user/assistant/system</p> <pre><code>public enum AuthorRole\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 AuthorRole Implements IComparable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.common.authorrole/#fields","title":"Fields","text":"Name Value Description Unknown -1 Role is unknown System 0 Message comes from a \"system\" prompt, not written by a user or language model User 1 Message comes from the user Assistant 2 Messages was generated by the language model"},{"location":"xmldocs/llama.common.chathistory/","title":"ChatHistory","text":"<p>Namespace: LLama.Common</p> <p>The chat history class</p> <pre><code>public class ChatHistory\n</code></pre> <p>Inheritance Object \u2192 ChatHistory</p>"},{"location":"xmldocs/llama.common.chathistory/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.common.chathistory/#messages","title":"Messages","text":"<p>List of messages in the chat</p> <pre><code>public List&lt;Message&gt; Messages { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#property-value","title":"Property Value","text":"<p>List&lt;Message&gt;</p>"},{"location":"xmldocs/llama.common.chathistory/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.common.chathistory/#chathistory_1","title":"ChatHistory()","text":"<p>Create a new instance of the chat content class</p> <pre><code>public ChatHistory()\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#chathistorymessage","title":"ChatHistory(Message[])","text":"<p>Create a new instance of the chat history from array of messages</p> <pre><code>public ChatHistory(Message[] messageHistory)\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#parameters","title":"Parameters","text":"<p><code>messageHistory</code> Message[]</p>"},{"location":"xmldocs/llama.common.chathistory/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.common.chathistory/#addmessageauthorrole-string","title":"AddMessage(AuthorRole, String)","text":"<p>Add a message to the chat history</p> <pre><code>public void AddMessage(AuthorRole authorRole, string content)\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#parameters_1","title":"Parameters","text":"<p><code>authorRole</code> AuthorRole Role of the message author</p> <p><code>content</code> String Message content</p>"},{"location":"xmldocs/llama.common.chathistory/#tojson","title":"ToJson()","text":"<p>Serialize the chat history to JSON</p> <pre><code>public string ToJson()\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.common.chathistory/#fromjsonstring","title":"FromJson(String)","text":"<p>Deserialize a chat history from JSON</p> <pre><code>public static ChatHistory FromJson(string json)\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#parameters_2","title":"Parameters","text":"<p><code>json</code> String</p>"},{"location":"xmldocs/llama.common.chathistory/#returns_1","title":"Returns","text":"<p>ChatHistory</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/","title":"FixedSizeQueue&lt;T&gt;","text":"<p>Namespace: LLama.Common</p> <p>A queue with fixed storage size.  Currently it's only a naive implementation and needs to be further optimized in the future.</p> <pre><code>public class FixedSizeQueue&lt;T&gt; : , , , System.Collections.IEnumerable\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#type-parameters","title":"Type Parameters","text":"<p><code>T</code></p> <p>Inheritance Object \u2192 FixedSizeQueue&lt;T&gt; Implements IReadOnlyList&lt;T&gt;, IReadOnlyCollection&lt;T&gt;, IEnumerable&lt;T&gt;, IEnumerable</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.common.fixedsizequeue-1/#item","title":"Item","text":"<pre><code>public T Item { get; }\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#property-value","title":"Property Value","text":"<p>T</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#count","title":"Count","text":"<p>Number of items in this queue</p> <pre><code>public int Count { get; }\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#capacity","title":"Capacity","text":"<p>Maximum number of items allowed in this queue</p> <pre><code>public int Capacity { get; }\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.common.fixedsizequeue-1/#fixedsizequeueint32","title":"FixedSizeQueue(Int32)","text":"<p>Create a new queue</p> <pre><code>public FixedSizeQueue(int size)\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#parameters","title":"Parameters","text":"<p><code>size</code> Int32 the maximum number of items to store in this queue</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#fixedsizequeueint32-ienumerablet","title":"FixedSizeQueue(Int32, IEnumerable&lt;T&gt;)","text":"<p>Fill the quene with the data. Please ensure that data.Count &lt;= size</p> <pre><code>public FixedSizeQueue(int size, IEnumerable&lt;T&gt; data)\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#parameters_1","title":"Parameters","text":"<p><code>size</code> Int32</p> <p><code>data</code> IEnumerable&lt;T&gt;</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.common.fixedsizequeue-1/#enqueuet","title":"Enqueue(T)","text":"<p>Enquene an element.</p> <pre><code>public void Enqueue(T item)\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#parameters_2","title":"Parameters","text":"<p><code>item</code> T</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#getenumerator","title":"GetEnumerator()","text":"<pre><code>public IEnumerator&lt;T&gt; GetEnumerator()\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#returns","title":"Returns","text":"<p>IEnumerator&lt;T&gt;</p>"},{"location":"xmldocs/llama.common.inferenceparams/","title":"InferenceParams","text":"<p>Namespace: LLama.Common</p> <p>The parameters used for inference.</p> <pre><code>public class InferenceParams : LLama.Abstractions.IInferenceParams, System.IEquatable`1[[LLama.Common.InferenceParams, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 InferenceParams Implements IInferenceParams, IEquatable&lt;InferenceParams&gt;</p>"},{"location":"xmldocs/llama.common.inferenceparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.common.inferenceparams/#tokenskeep","title":"TokensKeep","text":"<p>number of tokens to keep from initial prompt</p> <pre><code>public int TokensKeep { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.inferenceparams/#maxtokens","title":"MaxTokens","text":"<p>how many new tokens to predict (n_predict), set to -1 to inifinitely generate response  until it complete.</p> <pre><code>public int MaxTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.inferenceparams/#logitbias","title":"LogitBias","text":"<p>logit bias for specific tokens</p> <pre><code>public Dictionary&lt;LLamaToken, float&gt; LogitBias { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_2","title":"Property Value","text":"<p>Dictionary&lt;LLamaToken, Single&gt;</p>"},{"location":"xmldocs/llama.common.inferenceparams/#antiprompts","title":"AntiPrompts","text":"<p>Sequences where the model will stop generating further tokens.</p> <pre><code>public IReadOnlyList&lt;string&gt; AntiPrompts { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_3","title":"Property Value","text":"<p>IReadOnlyList&lt;String&gt;</p>"},{"location":"xmldocs/llama.common.inferenceparams/#topk","title":"TopK","text":"<pre><code>public int TopK { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_4","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.inferenceparams/#topp","title":"TopP","text":"<pre><code>public float TopP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_5","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.common.inferenceparams/#minp","title":"MinP","text":"<pre><code>public float MinP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_6","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.common.inferenceparams/#tfsz","title":"TfsZ","text":"<pre><code>public float TfsZ { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_7","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.common.inferenceparams/#typicalp","title":"TypicalP","text":"<pre><code>public float TypicalP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_8","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.common.inferenceparams/#temperature","title":"Temperature","text":"<pre><code>public float Temperature { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_9","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.common.inferenceparams/#repeatpenalty","title":"RepeatPenalty","text":"<pre><code>public float RepeatPenalty { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_10","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.common.inferenceparams/#repeatlasttokenscount","title":"RepeatLastTokensCount","text":"<pre><code>public int RepeatLastTokensCount { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_11","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.inferenceparams/#frequencypenalty","title":"FrequencyPenalty","text":"<pre><code>public float FrequencyPenalty { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_12","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.common.inferenceparams/#presencepenalty","title":"PresencePenalty","text":"<pre><code>public float PresencePenalty { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_13","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.common.inferenceparams/#mirostat","title":"Mirostat","text":"<pre><code>public MirostatType Mirostat { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_14","title":"Property Value","text":"<p>MirostatType</p>"},{"location":"xmldocs/llama.common.inferenceparams/#mirostattau","title":"MirostatTau","text":"<pre><code>public float MirostatTau { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_15","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.common.inferenceparams/#mirostateta","title":"MirostatEta","text":"<pre><code>public float MirostatEta { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_16","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.common.inferenceparams/#penalizenl","title":"PenalizeNL","text":"<pre><code>public bool PenalizeNL { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_17","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.inferenceparams/#grammar","title":"Grammar","text":"<pre><code>public SafeLLamaGrammarHandle Grammar { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_18","title":"Property Value","text":"<p>SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.common.inferenceparams/#samplingpipeline","title":"SamplingPipeline","text":"<pre><code>public ISamplingPipeline SamplingPipeline { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_19","title":"Property Value","text":"<p>ISamplingPipeline</p>"},{"location":"xmldocs/llama.common.inferenceparams/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.common.inferenceparams/#inferenceparams_1","title":"InferenceParams()","text":"<pre><code>public InferenceParams()\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.common.inferenceparams/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.common.inferenceparams/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#parameters","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.common.inferenceparams/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.inferenceparams/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.inferenceparams/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#parameters_1","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.common.inferenceparams/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.inferenceparams/#equalsinferenceparams","title":"Equals(InferenceParams)","text":"<pre><code>public bool Equals(InferenceParams other)\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#parameters_2","title":"Parameters","text":"<p><code>other</code> InferenceParams</p>"},{"location":"xmldocs/llama.common.inferenceparams/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.inferenceparams/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public InferenceParams &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#returns_5","title":"Returns","text":"<p>InferenceParams</p>"},{"location":"xmldocs/llama.common.mirostattype/","title":"MirostatType","text":"<p>Namespace: LLama.Common</p> <p>Type of \"mirostat\" sampling to use.  https://github.com/basusourya/mirostat</p> <pre><code>public enum MirostatType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 MirostatType Implements IComparable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.common.mirostattype/#fields","title":"Fields","text":"Name Value Description Disable 0 Disable Mirostat sampling Mirostat 1 Original mirostat algorithm Mirostat2 2 Mirostat 2.0 algorithm"},{"location":"xmldocs/llama.common.modelparams/","title":"ModelParams","text":"<p>Namespace: LLama.Common</p> <p>The parameters for initializing a LLama model.</p> <pre><code>public class ModelParams : LLama.Abstractions.ILLamaParams, LLama.Abstractions.IModelParams, LLama.Abstractions.IContextParams, System.IEquatable`1[[LLama.Common.ModelParams, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 ModelParams Implements ILLamaParams, IModelParams, IContextParams, IEquatable&lt;ModelParams&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.common.modelparams/#contextsize","title":"ContextSize","text":"<pre><code>public Nullable&lt;uint&gt; ContextSize { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#maingpu","title":"MainGpu","text":"<pre><code>public int MainGpu { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.modelparams/#splitmode","title":"SplitMode","text":"<pre><code>public GPUSplitMode SplitMode { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_2","title":"Property Value","text":"<p>GPUSplitMode</p>"},{"location":"xmldocs/llama.common.modelparams/#gpulayercount","title":"GpuLayerCount","text":"<pre><code>public int GpuLayerCount { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_3","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.modelparams/#seed","title":"Seed","text":"<pre><code>public uint Seed { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_4","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.common.modelparams/#usememorymap","title":"UseMemorymap","text":"<pre><code>public bool UseMemorymap { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_5","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#usememorylock","title":"UseMemoryLock","text":"<pre><code>public bool UseMemoryLock { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_6","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#modelpath","title":"ModelPath","text":"<pre><code>public string ModelPath { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.common.modelparams/#loraadapters","title":"LoraAdapters","text":"<pre><code>public AdapterCollection LoraAdapters { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_8","title":"Property Value","text":"<p>AdapterCollection</p>"},{"location":"xmldocs/llama.common.modelparams/#lorabase","title":"LoraBase","text":"<pre><code>public string LoraBase { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_9","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.common.modelparams/#threads","title":"Threads","text":"<pre><code>public Nullable&lt;uint&gt; Threads { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_10","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#batchthreads","title":"BatchThreads","text":"<pre><code>public Nullable&lt;uint&gt; BatchThreads { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_11","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#batchsize","title":"BatchSize","text":"<pre><code>public uint BatchSize { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_12","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.common.modelparams/#embeddingmode","title":"EmbeddingMode","text":"<pre><code>public bool EmbeddingMode { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_13","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#tensorsplits","title":"TensorSplits","text":"<pre><code>public TensorSplitsCollection TensorSplits { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_14","title":"Property Value","text":"<p>TensorSplitsCollection</p>"},{"location":"xmldocs/llama.common.modelparams/#metadataoverrides","title":"MetadataOverrides","text":"<pre><code>public List&lt;MetadataOverride&gt; MetadataOverrides { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_15","title":"Property Value","text":"<p>List&lt;MetadataOverride&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#ropefrequencybase","title":"RopeFrequencyBase","text":"<pre><code>public Nullable&lt;float&gt; RopeFrequencyBase { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_16","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#ropefrequencyscale","title":"RopeFrequencyScale","text":"<pre><code>public Nullable&lt;float&gt; RopeFrequencyScale { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_17","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnextrapolationfactor","title":"YarnExtrapolationFactor","text":"<pre><code>public Nullable&lt;float&gt; YarnExtrapolationFactor { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_18","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnattentionfactor","title":"YarnAttentionFactor","text":"<pre><code>public Nullable&lt;float&gt; YarnAttentionFactor { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_19","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnbetafast","title":"YarnBetaFast","text":"<pre><code>public Nullable&lt;float&gt; YarnBetaFast { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_20","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnbetaslow","title":"YarnBetaSlow","text":"<pre><code>public Nullable&lt;float&gt; YarnBetaSlow { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_21","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnoriginalcontext","title":"YarnOriginalContext","text":"<pre><code>public Nullable&lt;uint&gt; YarnOriginalContext { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_22","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnscalingtype","title":"YarnScalingType","text":"<pre><code>public Nullable&lt;RopeScalingType&gt; YarnScalingType { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_23","title":"Property Value","text":"<p>Nullable&lt;RopeScalingType&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#typek","title":"TypeK","text":"<pre><code>public Nullable&lt;GGMLType&gt; TypeK { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_24","title":"Property Value","text":"<p>Nullable&lt;GGMLType&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#typev","title":"TypeV","text":"<pre><code>public Nullable&lt;GGMLType&gt; TypeV { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_25","title":"Property Value","text":"<p>Nullable&lt;GGMLType&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#nokqvoffload","title":"NoKqvOffload","text":"<pre><code>public bool NoKqvOffload { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_26","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#defragthreshold","title":"DefragThreshold","text":"<pre><code>public float DefragThreshold { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_27","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.common.modelparams/#dopooling","title":"DoPooling","text":"<pre><code>public bool DoPooling { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_28","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#vocabonly","title":"VocabOnly","text":"<pre><code>public bool VocabOnly { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_29","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#encoding","title":"Encoding","text":"<pre><code>public Encoding Encoding { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_30","title":"Property Value","text":"<p>Encoding</p>"},{"location":"xmldocs/llama.common.modelparams/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.common.modelparams/#modelparamsstring","title":"ModelParams(String)","text":"<pre><code>public ModelParams(string modelPath)\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#parameters","title":"Parameters","text":"<p><code>modelPath</code> String The model path.</p>"},{"location":"xmldocs/llama.common.modelparams/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.common.modelparams/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.common.modelparams/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.common.modelparams/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.modelparams/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.common.modelparams/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#equalsmodelparams","title":"Equals(ModelParams)","text":"<pre><code>public bool Equals(ModelParams other)\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#parameters_3","title":"Parameters","text":"<p><code>other</code> ModelParams</p>"},{"location":"xmldocs/llama.common.modelparams/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public ModelParams &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#returns_5","title":"Returns","text":"<p>ModelParams</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/","title":"GrammarExpectedName","text":"<p>Namespace: LLama.Exceptions</p> <p>Failed to parse a \"name\" element when one was expected</p> <pre><code>public class GrammarExpectedName : GrammarFormatException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 GrammarFormatException \u2192 GrammarExpectedName Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedname/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/","title":"GrammarExpectedNext","text":"<p>Namespace: LLama.Exceptions</p> <p>A specified string was expected when parsing</p> <pre><code>public class GrammarExpectedNext : GrammarFormatException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 GrammarFormatException \u2192 GrammarExpectedNext Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectednext/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/","title":"GrammarExpectedPrevious","text":"<p>Namespace: LLama.Exceptions</p> <p>A specified character was expected to preceded another when parsing</p> <pre><code>public class GrammarExpectedPrevious : GrammarFormatException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 GrammarFormatException \u2192 GrammarExpectedPrevious Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarexpectedprevious/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/","title":"GrammarFormatException","text":"<p>Namespace: LLama.Exceptions</p> <p>Base class for all grammar exceptions</p> <pre><code>public abstract class GrammarFormatException : System.Exception, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 GrammarFormatException Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.grammarformatexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarformatexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/","title":"GrammarUnexpectedCharAltElement","text":"<p>Namespace: LLama.Exceptions</p> <p>A CHAR_ALT was created without a preceding CHAR element</p> <pre><code>public class GrammarUnexpectedCharAltElement : GrammarFormatException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 GrammarFormatException \u2192 GrammarUnexpectedCharAltElement Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharaltelement/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/","title":"GrammarUnexpectedCharRngElement","text":"<p>Namespace: LLama.Exceptions</p> <p>A CHAR_RNG was created without a preceding CHAR element</p> <pre><code>public class GrammarUnexpectedCharRngElement : GrammarFormatException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 GrammarFormatException \u2192 GrammarUnexpectedCharRngElement Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedcharrngelement/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/","title":"GrammarUnexpectedEndElement","text":"<p>Namespace: LLama.Exceptions</p> <p>An END was encountered before the last element</p> <pre><code>public class GrammarUnexpectedEndElement : GrammarFormatException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 GrammarFormatException \u2192 GrammarUnexpectedEndElement Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendelement/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/","title":"GrammarUnexpectedEndOfInput","text":"<p>Namespace: LLama.Exceptions</p> <p>End-of-file was encountered while parsing</p> <pre><code>public class GrammarUnexpectedEndOfInput : GrammarFormatException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 GrammarFormatException \u2192 GrammarUnexpectedEndOfInput Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedendofinput/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/","title":"GrammarUnexpectedHexCharsCount","text":"<p>Namespace: LLama.Exceptions</p> <p>An incorrect number of characters were encountered while parsing a hex literal</p> <pre><code>public class GrammarUnexpectedHexCharsCount : GrammarFormatException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 GrammarFormatException \u2192 GrammarUnexpectedHexCharsCount Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunexpectedhexcharscount/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/","title":"GrammarUnknownEscapeCharacter","text":"<p>Namespace: LLama.Exceptions</p> <p>An unexpected character was encountered after an escape sequence</p> <pre><code>public class GrammarUnknownEscapeCharacter : GrammarFormatException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 GrammarFormatException \u2192 GrammarUnknownEscapeCharacter Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.grammarunknownescapecharacter/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/","title":"LLamaDecodeError","text":"<p>Namespace: LLama.Exceptions</p> <p><code>llama_decode</code> return a non-zero status code</p> <pre><code>public class LLamaDecodeError : RuntimeError, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 RuntimeError \u2192 LLamaDecodeError Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#returncode","title":"ReturnCode","text":"<p>The return status code</p> <pre><code>public DecodeResult ReturnCode { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value","title":"Property Value","text":"<p>DecodeResult</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_1","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_2","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_3","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_4","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_6","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_7","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_8","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#llamadecodeerrordecoderesult","title":"LLamaDecodeError(DecodeResult)","text":"<pre><code>public LLamaDecodeError(DecodeResult returnCode)\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#parameters","title":"Parameters","text":"<p><code>returnCode</code> DecodeResult</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/","title":"LoadWeightsFailedException","text":"<p>Namespace: LLama.Exceptions</p> <p>Loading model weights failed</p> <pre><code>public class LoadWeightsFailedException : RuntimeError, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 RuntimeError \u2192 LoadWeightsFailedException Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#modelpath","title":"ModelPath","text":"<p>The model path which failed to load</p> <pre><code>public string ModelPath { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_1","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_2","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_3","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_4","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_6","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_7","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_8","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#loadweightsfailedexceptionstring","title":"LoadWeightsFailedException(String)","text":"<pre><code>public LoadWeightsFailedException(string modelPath)\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#parameters","title":"Parameters","text":"<p><code>modelPath</code> String</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/","title":"RuntimeError","text":"<p>Namespace: LLama.Exceptions</p> <p>Base class for LLamaSharp runtime errors (i.e. errors produced by llama.cpp, converted into exceptions)</p> <pre><code>public class RuntimeError : System.Exception, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 RuntimeError Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.runtimeerror/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.exceptions.runtimeerror/#runtimeerrorstring","title":"RuntimeError(String)","text":"<p>Create a new RuntimeError</p> <pre><code>public RuntimeError(string message)\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#parameters","title":"Parameters","text":"<p><code>message</code> String</p>"},{"location":"xmldocs/llama.extensions.icontextparamsextensions/","title":"IContextParamsExtensions","text":"<p>Namespace: LLama.Extensions</p> <p>Extention methods to the IContextParams interface</p> <pre><code>public static class IContextParamsExtensions\n</code></pre> <p>Inheritance Object \u2192 IContextParamsExtensions</p>"},{"location":"xmldocs/llama.extensions.icontextparamsextensions/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.extensions.icontextparamsextensions/#tollamacontextparamsicontextparams-llamacontextparams","title":"ToLlamaContextParams(IContextParams, LLamaContextParams&amp;)","text":"<p>Convert the given <code>IModelParams</code> into a <code>LLamaContextParams</code></p> <pre><code>public static void ToLlamaContextParams(IContextParams params, LLamaContextParams&amp; result)\n</code></pre>"},{"location":"xmldocs/llama.extensions.icontextparamsextensions/#parameters","title":"Parameters","text":"<p><code>params</code> IContextParams</p> <p><code>result</code> LLamaContextParams&amp;</p>"},{"location":"xmldocs/llama.extensions.icontextparamsextensions/#exceptions","title":"Exceptions","text":"<p>FileNotFoundException</p> <p>ArgumentException</p>"},{"location":"xmldocs/llama.extensions.imodelparamsextensions/","title":"IModelParamsExtensions","text":"<p>Namespace: LLama.Extensions</p> <p>Extention methods to the IModelParams interface</p> <pre><code>public static class IModelParamsExtensions\n</code></pre> <p>Inheritance Object \u2192 IModelParamsExtensions</p>"},{"location":"xmldocs/llama.extensions.imodelparamsextensions/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.extensions.imodelparamsextensions/#tollamamodelparamsimodelparams-llamamodelparams","title":"ToLlamaModelParams(IModelParams, LLamaModelParams&amp;)","text":"<p>Convert the given <code>IModelParams</code> into a <code>LLamaModelParams</code></p> <pre><code>public static IDisposable ToLlamaModelParams(IModelParams params, LLamaModelParams&amp; result)\n</code></pre>"},{"location":"xmldocs/llama.extensions.imodelparamsextensions/#parameters","title":"Parameters","text":"<p><code>params</code> IModelParams</p> <p><code>result</code> LLamaModelParams&amp;</p>"},{"location":"xmldocs/llama.extensions.imodelparamsextensions/#returns","title":"Returns","text":"<p>IDisposable</p>"},{"location":"xmldocs/llama.extensions.imodelparamsextensions/#exceptions","title":"Exceptions","text":"<p>FileNotFoundException</p> <p>ArgumentException</p>"},{"location":"xmldocs/llama.grammars.grammar/","title":"Grammar","text":"<p>Namespace: LLama.Grammars</p> <p>A grammar is a set of GrammarRules for deciding which characters are valid next. Can be used to constrain  output to certain formats - e.g. force the model to output JSON</p> <pre><code>public sealed class Grammar\n</code></pre> <p>Inheritance Object \u2192 Grammar</p>"},{"location":"xmldocs/llama.grammars.grammar/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.grammars.grammar/#startruleindex","title":"StartRuleIndex","text":"<p>Index of the initial rule to start from</p> <pre><code>public ulong StartRuleIndex { get; }\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammar/#property-value","title":"Property Value","text":"<p>UInt64</p>"},{"location":"xmldocs/llama.grammars.grammar/#rules","title":"Rules","text":"<p>The rules which make up this grammar</p> <pre><code>public IReadOnlyList&lt;GrammarRule&gt; Rules { get; }\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammar/#property-value_1","title":"Property Value","text":"<p>IReadOnlyList&lt;GrammarRule&gt;</p>"},{"location":"xmldocs/llama.grammars.grammar/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.grammars.grammar/#grammarireadonlylistgrammarrule-uint64","title":"Grammar(IReadOnlyList&lt;GrammarRule&gt;, UInt64)","text":"<p>Create a new grammar from a set of rules</p> <pre><code>public Grammar(IReadOnlyList&lt;GrammarRule&gt; rules, ulong startRuleIndex)\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammar/#parameters","title":"Parameters","text":"<p><code>rules</code> IReadOnlyList&lt;GrammarRule&gt; The rules which make up this grammar</p> <p><code>startRuleIndex</code> UInt64 Index of the initial rule to start from</p>"},{"location":"xmldocs/llama.grammars.grammar/#exceptions","title":"Exceptions","text":"<p>ArgumentOutOfRangeException</p>"},{"location":"xmldocs/llama.grammars.grammar/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.grammars.grammar/#createinstance","title":"CreateInstance()","text":"<p>Create a <code>SafeLLamaGrammarHandle</code> instance to use for parsing</p> <pre><code>public SafeLLamaGrammarHandle CreateInstance()\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammar/#returns","title":"Returns","text":"<p>SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.grammars.grammar/#parsestring-string","title":"Parse(String, String)","text":"<p>Parse a string of GGML BNF into a Grammar</p> <pre><code>public static Grammar Parse(string gbnf, string startRule)\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammar/#parameters_1","title":"Parameters","text":"<p><code>gbnf</code> String The string to parse</p> <p><code>startRule</code> String Name of the start rule of this grammar</p>"},{"location":"xmldocs/llama.grammars.grammar/#returns_1","title":"Returns","text":"<p>Grammar A Grammar which can be converted into a SafeLLamaGrammarHandle for sampling</p>"},{"location":"xmldocs/llama.grammars.grammar/#exceptions_1","title":"Exceptions","text":"<p>GrammarFormatException Thrown if input is malformed</p>"},{"location":"xmldocs/llama.grammars.grammar/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammar/#returns_2","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.grammars.grammarrule/","title":"GrammarRule","text":"<p>Namespace: LLama.Grammars</p> <p>A single rule in a Grammar</p> <pre><code>public sealed class GrammarRule : System.IEquatable`1[[LLama.Grammars.GrammarRule, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 GrammarRule Implements IEquatable&lt;GrammarRule&gt;</p>"},{"location":"xmldocs/llama.grammars.grammarrule/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.grammars.grammarrule/#name","title":"Name","text":"<p>Name of this rule</p> <pre><code>public string Name { get; }\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammarrule/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.grammars.grammarrule/#elements","title":"Elements","text":"<p>The elements of this grammar rule</p> <pre><code>public IReadOnlyList&lt;LLamaGrammarElement&gt; Elements { get; }\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammarrule/#property-value_1","title":"Property Value","text":"<p>IReadOnlyList&lt;LLamaGrammarElement&gt;</p>"},{"location":"xmldocs/llama.grammars.grammarrule/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.grammars.grammarrule/#grammarrulestring-ireadonlylistllamagrammarelement","title":"GrammarRule(String, IReadOnlyList&lt;LLamaGrammarElement&gt;)","text":"<p>Create a new GrammarRule containing the given elements</p> <pre><code>public GrammarRule(string name, IReadOnlyList&lt;LLamaGrammarElement&gt; elements)\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammarrule/#parameters","title":"Parameters","text":"<p><code>name</code> String</p> <p><code>elements</code> IReadOnlyList&lt;LLamaGrammarElement&gt;</p>"},{"location":"xmldocs/llama.grammars.grammarrule/#exceptions","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.grammars.grammarrule/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.grammars.grammarrule/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammarrule/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.grammars.grammarrule/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammarrule/#returns_1","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.grammars.grammarrule/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammarrule/#parameters_1","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.grammars.grammarrule/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.grammars.grammarrule/#equalsgrammarrule","title":"Equals(GrammarRule)","text":"<pre><code>public bool Equals(GrammarRule other)\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammarrule/#parameters_2","title":"Parameters","text":"<p><code>other</code> GrammarRule</p>"},{"location":"xmldocs/llama.grammars.grammarrule/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.grammars.grammarrule/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public GrammarRule &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.grammars.grammarrule/#returns_4","title":"Returns","text":"<p>GrammarRule</p>"},{"location":"xmldocs/llama.ichatmodel/","title":"IChatModel","text":"<p>Namespace: LLama</p> <pre><code>public interface IChatModel\n</code></pre>"},{"location":"xmldocs/llama.ichatmodel/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.ichatmodel/#name","title":"Name","text":"<pre><code>public abstract string Name { get; }\n</code></pre>"},{"location":"xmldocs/llama.ichatmodel/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.ichatmodel/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.ichatmodel/#chatstring-string","title":"Chat(String, String)","text":"<pre><code>IEnumerable&lt;string&gt; Chat(string text, string prompt)\n</code></pre>"},{"location":"xmldocs/llama.ichatmodel/#parameters","title":"Parameters","text":"<p><code>text</code> String</p> <p><code>prompt</code> String</p>"},{"location":"xmldocs/llama.ichatmodel/#returns","title":"Returns","text":"<p>IEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.ichatmodel/#initchatpromptstring","title":"InitChatPrompt(String)","text":"<pre><code>void InitChatPrompt(string prompt)\n</code></pre>"},{"location":"xmldocs/llama.ichatmodel/#parameters_1","title":"Parameters","text":"<p><code>prompt</code> String</p>"},{"location":"xmldocs/llama.ichatmodel/#initchatantipromptstring","title":"InitChatAntiprompt(String[])","text":"<pre><code>void InitChatAntiprompt(String[] antiprompt)\n</code></pre>"},{"location":"xmldocs/llama.ichatmodel/#parameters_2","title":"Parameters","text":"<p><code>antiprompt</code> String[]</p>"},{"location":"xmldocs/llama.llamacache/","title":"LLamaCache","text":"<p>Namespace: LLama</p> <pre><code>public class LLamaCache\n</code></pre> <p>Inheritance Object \u2192 LLamaCache</p>"},{"location":"xmldocs/llama.llamacache/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.llamacache/#cachesize","title":"CacheSize","text":"<pre><code>public int CacheSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamacache/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamacache/#item","title":"Item","text":"<pre><code>public LLamaState Item { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.llamacache/#property-value_1","title":"Property Value","text":"<p>LLamaState</p>"},{"location":"xmldocs/llama.llamacache/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamacache/#llamacacheint32","title":"LLamaCache(Int32)","text":"<pre><code>public LLamaCache(int capacity)\n</code></pre>"},{"location":"xmldocs/llama.llamacache/#parameters","title":"Parameters","text":"<p><code>capacity</code> Int32</p>"},{"location":"xmldocs/llama.llamacache/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamacache/#containsint32","title":"Contains(Int32[])","text":"<pre><code>public bool Contains(Int32[] key)\n</code></pre>"},{"location":"xmldocs/llama.llamacache/#parameters_1","title":"Parameters","text":"<p><code>key</code> Int32[]</p>"},{"location":"xmldocs/llama.llamacache/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.llamaembedder/","title":"LLamaEmbedder","text":"<p>Namespace: LLama</p> <p>The embedder for LLama, which supports getting embeddings from text.</p> <pre><code>public class LLamaEmbedder\n</code></pre> <p>Inheritance Object \u2192 LLamaEmbedder</p>"},{"location":"xmldocs/llama.llamaembedder/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamaembedder/#llamaembedderllamaparams","title":"LLamaEmbedder(LLamaParams)","text":"<pre><code>public LLamaEmbedder(LLamaParams params)\n</code></pre>"},{"location":"xmldocs/llama.llamaembedder/#parameters","title":"Parameters","text":"<p><code>params</code> LLamaParams</p>"},{"location":"xmldocs/llama.llamaembedder/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamaembedder/#getembeddingsstring-int32-boolean","title":"GetEmbeddings(String, Int32, Boolean)","text":"<pre><code>public Single[] GetEmbeddings(string text, int n_thread, bool add_bos)\n</code></pre>"},{"location":"xmldocs/llama.llamaembedder/#parameters_1","title":"Parameters","text":"<p><code>text</code> String</p> <p><code>n_thread</code> Int32</p> <p><code>add_bos</code> Boolean</p>"},{"location":"xmldocs/llama.llamaembedder/#returns","title":"Returns","text":"<p>Single[]</p>"},{"location":"xmldocs/llama.llamamodel/","title":"LLamaModel","text":"<p>Namespace: LLama</p> <pre><code>public class LLamaModel : IChatModel\n</code></pre> <p>Inheritance Object \u2192 LLamaModel Implements IChatModel</p>"},{"location":"xmldocs/llama.llamamodel/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.llamamodel/#name","title":"Name","text":"<pre><code>public string Name { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.llamamodel/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.llamamodel/#nativehandle","title":"NativeHandle","text":"<pre><code>public SafeLLamaContextHandle NativeHandle { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamamodel/#property-value_1","title":"Property Value","text":"<p>SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.llamamodel/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamamodel/#llamamodelstring-string-boolean-boolean-int32-int32-int32-int32-int32-int32-int32-dictionaryint32-single-int32-single-single-single-single-single-int32-single-single-int32-single-single-string-string-string-string-liststring-string-string-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean","title":"LLamaModel(String, String, Boolean, Boolean, Int32, Int32, Int32, Int32, Int32, Int32, Int32, Dictionary&lt;Int32, Single&gt;, Int32, Single, Single, Single, Single, Single, Int32, Single, Single, Int32, Single, Single, String, String, String, String, List&lt;String&gt;, String, String, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean)","text":"<pre><code>public LLamaModel(string model_path, string model_name, bool echo_input, bool verbose, int seed, int n_threads, int n_predict, int n_parts, int n_ctx, int n_batch, int n_keep, Dictionary&lt;int, float&gt; logit_bias, int top_k, float top_p, float tfs_z, float typical_p, float temp, float repeat_penalty, int repeat_last_n, float frequency_penalty, float presence_penalty, int mirostat, float mirostat_tau, float mirostat_eta, string prompt, string path_session, string input_prefix, string input_suffix, List&lt;string&gt; antiprompt, string lora_adapter, string lora_base, bool memory_f16, bool random_prompt, bool use_color, bool interactive, bool embedding, bool interactive_first, bool instruct, bool penalize_nl, bool perplexity, bool use_mmap, bool use_mlock, bool mem_test, bool verbose_prompt)\n</code></pre>"},{"location":"xmldocs/llama.llamamodel/#parameters","title":"Parameters","text":"<p><code>model_path</code> String</p> <p><code>model_name</code> String</p> <p><code>echo_input</code> Boolean</p> <p><code>verbose</code> Boolean</p> <p><code>seed</code> Int32</p> <p><code>n_threads</code> Int32</p> <p><code>n_predict</code> Int32</p> <p><code>n_parts</code> Int32</p> <p><code>n_ctx</code> Int32</p> <p><code>n_batch</code> Int32</p> <p><code>n_keep</code> Int32</p> <p><code>logit_bias</code> Dictionary&lt;Int32, Single&gt;</p> <p><code>top_k</code> Int32</p> <p><code>top_p</code> Single</p> <p><code>tfs_z</code> Single</p> <p><code>typical_p</code> Single</p> <p><code>temp</code> Single</p> <p><code>repeat_penalty</code> Single</p> <p><code>repeat_last_n</code> Int32</p> <p><code>frequency_penalty</code> Single</p> <p><code>presence_penalty</code> Single</p> <p><code>mirostat</code> Int32</p> <p><code>mirostat_tau</code> Single</p> <p><code>mirostat_eta</code> Single</p> <p><code>prompt</code> String</p> <p><code>path_session</code> String</p> <p><code>input_prefix</code> String</p> <p><code>input_suffix</code> String</p> <p><code>antiprompt</code> List&lt;String&gt;</p> <p><code>lora_adapter</code> String</p> <p><code>lora_base</code> String</p> <p><code>memory_f16</code> Boolean</p> <p><code>random_prompt</code> Boolean</p> <p><code>use_color</code> Boolean</p> <p><code>interactive</code> Boolean</p> <p><code>embedding</code> Boolean</p> <p><code>interactive_first</code> Boolean</p> <p><code>instruct</code> Boolean</p> <p><code>penalize_nl</code> Boolean</p> <p><code>perplexity</code> Boolean</p> <p><code>use_mmap</code> Boolean</p> <p><code>use_mlock</code> Boolean</p> <p><code>mem_test</code> Boolean</p> <p><code>verbose_prompt</code> Boolean</p>"},{"location":"xmldocs/llama.llamamodel/#llamamodelllamaparams-string-boolean-boolean","title":"LLamaModel(LLamaParams, String, Boolean, Boolean)","text":"<pre><code>public LLamaModel(LLamaParams params, string name, bool echo_input, bool verbose)\n</code></pre>"},{"location":"xmldocs/llama.llamamodel/#parameters_1","title":"Parameters","text":"<p><code>params</code> LLamaParams</p> <p><code>name</code> String</p> <p><code>echo_input</code> Boolean</p> <p><code>verbose</code> Boolean</p>"},{"location":"xmldocs/llama.llamamodel/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamamodel/#withpromptstring","title":"WithPrompt(String)","text":"<pre><code>public LLamaModel WithPrompt(string prompt)\n</code></pre>"},{"location":"xmldocs/llama.llamamodel/#parameters_2","title":"Parameters","text":"<p><code>prompt</code> String</p>"},{"location":"xmldocs/llama.llamamodel/#returns","title":"Returns","text":"<p>LLamaModel</p>"},{"location":"xmldocs/llama.llamamodel/#withpromptfilestring","title":"WithPromptFile(String)","text":"<pre><code>public LLamaModel WithPromptFile(string promptFileName)\n</code></pre>"},{"location":"xmldocs/llama.llamamodel/#parameters_3","title":"Parameters","text":"<p><code>promptFileName</code> String</p>"},{"location":"xmldocs/llama.llamamodel/#returns_1","title":"Returns","text":"<p>LLamaModel</p>"},{"location":"xmldocs/llama.llamamodel/#initchatpromptstring","title":"InitChatPrompt(String)","text":"<pre><code>public void InitChatPrompt(string prompt)\n</code></pre>"},{"location":"xmldocs/llama.llamamodel/#parameters_4","title":"Parameters","text":"<p><code>prompt</code> String</p>"},{"location":"xmldocs/llama.llamamodel/#initchatantipromptstring","title":"InitChatAntiprompt(String[])","text":"<pre><code>public void InitChatAntiprompt(String[] antiprompt)\n</code></pre>"},{"location":"xmldocs/llama.llamamodel/#parameters_5","title":"Parameters","text":"<p><code>antiprompt</code> String[]</p>"},{"location":"xmldocs/llama.llamamodel/#chatstring-string","title":"Chat(String, String)","text":"<pre><code>public IEnumerable&lt;string&gt; Chat(string text, string prompt)\n</code></pre>"},{"location":"xmldocs/llama.llamamodel/#parameters_6","title":"Parameters","text":"<p><code>text</code> String</p> <p><code>prompt</code> String</p>"},{"location":"xmldocs/llama.llamamodel/#returns_2","title":"Returns","text":"<p>IEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.llamamodel/#callstring","title":"Call(String)","text":"<pre><code>public IEnumerable&lt;string&gt; Call(string text)\n</code></pre>"},{"location":"xmldocs/llama.llamamodel/#parameters_7","title":"Parameters","text":"<p><code>text</code> String</p>"},{"location":"xmldocs/llama.llamamodel/#returns_3","title":"Returns","text":"<p>IEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.llamamodelv1/","title":"LLamaModelV1","text":"<p>Namespace: LLama</p>"},{"location":"xmldocs/llama.llamamodelv1/#caution","title":"Caution","text":"<p>This type is obsolete.</p> <pre><code>public class LLamaModelV1\n</code></pre> <p>Inheritance Object \u2192 LLamaModelV1</p>"},{"location":"xmldocs/llama.llamamodelv1/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamamodelv1/#llamamodelv1string-int32-int32-int32-boolean-boolean-boolean-boolean-boolean-boolean-int32-int32-int32-string-string-boolean","title":"LLamaModelV1(String, Int32, Int32, Int32, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Int32, Int32, Int32, String, String, Boolean)","text":"<pre><code>public LLamaModelV1(string model_path, int n_ctx, int n_parts, int seed, bool f16_kv, bool logits_all, bool vocab_only, bool use_mmap, bool use_mlock, bool embedding, int n_threads, int n_batch, int last_n_tokens_size, string lora_base, string lora_path, bool verbose)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters","title":"Parameters","text":"<p><code>model_path</code> String</p> <p><code>n_ctx</code> Int32</p> <p><code>n_parts</code> Int32</p> <p><code>seed</code> Int32</p> <p><code>f16_kv</code> Boolean</p> <p><code>logits_all</code> Boolean</p> <p><code>vocab_only</code> Boolean</p> <p><code>use_mmap</code> Boolean</p> <p><code>use_mlock</code> Boolean</p> <p><code>embedding</code> Boolean</p> <p><code>n_threads</code> Int32</p> <p><code>n_batch</code> Int32</p> <p><code>last_n_tokens_size</code> Int32</p> <p><code>lora_base</code> String</p> <p><code>lora_path</code> String</p> <p><code>verbose</code> Boolean</p>"},{"location":"xmldocs/llama.llamamodelv1/#llamamodelv1llamamodelv1","title":"LLamaModelV1(LLamaModelV1)","text":"<pre><code>public LLamaModelV1(LLamaModelV1 other)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_1","title":"Parameters","text":"<p><code>other</code> LLamaModelV1</p>"},{"location":"xmldocs/llama.llamamodelv1/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamamodelv1/#tokenizestring","title":"Tokenize(String)","text":"<pre><code>public List&lt;int&gt; Tokenize(string text)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_2","title":"Parameters","text":"<p><code>text</code> String</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns","title":"Returns","text":"<p>List&lt;Int32&gt;</p>"},{"location":"xmldocs/llama.llamamodelv1/#detokenizeienumerableint32","title":"DeTokenize(IEnumerable&lt;Int32&gt;)","text":"<pre><code>public string DeTokenize(IEnumerable&lt;int&gt; tokens)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_3","title":"Parameters","text":"<p><code>tokens</code> IEnumerable&lt;Int32&gt;</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns_1","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.llamamodelv1/#detokenizeint32","title":"DeTokenize(Int32)","text":"<pre><code>public string DeTokenize(int token)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_4","title":"Parameters","text":"<p><code>token</code> Int32</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns_2","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.llamamodelv1/#setcachellamacache","title":"SetCache(LLamaCache)","text":"<pre><code>public void SetCache(LLamaCache cache)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_5","title":"Parameters","text":"<p><code>cache</code> LLamaCache</p>"},{"location":"xmldocs/llama.llamamodelv1/#reset","title":"Reset()","text":"<pre><code>public void Reset()\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#evallistint32","title":"Eval(List&lt;Int32&gt;)","text":"<pre><code>public void Eval(List&lt;int&gt; tokens)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_6","title":"Parameters","text":"<p><code>tokens</code> List&lt;Int32&gt;</p>"},{"location":"xmldocs/llama.llamamodelv1/#sampleint32-single-single-single-single-single","title":"Sample(Int32, Single, Single, Single, Single, Single)","text":"<pre><code>public int Sample(int top_k, float top_p, float temp, float repeat_penalty, float frequency_penalty, float presence_penalty)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_7","title":"Parameters","text":"<p><code>top_k</code> Int32</p> <p><code>top_p</code> Single</p> <p><code>temp</code> Single</p> <p><code>repeat_penalty</code> Single</p> <p><code>frequency_penalty</code> Single</p> <p><code>presence_penalty</code> Single</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns_3","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamamodelv1/#generateienumerableint32-int32-single-single-single-single-single-boolean","title":"Generate(IEnumerable&lt;Int32&gt;, Int32, Single, Single, Single, Single, Single, Boolean)","text":"<pre><code>public IEnumerable&lt;int&gt; Generate(IEnumerable&lt;int&gt; tokens, int top_k, float top_p, float temp, float repeat_penalty, float frequency_penalty, float presence_penalty, bool reset)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_8","title":"Parameters","text":"<p><code>tokens</code> IEnumerable&lt;Int32&gt;</p> <p><code>top_k</code> Int32</p> <p><code>top_p</code> Single</p> <p><code>temp</code> Single</p> <p><code>repeat_penalty</code> Single</p> <p><code>frequency_penalty</code> Single</p> <p><code>presence_penalty</code> Single</p> <p><code>reset</code> Boolean</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns_4","title":"Returns","text":"<p>IEnumerable&lt;Int32&gt;</p>"},{"location":"xmldocs/llama.llamamodelv1/#createembeddingstring","title":"CreateEmbedding(String)","text":"<pre><code>public Embedding CreateEmbedding(string input)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_9","title":"Parameters","text":"<p><code>input</code> String</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns_5","title":"Returns","text":"<p>Embedding</p>"},{"location":"xmldocs/llama.llamamodelv1/#embedstring","title":"Embed(String)","text":"<pre><code>public Single[] Embed(string input)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_10","title":"Parameters","text":"<p><code>input</code> String</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns_6","title":"Returns","text":"<p>Single[]</p>"},{"location":"xmldocs/llama.llamamodelv1/#createcompletionstring-string-int32-single-single-int32-boolean-string-single-single-single-int32","title":"CreateCompletion(String, String, Int32, Single, Single, Int32, Boolean, String[], Single, Single, Single, Int32)","text":"<pre><code>public IEnumerable&lt;CompletionChunk&gt; CreateCompletion(string prompt, string suffix, int max_tokens, float temperature, float top_p, int logprobs, bool echo, String[] stop, float frequency_penalty, float presence_penalty, float repeat_penalty, int top_k)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_11","title":"Parameters","text":"<p><code>prompt</code> String</p> <p><code>suffix</code> String</p> <p><code>max_tokens</code> Int32</p> <p><code>temperature</code> Single</p> <p><code>top_p</code> Single</p> <p><code>logprobs</code> Int32</p> <p><code>echo</code> Boolean</p> <p><code>stop</code> String[]</p> <p><code>frequency_penalty</code> Single</p> <p><code>presence_penalty</code> Single</p> <p><code>repeat_penalty</code> Single</p> <p><code>top_k</code> Int32</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns_7","title":"Returns","text":"<p>IEnumerable&lt;CompletionChunk&gt;</p>"},{"location":"xmldocs/llama.llamamodelv1/#callstring-string-int32-single-single-int32-boolean-string-single-single-single-int32","title":"Call(String, String, Int32, Single, Single, Int32, Boolean, String[], Single, Single, Single, Int32)","text":"<pre><code>public IEnumerable&lt;CompletionChunk&gt; Call(string prompt, string suffix, int max_tokens, float temperature, float top_p, int logprobs, bool echo, String[] stop, float frequency_penalty, float presence_penalty, float repeat_penalty, int top_k)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_12","title":"Parameters","text":"<p><code>prompt</code> String</p> <p><code>suffix</code> String</p> <p><code>max_tokens</code> Int32</p> <p><code>temperature</code> Single</p> <p><code>top_p</code> Single</p> <p><code>logprobs</code> Int32</p> <p><code>echo</code> Boolean</p> <p><code>stop</code> String[]</p> <p><code>frequency_penalty</code> Single</p> <p><code>presence_penalty</code> Single</p> <p><code>repeat_penalty</code> Single</p> <p><code>top_k</code> Int32</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns_8","title":"Returns","text":"<p>IEnumerable&lt;CompletionChunk&gt;</p>"},{"location":"xmldocs/llama.llamamodelv1/#createchatcompletionienumerablechatcompletionmessage-single-single-int32-string-int32-single-single-single","title":"CreateChatCompletion(IEnumerable&lt;ChatCompletionMessage&gt;, Single, Single, Int32, String[], Int32, Single, Single, Single)","text":"<pre><code>public IEnumerable&lt;ChatCompletionChunk&gt; CreateChatCompletion(IEnumerable&lt;ChatCompletionMessage&gt; messages, float temperature, float top_p, int top_k, String[] stop, int max_tokens, float presence_penalty, float frequency_penalty, float repeat_penalty)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_13","title":"Parameters","text":"<p><code>messages</code> IEnumerable&lt;ChatCompletionMessage&gt;</p> <p><code>temperature</code> Single</p> <p><code>top_p</code> Single</p> <p><code>top_k</code> Int32</p> <p><code>stop</code> String[]</p> <p><code>max_tokens</code> Int32</p> <p><code>presence_penalty</code> Single</p> <p><code>frequency_penalty</code> Single</p> <p><code>repeat_penalty</code> Single</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns_9","title":"Returns","text":"<p>IEnumerable&lt;ChatCompletionChunk&gt;</p>"},{"location":"xmldocs/llama.llamamodelv1/#savestate","title":"SaveState()","text":"<pre><code>public LLamaState SaveState()\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#returns_10","title":"Returns","text":"<p>LLamaState</p>"},{"location":"xmldocs/llama.llamamodelv1/#loadstatellamastate","title":"LoadState(LLamaState)","text":"<pre><code>public void LoadState(LLamaState state)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_14","title":"Parameters","text":"<p><code>state</code> LLamaState</p>"},{"location":"xmldocs/llama.llamamodelv1/#longesttokenprefixienumerableint32-ienumerableint32","title":"LongestTokenPrefix(IEnumerable&lt;Int32&gt;, IEnumerable&lt;Int32&gt;)","text":"<pre><code>internal static int LongestTokenPrefix(IEnumerable&lt;int&gt; a, IEnumerable&lt;int&gt; b)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_15","title":"Parameters","text":"<p><code>a</code> IEnumerable&lt;Int32&gt;</p> <p><code>b</code> IEnumerable&lt;Int32&gt;</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns_11","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamamodelv1/#createchatcompletiong__getrole31_0chatcompletionmessage","title":"&lt;CreateChatCompletion&gt;g__GetRole|31_0(ChatCompletionMessage)","text":"<pre><code>internal static string &lt;CreateChatCompletion&gt;g__GetRole|31_0(ChatCompletionMessage message)\n</code></pre>"},{"location":"xmldocs/llama.llamamodelv1/#parameters_16","title":"Parameters","text":"<p><code>message</code> ChatCompletionMessage</p>"},{"location":"xmldocs/llama.llamamodelv1/#returns_12","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.llamaparams/","title":"LLamaParams","text":"<p>Namespace: LLama</p> <pre><code>public struct LLamaParams\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaParams</p>"},{"location":"xmldocs/llama.llamaparams/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.llamaparams/#seed","title":"seed","text":"<pre><code>public int seed;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#n_threads","title":"n_threads","text":"<pre><code>public int n_threads;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#n_predict","title":"n_predict","text":"<pre><code>public int n_predict;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#n_parts","title":"n_parts","text":"<pre><code>public int n_parts;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#n_ctx","title":"n_ctx","text":"<pre><code>public int n_ctx;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#n_batch","title":"n_batch","text":"<pre><code>public int n_batch;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#n_keep","title":"n_keep","text":"<pre><code>public int n_keep;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#logit_bias","title":"logit_bias","text":"<pre><code>public Dictionary&lt;int, float&gt; logit_bias;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#top_k","title":"top_k","text":"<pre><code>public int top_k;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#top_p","title":"top_p","text":"<pre><code>public float top_p;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#tfs_z","title":"tfs_z","text":"<pre><code>public float tfs_z;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#typical_p","title":"typical_p","text":"<pre><code>public float typical_p;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#temp","title":"temp","text":"<pre><code>public float temp;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#repeat_penalty","title":"repeat_penalty","text":"<pre><code>public float repeat_penalty;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#repeat_last_n","title":"repeat_last_n","text":"<pre><code>public int repeat_last_n;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#frequency_penalty","title":"frequency_penalty","text":"<pre><code>public float frequency_penalty;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#presence_penalty","title":"presence_penalty","text":"<pre><code>public float presence_penalty;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#mirostat","title":"mirostat","text":"<pre><code>public int mirostat;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#mirostat_tau","title":"mirostat_tau","text":"<pre><code>public float mirostat_tau;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#mirostat_eta","title":"mirostat_eta","text":"<pre><code>public float mirostat_eta;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#model","title":"model","text":"<pre><code>public string model;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#prompt","title":"prompt","text":"<pre><code>public string prompt;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#path_session","title":"path_session","text":"<pre><code>public string path_session;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#input_prefix","title":"input_prefix","text":"<pre><code>public string input_prefix;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#input_suffix","title":"input_suffix","text":"<pre><code>public string input_suffix;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#antiprompt","title":"antiprompt","text":"<pre><code>public List&lt;string&gt; antiprompt;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#lora_adapter","title":"lora_adapter","text":"<pre><code>public string lora_adapter;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#lora_base","title":"lora_base","text":"<pre><code>public string lora_base;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#memory_f16","title":"memory_f16","text":"<pre><code>public bool memory_f16;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#random_prompt","title":"random_prompt","text":"<pre><code>public bool random_prompt;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#use_color","title":"use_color","text":"<pre><code>public bool use_color;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#interactive","title":"interactive","text":"<pre><code>public bool interactive;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#embedding","title":"embedding","text":"<pre><code>public bool embedding;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#interactive_first","title":"interactive_first","text":"<pre><code>public bool interactive_first;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#instruct","title":"instruct","text":"<pre><code>public bool instruct;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#penalize_nl","title":"penalize_nl","text":"<pre><code>public bool penalize_nl;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#perplexity","title":"perplexity","text":"<pre><code>public bool perplexity;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#use_mmap","title":"use_mmap","text":"<pre><code>public bool use_mmap;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#use_mlock","title":"use_mlock","text":"<pre><code>public bool use_mlock;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#mem_test","title":"mem_test","text":"<pre><code>public bool mem_test;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#verbose_prompt","title":"verbose_prompt","text":"<pre><code>public bool verbose_prompt;\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamaparams/#llamaparamsint32-int32-int32-int32-int32-int32-int32-dictionaryint32-single-int32-single-single-single-single-single-int32-single-single-int32-single-single-string-string-string-string-string-liststring-string-string-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean-boolean","title":"LLamaParams(Int32, Int32, Int32, Int32, Int32, Int32, Int32, Dictionary&lt;Int32, Single&gt;, Int32, Single, Single, Single, Single, Single, Int32, Single, Single, Int32, Single, Single, String, String, String, String, String, List&lt;String&gt;, String, String, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean, Boolean)","text":"<pre><code>LLamaParams(int seed, int n_threads, int n_predict, int n_parts, int n_ctx, int n_batch, int n_keep, Dictionary&lt;int, float&gt; logit_bias, int top_k, float top_p, float tfs_z, float typical_p, float temp, float repeat_penalty, int repeat_last_n, float frequency_penalty, float presence_penalty, int mirostat, float mirostat_tau, float mirostat_eta, string model, string prompt, string path_session, string input_prefix, string input_suffix, List&lt;string&gt; antiprompt, string lora_adapter, string lora_base, bool memory_f16, bool random_prompt, bool use_color, bool interactive, bool embedding, bool interactive_first, bool instruct, bool penalize_nl, bool perplexity, bool use_mmap, bool use_mlock, bool mem_test, bool verbose_prompt)\n</code></pre>"},{"location":"xmldocs/llama.llamaparams/#parameters","title":"Parameters","text":"<p><code>seed</code> Int32</p> <p><code>n_threads</code> Int32</p> <p><code>n_predict</code> Int32</p> <p><code>n_parts</code> Int32</p> <p><code>n_ctx</code> Int32</p> <p><code>n_batch</code> Int32</p> <p><code>n_keep</code> Int32</p> <p><code>logit_bias</code> Dictionary&lt;Int32, Single&gt;</p> <p><code>top_k</code> Int32</p> <p><code>top_p</code> Single</p> <p><code>tfs_z</code> Single</p> <p><code>typical_p</code> Single</p> <p><code>temp</code> Single</p> <p><code>repeat_penalty</code> Single</p> <p><code>repeat_last_n</code> Int32</p> <p><code>frequency_penalty</code> Single</p> <p><code>presence_penalty</code> Single</p> <p><code>mirostat</code> Int32</p> <p><code>mirostat_tau</code> Single</p> <p><code>mirostat_eta</code> Single</p> <p><code>model</code> String</p> <p><code>prompt</code> String</p> <p><code>path_session</code> String</p> <p><code>input_prefix</code> String</p> <p><code>input_suffix</code> String</p> <p><code>antiprompt</code> List&lt;String&gt;</p> <p><code>lora_adapter</code> String</p> <p><code>lora_base</code> String</p> <p><code>memory_f16</code> Boolean</p> <p><code>random_prompt</code> Boolean</p> <p><code>use_color</code> Boolean</p> <p><code>interactive</code> Boolean</p> <p><code>embedding</code> Boolean</p> <p><code>interactive_first</code> Boolean</p> <p><code>instruct</code> Boolean</p> <p><code>penalize_nl</code> Boolean</p> <p><code>perplexity</code> Boolean</p> <p><code>use_mmap</code> Boolean</p> <p><code>use_mlock</code> Boolean</p> <p><code>mem_test</code> Boolean</p> <p><code>verbose_prompt</code> Boolean</p>"},{"location":"xmldocs/llama.llamaquantizer/","title":"LLamaQuantizer","text":"<p>Namespace: LLama</p> <p>The quantizer to quantize the model.</p> <pre><code>public static class LLamaQuantizer\n</code></pre> <p>Inheritance Object \u2192 LLamaQuantizer</p>"},{"location":"xmldocs/llama.llamaquantizer/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamaquantizer/#quantizestring-string-llamaftype-int32-boolean-boolean","title":"Quantize(String, String, LLamaFtype, Int32, Boolean, Boolean)","text":"<p>Quantize the model.</p> <pre><code>public static bool Quantize(string srcFileName, string dstFilename, LLamaFtype ftype, int nthread, bool allowRequantize, bool quantizeOutputTensor)\n</code></pre>"},{"location":"xmldocs/llama.llamaquantizer/#parameters","title":"Parameters","text":"<p><code>srcFileName</code> String The model file to be quantized.</p> <p><code>dstFilename</code> String The path to save the quantized model.</p> <p><code>ftype</code> LLamaFtype The type of quantization.</p> <p><code>nthread</code> Int32 Thread to be used during the quantization. By default it's the physical core number.</p> <p><code>allowRequantize</code> Boolean</p> <p><code>quantizeOutputTensor</code> Boolean</p>"},{"location":"xmldocs/llama.llamaquantizer/#returns","title":"Returns","text":"<p>Boolean Whether the quantization is successful.</p>"},{"location":"xmldocs/llama.llamaquantizer/#exceptions","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.llamaquantizer/#quantizestring-string-string-int32-boolean-boolean","title":"Quantize(String, String, String, Int32, Boolean, Boolean)","text":"<p>Quantize the model.</p> <pre><code>public static bool Quantize(string srcFileName, string dstFilename, string ftype, int nthread, bool allowRequantize, bool quantizeOutputTensor)\n</code></pre>"},{"location":"xmldocs/llama.llamaquantizer/#parameters_1","title":"Parameters","text":"<p><code>srcFileName</code> String The model file to be quantized.</p> <p><code>dstFilename</code> String The path to save the quantized model.</p> <p><code>ftype</code> String The type of quantization.</p> <p><code>nthread</code> Int32 Thread to be used during the quantization. By default it's the physical core number.</p> <p><code>allowRequantize</code> Boolean</p> <p><code>quantizeOutputTensor</code> Boolean</p>"},{"location":"xmldocs/llama.llamaquantizer/#returns_1","title":"Returns","text":"<p>Boolean Whether the quantization is successful.</p>"},{"location":"xmldocs/llama.llamaquantizer/#exceptions_1","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.llamastate/","title":"LLamaState","text":"<p>Namespace: LLama</p> <pre><code>public class LLamaState : System.IEquatable`1[[LLama.LLamaState, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 LLamaState Implements IEquatable&lt;LLamaState&gt;</p>"},{"location":"xmldocs/llama.llamastate/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.llamastate/#evaltokens","title":"EvalTokens","text":"<pre><code>public Queue&lt;int&gt; EvalTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#property-value","title":"Property Value","text":"<p>Queue&lt;Int32&gt;</p>"},{"location":"xmldocs/llama.llamastate/#evallogits","title":"EvalLogits","text":"<pre><code>public Queue&lt;Single[]&gt; EvalLogits { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#property-value_1","title":"Property Value","text":"<p>Queue&lt;Single[]&gt;</p>"},{"location":"xmldocs/llama.llamastate/#state","title":"State","text":"<pre><code>public Byte[] State { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#property-value_2","title":"Property Value","text":"<p>Byte[]</p>"},{"location":"xmldocs/llama.llamastate/#size","title":"Size","text":"<pre><code>public int Size { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#property-value_3","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamastate/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamastate/#llamastatequeueint32-queuesingle-byte-int32","title":"LLamaState(Queue&lt;Int32&gt;, Queue&lt;Single[]&gt;, Byte[], Int32)","text":"<pre><code>public LLamaState(Queue&lt;int&gt; EvalTokens, Queue&lt;Single[]&gt; EvalLogits, Byte[] State, int Size)\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#parameters","title":"Parameters","text":"<p><code>EvalTokens</code> Queue&lt;Int32&gt;</p> <p><code>EvalLogits</code> Queue&lt;Single[]&gt;</p> <p><code>State</code> Byte[]</p> <p><code>Size</code> Int32</p>"},{"location":"xmldocs/llama.llamastate/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamastate/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.llamastate/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.llamastate/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.llamastate/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamastate/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.llamastate/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.llamastate/#equalsllamastate","title":"Equals(LLamaState)","text":"<pre><code>public bool Equals(LLamaState other)\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#parameters_3","title":"Parameters","text":"<p><code>other</code> LLamaState</p>"},{"location":"xmldocs/llama.llamastate/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.llamastate/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public LLamaState &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#returns_5","title":"Returns","text":"<p>LLamaState</p>"},{"location":"xmldocs/llama.llamastate/#deconstructqueue1-queue1-byte-int32","title":"Deconstruct(Queue<code>1&amp;, Queue</code>1&amp;, Byte[]&amp;, Int32&amp;)","text":"<pre><code>public void Deconstruct(Queue`1&amp; EvalTokens, Queue`1&amp; EvalLogits, Byte[]&amp; State, Int32&amp; Size)\n</code></pre>"},{"location":"xmldocs/llama.llamastate/#parameters_4","title":"Parameters","text":"<p><code>EvalTokens</code> Queue`1&amp;</p> <p><code>EvalLogits</code> Queue`1&amp;</p> <p><code>State</code> Byte[]&amp;</p> <p><code>Size</code> Int32&amp;</p>"},{"location":"xmldocs/llama.llamatransforms/","title":"LLamaTransforms","text":"<p>Namespace: LLama</p> <p>A class that contains all the transforms provided internally by LLama.</p> <pre><code>public class LLamaTransforms\n</code></pre> <p>Inheritance Object \u2192 LLamaTransforms</p>"},{"location":"xmldocs/llama.llamatransforms/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamatransforms/#llamatransforms_1","title":"LLamaTransforms()","text":"<pre><code>public LLamaTransforms()\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/","title":"LLavaWeights","text":"<p>Namespace: LLama</p> <p>A set of llava model weights (mmproj), loaded into memory.</p> <pre><code>public sealed class LLavaWeights : System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 LLavaWeights Implements IDisposable</p>"},{"location":"xmldocs/llama.llavaweights/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.llavaweights/#nativehandle","title":"NativeHandle","text":"<p>The native handle, which is used in the native APIs</p> <pre><code>public SafeLlavaModelHandle NativeHandle { get; }\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#property-value","title":"Property Value","text":"<p>SafeLlavaModelHandle</p> <p>Remarks:</p> <p>Be careful how you use this!</p>"},{"location":"xmldocs/llama.llavaweights/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llavaweights/#loadfromfilestring","title":"LoadFromFile(String)","text":"<p>Load weights into memory</p> <pre><code>public static LLavaWeights LoadFromFile(string mmProject)\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#parameters","title":"Parameters","text":"<p><code>mmProject</code> String path to the \"mmproj\" model file</p>"},{"location":"xmldocs/llama.llavaweights/#returns","title":"Returns","text":"<p>LLavaWeights</p>"},{"location":"xmldocs/llama.llavaweights/#createimageembeddingsllamacontext-byte","title":"CreateImageEmbeddings(LLamaContext, Byte[])","text":"<p>Create the Image Embeddings from the bytes of an image.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(LLamaContext ctxLlama, Byte[] image)\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#parameters_1","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext</p> <p><code>image</code> Byte[] Image bytes. Supported formats:  JPGPNGBMPTGA</p>"},{"location":"xmldocs/llama.llavaweights/#returns_1","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.llavaweights/#createimageembeddingsllamacontext-string","title":"CreateImageEmbeddings(LLamaContext, String)","text":"<p>Create the Image Embeddings from the bytes of an image.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(LLamaContext ctxLlama, string image)\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#parameters_2","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext</p> <p><code>image</code> String Path to the image file. Supported formats:  JPGPNGBMPTGA</p>"},{"location":"xmldocs/llama.llavaweights/#returns_2","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.llavaweights/#exceptions","title":"Exceptions","text":"<p>InvalidOperationException</p>"},{"location":"xmldocs/llama.llavaweights/#evalimageembedllamacontext-safellavaimageembedhandle-int32","title":"EvalImageEmbed(LLamaContext, SafeLlavaImageEmbedHandle, Int32&amp;)","text":"<p>Eval the image embeddings</p> <pre><code>public bool EvalImageEmbed(LLamaContext ctxLlama, SafeLlavaImageEmbedHandle imageEmbed, Int32&amp; n_past)\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#parameters_3","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext</p> <p><code>imageEmbed</code> SafeLlavaImageEmbedHandle</p> <p><code>n_past</code> Int32&amp;</p>"},{"location":"xmldocs/llama.llavaweights/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.llavaweights/#dispose","title":"Dispose()","text":"<pre><code>public void Dispose()\n</code></pre>"},{"location":"xmldocs/llama.native.decoderesult/","title":"DecodeResult","text":"<p>Namespace: LLama.Native</p> <p>Return codes from llama_decode</p> <pre><code>public enum DecodeResult\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 DecodeResult Implements IComparable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.decoderesult/#fields","title":"Fields","text":"Name Value Description Error -1 An unspecified error Ok 0 Ok. NoKvSlot 1 Could not find a KV slot for the batch (try reducing the size of the batch or increase the context)"},{"location":"xmldocs/llama.native.ggmltype/","title":"GGMLType","text":"<p>Namespace: LLama.Native</p> <p>Possible GGML quantisation types</p> <pre><code>public enum GGMLType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 GGMLType Implements IComparable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.ggmltype/#fields","title":"Fields","text":"Name Value Description GGML_TYPE_F32 0 Full 32 bit float GGML_TYPE_F16 1 16 bit float GGML_TYPE_Q4_0 2 4 bit float GGML_TYPE_Q4_1 3 4 bit float GGML_TYPE_Q5_0 6 5 bit float GGML_TYPE_Q5_1 7 5 bit float GGML_TYPE_Q8_0 8 8 bit float GGML_TYPE_Q8_1 9 8 bit float GGML_TYPE_Q2_K 10 \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw) GGML_TYPE_Q3_K 11 \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw. GGML_TYPE_Q4_K 12 \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw. GGML_TYPE_Q5_K 13 \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw GGML_TYPE_Q6_K 14 \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw GGML_TYPE_Q8_K 15 \"type-0\" 8-bit quantization. Only used for quantizing intermediate results. The difference to the existing Q8_0 is that the block size is 256. All 2-6 bit dot products are implemented for this quantization type. GGML_TYPE_I8 16 Integer, 8 bit GGML_TYPE_I16 17 Integer, 16 bit GGML_TYPE_I32 18 Integer, 32 bit GGML_TYPE_COUNT 19 The value of this entry is the count of the number of possible quant types."},{"location":"xmldocs/llama.native.gpusplitmode/","title":"GPUSplitMode","text":"<p>Namespace: LLama.Native</p> <pre><code>public enum GPUSplitMode\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 GPUSplitMode Implements IComparable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>llama_split_mode</p>"},{"location":"xmldocs/llama.native.gpusplitmode/#fields","title":"Fields","text":"Name Value Description None 0 Single GPU Layer 1 Split layers and KV across GPUs Row 2 split rows across GPUs"},{"location":"xmldocs/llama.native.llamabatch/","title":"LLamaBatch","text":"<p>Namespace: LLama.Native</p> <p>A batch allows submitting multiple tokens to multiple sequences simultaneously</p> <pre><code>public class LLamaBatch\n</code></pre> <p>Inheritance Object \u2192 LLamaBatch</p>"},{"location":"xmldocs/llama.native.llamabatch/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamabatch/#tokencount","title":"TokenCount","text":"<p>The number of tokens in this batch</p> <pre><code>public int TokenCount { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamabatch/#sequencecapacity","title":"SequenceCapacity","text":"<p>Maximum number of sequences a token can be assigned to (automatically grows if exceeded)</p> <pre><code>public int SequenceCapacity { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamabatch/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.llamabatch/#llamabatch_1","title":"LLamaBatch()","text":"<p>Create a new batch for submitting inputs to llama.cpp</p> <pre><code>public LLamaBatch()\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamabatch/#tonativebatchllamanativebatch","title":"ToNativeBatch(LLamaNativeBatch&amp;)","text":"<pre><code>internal GroupDisposable ToNativeBatch(LLamaNativeBatch&amp; batch)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#parameters","title":"Parameters","text":"<p><code>batch</code> LLamaNativeBatch&amp;</p>"},{"location":"xmldocs/llama.native.llamabatch/#returns","title":"Returns","text":"<p>GroupDisposable</p>"},{"location":"xmldocs/llama.native.llamabatch/#addllamatoken-llamapos-readonlyspanllamaseqid-boolean","title":"Add(LLamaToken, LLamaPos, ReadOnlySpan&lt;LLamaSeqId&gt;, Boolean)","text":"<p>Add a single token to the batch at the same position in several sequences</p> <pre><code>public int Add(LLamaToken token, LLamaPos pos, ReadOnlySpan&lt;LLamaSeqId&gt; sequences, bool logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#parameters_1","title":"Parameters","text":"<p><code>token</code> LLamaToken The token to add</p> <p><code>pos</code> LLamaPos The position to add it att</p> <p><code>sequences</code> ReadOnlySpan&lt;LLamaSeqId&gt; The set of sequences to add this token to</p> <p><code>logits</code> Boolean</p>"},{"location":"xmldocs/llama.native.llamabatch/#returns_1","title":"Returns","text":"<p>Int32 The index that the token was added at. Use this for GetLogitsIth</p> <p>Remarks:</p> <p>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</p>"},{"location":"xmldocs/llama.native.llamabatch/#addllamatoken-llamapos-listllamaseqid-boolean","title":"Add(LLamaToken, LLamaPos, List&lt;LLamaSeqId&gt;, Boolean)","text":"<p>Add a single token to the batch at the same position in several sequences</p> <pre><code>public int Add(LLamaToken token, LLamaPos pos, List&lt;LLamaSeqId&gt; sequences, bool logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#parameters_2","title":"Parameters","text":"<p><code>token</code> LLamaToken The token to add</p> <p><code>pos</code> LLamaPos The position to add it att</p> <p><code>sequences</code> List&lt;LLamaSeqId&gt; The set of sequences to add this token to</p> <p><code>logits</code> Boolean</p>"},{"location":"xmldocs/llama.native.llamabatch/#returns_2","title":"Returns","text":"<p>Int32 The index that the token was added at. Use this for GetLogitsIth</p> <p>Remarks:</p> <p>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</p>"},{"location":"xmldocs/llama.native.llamabatch/#addllamatoken-llamapos-llamaseqid-boolean","title":"Add(LLamaToken, LLamaPos, LLamaSeqId, Boolean)","text":"<p>Add a single token to the batch at a certain position for a single sequences</p> <pre><code>public int Add(LLamaToken token, LLamaPos pos, LLamaSeqId sequence, bool logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#parameters_3","title":"Parameters","text":"<p><code>token</code> LLamaToken The token to add</p> <p><code>pos</code> LLamaPos The position to add it att</p> <p><code>sequence</code> LLamaSeqId The sequence to add this token to</p> <p><code>logits</code> Boolean</p>"},{"location":"xmldocs/llama.native.llamabatch/#returns_3","title":"Returns","text":"<p>Int32 The index that the token was added at. Use this for GetLogitsIth</p> <p>Remarks:</p> <p>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</p>"},{"location":"xmldocs/llama.native.llamabatch/#addrangereadonlyspanllamatoken-llamapos-llamaseqid-boolean","title":"AddRange(ReadOnlySpan&lt;LLamaToken&gt;, LLamaPos, LLamaSeqId, Boolean)","text":"<p>Add a range of tokens to a single sequence, start at the given position.</p> <pre><code>public int AddRange(ReadOnlySpan&lt;LLamaToken&gt; tokens, LLamaPos start, LLamaSeqId sequence, bool logitsLast)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#parameters_4","title":"Parameters","text":"<p><code>tokens</code> ReadOnlySpan&lt;LLamaToken&gt; The tokens to add</p> <p><code>start</code> LLamaPos The starting position to add tokens at</p> <p><code>sequence</code> LLamaSeqId The sequence to add this token to</p> <p><code>logitsLast</code> Boolean Whether the final token should generate logits</p>"},{"location":"xmldocs/llama.native.llamabatch/#returns_4","title":"Returns","text":"<p>Int32 The index that the final token was added at. Use this for GetLogitsIth</p>"},{"location":"xmldocs/llama.native.llamabatch/#clear","title":"Clear()","text":"<p>Set TokenCount to zero for this batch</p> <pre><code>public void Clear()\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#getlogitpositionsspanvaluetuplellamaseqid-int32","title":"GetLogitPositions(Span&lt;ValueTuple&lt;LLamaSeqId, Int32&gt;&gt;)","text":"<p>Get the positions where logits can be sampled from</p> <pre><code>internal Span&lt;ValueTuple&lt;LLamaSeqId, int&gt;&gt; GetLogitPositions(Span&lt;ValueTuple&lt;LLamaSeqId, int&gt;&gt; dest)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#parameters_5","title":"Parameters","text":"<p><code>dest</code> Span&lt;ValueTuple&lt;LLamaSeqId, Int32&gt;&gt;</p>"},{"location":"xmldocs/llama.native.llamabatch/#returns_5","title":"Returns","text":"<p>Span&lt;ValueTuple&lt;LLamaSeqId, Int32&gt;&gt;</p>"},{"location":"xmldocs/llama.native.llamabeamsstate/","title":"LLamaBeamsState","text":"<p>Namespace: LLama.Native</p> <p>Passed to beam_search_callback function.  Whenever 0 &lt; common_prefix_length, this number of tokens should be copied from any of the beams  (e.g. beams[0]) as they will be removed (shifted) from all beams in all subsequent callbacks.</p> <pre><code>public struct LLamaBeamsState\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaBeamsState</p>"},{"location":"xmldocs/llama.native.llamabeamsstate/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamabeamsstate/#commonprefixlength","title":"CommonPrefixLength","text":"<p>Current max length of prefix tokens shared by all beams.</p> <pre><code>public ulong CommonPrefixLength;\n</code></pre>"},{"location":"xmldocs/llama.native.llamabeamsstate/#lastcall","title":"LastCall","text":"<p>True iff this is the last callback invocation.</p> <pre><code>public bool LastCall;\n</code></pre>"},{"location":"xmldocs/llama.native.llamabeamsstate/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamabeamsstate/#beams","title":"Beams","text":"<p>The current state of each beam</p> <pre><code>public Span&lt;LLamaBeamView&gt; Beams { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamabeamsstate/#property-value","title":"Property Value","text":"<p>Span&lt;LLamaBeamView&gt;</p>"},{"location":"xmldocs/llama.native.llamabeamview/","title":"LLamaBeamView","text":"<p>Namespace: LLama.Native</p> <p>Information about a single beam in a beam search</p> <pre><code>public struct LLamaBeamView\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaBeamView</p>"},{"location":"xmldocs/llama.native.llamabeamview/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamabeamview/#cumulativeprobability","title":"CumulativeProbability","text":"<p>Cumulative beam probability (renormalized relative to all beams)</p> <pre><code>public float CumulativeProbability;\n</code></pre>"},{"location":"xmldocs/llama.native.llamabeamview/#endofbeam","title":"EndOfBeam","text":"<p>Callback should set this to true when a beam is at end-of-beam.</p> <pre><code>public bool EndOfBeam;\n</code></pre>"},{"location":"xmldocs/llama.native.llamabeamview/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamabeamview/#tokens","title":"Tokens","text":"<p>Tokens in this beam</p> <pre><code>public Span&lt;LLamaToken&gt; Tokens { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamabeamview/#property-value","title":"Property Value","text":"<p>Span&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.native.llamachatmessage/","title":"LLamaChatMessage","text":"<p>Namespace: LLama.Native</p> <pre><code>public struct LLamaChatMessage\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaChatMessage</p> <p>Remarks:</p> <p>llama_chat_message</p>"},{"location":"xmldocs/llama.native.llamachatmessage/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamachatmessage/#role","title":"role","text":"<pre><code>public Byte* role;\n</code></pre>"},{"location":"xmldocs/llama.native.llamachatmessage/#content","title":"content","text":"<pre><code>public Byte* content;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/","title":"LLamaContextParams","text":"<p>Namespace: LLama.Native</p> <p>A C# representation of the llama.cpp <code>llama_context_params</code> struct</p> <pre><code>public struct LLamaContextParams\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaContextParams</p>"},{"location":"xmldocs/llama.native.llamacontextparams/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamacontextparams/#seed","title":"seed","text":"<p>RNG seed, -1 for random</p> <pre><code>public uint seed;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#n_ctx","title":"n_ctx","text":"<p>text context, 0 = from model</p> <pre><code>public uint n_ctx;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#n_batch","title":"n_batch","text":"<p>prompt processing batch size</p> <pre><code>public uint n_batch;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#n_threads","title":"n_threads","text":"<p>number of threads to use for generation</p> <pre><code>public uint n_threads;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#n_threads_batch","title":"n_threads_batch","text":"<p>number of threads to use for batch processing</p> <pre><code>public uint n_threads_batch;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#rope_scaling_type","title":"rope_scaling_type","text":"<p>RoPE scaling type, from <code>enum llama_rope_scaling_type</code></p> <pre><code>public RopeScalingType rope_scaling_type;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#rope_freq_base","title":"rope_freq_base","text":"<p>RoPE base frequency, 0 = from model</p> <pre><code>public float rope_freq_base;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#rope_freq_scale","title":"rope_freq_scale","text":"<p>RoPE frequency scaling factor, 0 = from model</p> <pre><code>public float rope_freq_scale;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#yarn_ext_factor","title":"yarn_ext_factor","text":"<p>YaRN extrapolation mix factor, negative = from model</p> <pre><code>public float yarn_ext_factor;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#yarn_attn_factor","title":"yarn_attn_factor","text":"<p>YaRN magnitude scaling factor</p> <pre><code>public float yarn_attn_factor;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#yarn_beta_fast","title":"yarn_beta_fast","text":"<p>YaRN low correction dim</p> <pre><code>public float yarn_beta_fast;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#yarn_beta_slow","title":"yarn_beta_slow","text":"<p>YaRN high correction dim</p> <pre><code>public float yarn_beta_slow;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#yarn_orig_ctx","title":"yarn_orig_ctx","text":"<p>YaRN original context size</p> <pre><code>public uint yarn_orig_ctx;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#defrag_threshold","title":"defrag_threshold","text":"<p>defragment the KV cache if holes/size &gt; defrag_threshold, Set to &lt; 0 to disable (default)</p> <pre><code>public float defrag_threshold;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#cb_eval","title":"cb_eval","text":"<p>ggml_backend_sched_eval_callback</p> <pre><code>public IntPtr cb_eval;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#cb_eval_user_data","title":"cb_eval_user_data","text":"<p>User data passed into cb_eval</p> <pre><code>public IntPtr cb_eval_user_data;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#type_k","title":"type_k","text":"<p>data type for K cache</p> <pre><code>public GGMLType type_k;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#type_v","title":"type_v","text":"<p>data type for V cache</p> <pre><code>public GGMLType type_v;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamacontextparams/#embedding","title":"embedding","text":"<p>embedding mode only</p> <pre><code>public bool embedding { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamacontextparams/#offload_kqv","title":"offload_kqv","text":"<p>whether to offload the KQV ops (including the KV cache) to GPU</p> <pre><code>public bool offload_kqv { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamacontextparams/#do_pooling","title":"do_pooling","text":"<p>Whether to pool (sum) embedding results by sequence id (ignored if no pooling layer)</p> <pre><code>public bool do_pooling { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#property-value_2","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamaftype/","title":"LLamaFtype","text":"<p>Namespace: LLama.Native</p> <p>Supported model file types</p> <pre><code>public enum LLamaFtype\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaFtype Implements IComparable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.llamaftype/#fields","title":"Fields","text":"Name Value Description LLAMA_FTYPE_ALL_F32 0 All f32 LLAMA_FTYPE_MOSTLY_F16 1 Mostly f16 LLAMA_FTYPE_MOSTLY_Q8_0 7 Mostly 8 bit LLAMA_FTYPE_MOSTLY_Q4_0 2 Mostly 4 bit LLAMA_FTYPE_MOSTLY_Q4_1 3 Mostly 4 bit LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 4 Mostly 4 bit, tok_embeddings.weight and output.weight are f16 LLAMA_FTYPE_MOSTLY_Q5_0 8 Mostly 5 bit LLAMA_FTYPE_MOSTLY_Q5_1 9 Mostly 5 bit LLAMA_FTYPE_MOSTLY_Q2_K 10 K-Quant 2 bit LLAMA_FTYPE_MOSTLY_Q3_K_S 11 K-Quant 3 bit (Small) LLAMA_FTYPE_MOSTLY_Q3_K_M 12 K-Quant 3 bit (Medium) LLAMA_FTYPE_MOSTLY_Q3_K_L 13 K-Quant 3 bit (Large) LLAMA_FTYPE_MOSTLY_Q4_K_S 14 K-Quant 4 bit (Small) LLAMA_FTYPE_MOSTLY_Q4_K_M 15 K-Quant 4 bit (Medium) LLAMA_FTYPE_MOSTLY_Q5_K_S 16 K-Quant 5 bit (Small) LLAMA_FTYPE_MOSTLY_Q5_K_M 17 K-Quant 5 bit (Medium) LLAMA_FTYPE_MOSTLY_Q6_K 18 K-Quant 6 bit LLAMA_FTYPE_MOSTLY_IQ2_XXS 19 except 1d tensors LLAMA_FTYPE_MOSTLY_IQ2_XS 20 except 1d tensors LLAMA_FTYPE_MOSTLY_Q2_K_S 21 except 1d tensors LLAMA_FTYPE_MOSTLY_IQ3_K_XS 22 except 1d tensors LLAMA_FTYPE_MOSTLY_IQ3_XXS 23 except 1d tensors LLAMA_FTYPE_MOSTLY_IQ1_S 24 except 1d tensors LLAMA_FTYPE_MOSTLY_IQ4_NL 25 except 1d tensors LLAMA_FTYPE_MOSTLY_IQ3_S 26 except 1d tensors LLAMA_FTYPE_MOSTLY_IQ3_M 27 except 1d tensors LLAMA_FTYPE_MOSTLY_IQ2_S 28 except 1d tensors LLAMA_FTYPE_MOSTLY_IQ2_M 29 except 1d tensors LLAMA_FTYPE_MOSTLY_IQ4_XS 30 except 1d tensors LLAMA_FTYPE_GUESSED 1024 File type was not specified"},{"location":"xmldocs/llama.native.llamagrammarelement/","title":"LLamaGrammarElement","text":"<p>Namespace: LLama.Native</p> <p>An element of a grammar</p> <pre><code>public struct LLamaGrammarElement\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaGrammarElement Implements IEquatable&lt;LLamaGrammarElement&gt;</p>"},{"location":"xmldocs/llama.native.llamagrammarelement/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamagrammarelement/#type","title":"Type","text":"<p>The type of this element</p> <pre><code>public LLamaGrammarElementType Type;\n</code></pre>"},{"location":"xmldocs/llama.native.llamagrammarelement/#value","title":"Value","text":"<p>Unicode code point or rule ID</p> <pre><code>public uint Value;\n</code></pre>"},{"location":"xmldocs/llama.native.llamagrammarelement/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.llamagrammarelement/#llamagrammarelementllamagrammarelementtype-uint32","title":"LLamaGrammarElement(LLamaGrammarElementType, UInt32)","text":"<p>Construct a new LLamaGrammarElement</p> <pre><code>LLamaGrammarElement(LLamaGrammarElementType type, uint value)\n</code></pre>"},{"location":"xmldocs/llama.native.llamagrammarelement/#parameters","title":"Parameters","text":"<p><code>type</code> LLamaGrammarElementType</p> <p><code>value</code> UInt32</p>"},{"location":"xmldocs/llama.native.llamagrammarelement/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamagrammarelement/#ischarelement","title":"IsCharElement()","text":"<pre><code>bool IsCharElement()\n</code></pre>"},{"location":"xmldocs/llama.native.llamagrammarelement/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamagrammarelement/#tostring","title":"ToString()","text":"<pre><code>string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.llamagrammarelement/#returns_1","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.llamagrammarelement/#gethashcode","title":"GetHashCode()","text":"<pre><code>int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.native.llamagrammarelement/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamagrammarelement/#equalsobject","title":"Equals(Object)","text":"<pre><code>bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.native.llamagrammarelement/#parameters_1","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.native.llamagrammarelement/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamagrammarelement/#equalsllamagrammarelement","title":"Equals(LLamaGrammarElement)","text":"<pre><code>bool Equals(LLamaGrammarElement other)\n</code></pre>"},{"location":"xmldocs/llama.native.llamagrammarelement/#parameters_2","title":"Parameters","text":"<p><code>other</code> LLamaGrammarElement</p>"},{"location":"xmldocs/llama.native.llamagrammarelement/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamagrammarelementtype/","title":"LLamaGrammarElementType","text":"<p>Namespace: LLama.Native</p> <p>grammar element type</p> <pre><code>public enum LLamaGrammarElementType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaGrammarElementType Implements IComparable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.llamagrammarelementtype/#fields","title":"Fields","text":"Name Value Description END 0 end of rule definition ALT 1 start of alternate definition for rule RULE_REF 2 non-terminal element: reference to rule CHAR 3 terminal element: character (code point) CHAR_NOT 4 inverse char(s) ([^a], [^a-b] [^abc]) CHAR_RNG_UPPER 5 modifies a preceding CHAR or CHAR_ALT to be an inclusive range ([a-z]) CHAR_ALT 6 modifies a preceding CHAR or CHAR_RNG_UPPER to add an alternate char to match ([ab], [a-zA])"},{"location":"xmldocs/llama.native.llamakvcacheview/","title":"LLamaKvCacheView","text":"<p>Namespace: LLama.Native</p> <p>An updateable view of the KV cache (llama_kv_cache_view)</p> <pre><code>public struct LLamaKvCacheView\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaKvCacheView</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewcell/","title":"LLamaKvCacheViewCell","text":"<p>Namespace: LLama.Native</p> <p>Information associated with an individual cell in the KV cache view (llama_kv_cache_view_cell)</p> <pre><code>public struct LLamaKvCacheViewCell\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaKvCacheViewCell</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewcell/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamakvcacheviewcell/#pos","title":"pos","text":"<p>The position for this cell. Takes KV cache shifts into account.  May be negative if the cell is not populated.</p> <pre><code>public LLamaPos pos;\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/","title":"LLamaKvCacheViewSafeHandle","text":"<p>Namespace: LLama.Native</p> <p>A safe handle for a LLamaKvCacheView</p> <pre><code>public class LLamaKvCacheViewSafeHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 LLamaKvCacheViewSafeHandle Implements IDisposable</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#llamakvcacheviewsafehandlesafellamacontexthandle-llamakvcacheview","title":"LLamaKvCacheViewSafeHandle(SafeLLamaContextHandle, LLamaKvCacheView)","text":"<p>Initialize a LLamaKvCacheViewSafeHandle which will call <code>llama_kv_cache_view_free</code> when disposed</p> <pre><code>public LLamaKvCacheViewSafeHandle(SafeLLamaContextHandle ctx, LLamaKvCacheView view)\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#parameters","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>view</code> LLamaKvCacheView</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#allocatesafellamacontexthandle-int32","title":"Allocate(SafeLLamaContextHandle, Int32)","text":"<p>Allocate a new KV cache view which can be used to inspect the KV cache</p> <pre><code>public static LLamaKvCacheViewSafeHandle Allocate(SafeLLamaContextHandle ctx, int maxSequences)\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#parameters_1","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>maxSequences</code> Int32 The maximum number of sequences visible in this view per cell</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#returns","title":"Returns","text":"<p>LLamaKvCacheViewSafeHandle</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#update","title":"Update()","text":"<p>Update this view</p> <pre><code>public void Update()\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#getview","title":"GetView()","text":"<p>Get the raw KV cache view</p> <pre><code>public LLamaKvCacheView&amp; GetView()\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#returns_2","title":"Returns","text":"<p>LLamaKvCacheView&amp;</p>"},{"location":"xmldocs/llama.native.llamaloglevel/","title":"LLamaLogLevel","text":"<p>Namespace: LLama.Native</p> <p>Severity level of a log message</p> <pre><code>public enum LLamaLogLevel\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaLogLevel Implements IComparable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.llamaloglevel/#fields","title":"Fields","text":"Name Value Description Error 2 Logs that highlight when the current flow of execution is stopped due to a failure. Warning 3 Logs that highlight an abnormal or unexpected event in the application flow, but do not otherwise cause the application execution to stop. Info 4 Logs that track the general flow of the application. Debug 5 Logs that are used for interactive investigation during development."},{"location":"xmldocs/llama.native.llamamodelkvoverridetype/","title":"LLamaModelKvOverrideType","text":"<p>Namespace: LLama.Native</p> <p>Specifies what type of value is being overridden by LLamaModelKvOverride</p> <pre><code>public enum LLamaModelKvOverrideType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaModelKvOverrideType Implements IComparable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>llama_model_kv_override_type</p>"},{"location":"xmldocs/llama.native.llamamodelkvoverridetype/#fields","title":"Fields","text":"Name Value Description Int 0 Overriding an int value Float 1 Overriding a float value Bool 2 Overriding a bool value"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/","title":"LLamaModelMetadataOverride","text":"<p>Namespace: LLama.Native</p> <p>Override a key/value pair in the llama model metadata (llama_model_kv_override)</p> <pre><code>public struct LLamaModelMetadataOverride\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaModelMetadataOverride</p>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#key","title":"key","text":"<p>Key to override</p> <pre><code>public &lt;key&gt;e__FixedBuffer key;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#tag","title":"Tag","text":"<p>Type of value</p> <pre><code>public LLamaModelKvOverrideType Tag;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#intvalue","title":"IntValue","text":"<p>Value, must only be used if Tag == LLAMA_KV_OVERRIDE_INT</p> <pre><code>public long IntValue;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#floatvalue","title":"FloatValue","text":"<p>Value, must only be used if Tag == LLAMA_KV_OVERRIDE_FLOAT</p> <pre><code>public double FloatValue;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#boolvalue","title":"BoolValue","text":"<p>Value, must only be used if Tag == LLAMA_KV_OVERRIDE_BOOL</p> <pre><code>public long BoolValue;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/","title":"LLamaModelParams","text":"<p>Namespace: LLama.Native</p> <p>A C# representation of the llama.cpp <code>llama_model_params</code> struct</p> <pre><code>public struct LLamaModelParams\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaModelParams</p>"},{"location":"xmldocs/llama.native.llamamodelparams/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamamodelparams/#n_gpu_layers","title":"n_gpu_layers","text":"<p>// number of layers to store in VRAM</p> <pre><code>public int n_gpu_layers;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#split_mode","title":"split_mode","text":"<p>how to split the model across multiple GPUs</p> <pre><code>public GPUSplitMode split_mode;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#main_gpu","title":"main_gpu","text":"<p>the GPU that is used for scratch and small tensors</p> <pre><code>public int main_gpu;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#tensor_split","title":"tensor_split","text":"<p>how to split layers across multiple GPUs (size: NativeApi.llama_max_devices())</p> <pre><code>public Single* tensor_split;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#progress_callback","title":"progress_callback","text":"<p>called with a progress value between 0 and 1, pass NULL to disable. If the provided progress_callback  returns true, model loading continues. If it returns false, model loading is immediately aborted.</p> <pre><code>public LlamaProgressCallback progress_callback;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#progress_callback_user_data","title":"progress_callback_user_data","text":"<p>context pointer passed to the progress callback</p> <pre><code>public Void* progress_callback_user_data;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#kv_overrides","title":"kv_overrides","text":"<p>override key-value pairs of the model meta data</p> <pre><code>public LLamaModelMetadataOverride* kv_overrides;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamamodelparams/#vocab_only","title":"vocab_only","text":"<p>only load the vocabulary, no weights</p> <pre><code>public bool vocab_only { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelparams/#use_mmap","title":"use_mmap","text":"<p>use mmap if possible</p> <pre><code>public bool use_mmap { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelparams/#use_mlock","title":"use_mlock","text":"<p>force system to keep model in RAM</p> <pre><code>public bool use_mlock { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#property-value_2","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/","title":"LLamaModelQuantizeParams","text":"<p>Namespace: LLama.Native</p> <p>Quantizer parameters used in the native API</p> <pre><code>public struct LLamaModelQuantizeParams\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaModelQuantizeParams</p> <p>Remarks:</p> <p>llama_model_quantize_params</p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#nthread","title":"nthread","text":"<p>number of threads to use for quantizing, if &lt;=0 will use std::thread::hardware_concurrency()</p> <pre><code>public int nthread;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#ftype","title":"ftype","text":"<p>quantize to this llama_ftype</p> <pre><code>public LLamaFtype ftype;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#imatrix","title":"imatrix","text":"<p>pointer to importance matrix data</p> <pre><code>public IntPtr imatrix;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#allow_requantize","title":"allow_requantize","text":"<p>allow quantizing non-f32/f16 tensors</p> <pre><code>public bool allow_requantize { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#quantize_output_tensor","title":"quantize_output_tensor","text":"<p>quantize output.weight</p> <pre><code>public bool quantize_output_tensor { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#only_copy","title":"only_copy","text":"<p>only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored</p> <pre><code>public bool only_copy { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#property-value_2","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#pure","title":"pure","text":"<p>disable k-quant mixtures and quantize all tensors to the same type</p> <pre><code>public bool pure { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamanativebatch/","title":"LLamaNativeBatch","text":"<p>Namespace: LLama.Native</p> <p>Input data for llama_decode  A llama_batch object can contain input about one or many sequences  The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens</p> <pre><code>public struct LLamaNativeBatch\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaNativeBatch</p>"},{"location":"xmldocs/llama.native.llamanativebatch/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamanativebatch/#n_tokens","title":"n_tokens","text":"<p>The number of items pointed at by pos, seq_id and logits.</p> <pre><code>public int n_tokens;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#tokens","title":"tokens","text":"<p>Either <code>n_tokens</code> of <code>llama_token</code>, or <code>NULL</code>, depending on how this batch was created</p> <pre><code>public LLamaToken* tokens;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#embd","title":"embd","text":"<p>Either <code>n_tokens * embd * sizeof(float)</code> or <code>NULL</code>, depending on how this batch was created</p> <pre><code>public Single* embd;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#pos","title":"pos","text":"<p>the positions of the respective token in the sequence</p> <pre><code>public LLamaPos* pos;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#n_seq_id","title":"n_seq_id","text":"<p>https://github.com/ggerganov/llama.cpp/blob/master/llama.h#L139 ???</p> <pre><code>public Int32* n_seq_id;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#seq_id","title":"seq_id","text":"<p>the sequence to which the respective token belongs</p> <pre><code>public LLamaSeqId** seq_id;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#logits","title":"logits","text":"<p>if zero, the logits for the respective token will not be output</p> <pre><code>public Byte* logits;\n</code></pre>"},{"location":"xmldocs/llama.native.llamapoolingtype/","title":"LLamaPoolingType","text":"<p>Namespace: LLama.Native</p> <pre><code>public enum LLamaPoolingType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaPoolingType Implements IComparable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>llama_pooling_type</p>"},{"location":"xmldocs/llama.native.llamapoolingtype/#fields","title":"Fields","text":"Name Value Description"},{"location":"xmldocs/llama.native.llamapos/","title":"LLamaPos","text":"<p>Namespace: LLama.Native</p> <p>Indicates position in a sequence</p> <pre><code>public struct LLamaPos\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaPos Implements IEquatable&lt;LLamaPos&gt;</p>"},{"location":"xmldocs/llama.native.llamapos/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamapos/#value","title":"Value","text":"<p>The raw value</p> <pre><code>public int Value;\n</code></pre>"},{"location":"xmldocs/llama.native.llamapos/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamapos/#tostring","title":"ToString()","text":"<pre><code>string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.llamapos/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.llamapos/#gethashcode","title":"GetHashCode()","text":"<pre><code>int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.native.llamapos/#returns_1","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamapos/#equalsobject","title":"Equals(Object)","text":"<pre><code>bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.native.llamapos/#parameters","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.native.llamapos/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamapos/#equalsllamapos","title":"Equals(LLamaPos)","text":"<pre><code>bool Equals(LLamaPos other)\n</code></pre>"},{"location":"xmldocs/llama.native.llamapos/#parameters_1","title":"Parameters","text":"<p><code>other</code> LLamaPos</p>"},{"location":"xmldocs/llama.native.llamapos/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamaropetype/","title":"LLamaRopeType","text":"<p>Namespace: LLama.Native</p> <pre><code>public enum LLamaRopeType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaRopeType Implements IComparable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.llamaropetype/#fields","title":"Fields","text":"Name Value Description"},{"location":"xmldocs/llama.native.llamaseqid/","title":"LLamaSeqId","text":"<p>Namespace: LLama.Native</p> <p>ID for a sequence in a batch</p> <pre><code>public struct LLamaSeqId\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaSeqId Implements IEquatable&lt;LLamaSeqId&gt;</p>"},{"location":"xmldocs/llama.native.llamaseqid/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamaseqid/#value","title":"Value","text":"<p>The raw value</p> <pre><code>public int Value;\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#zero","title":"Zero","text":"<p>LLamaSeqId with value 0</p> <pre><code>public static LLamaSeqId Zero;\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamaseqid/#tostring","title":"ToString()","text":"<pre><code>string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.llamaseqid/#gethashcode","title":"GetHashCode()","text":"<pre><code>int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#returns_1","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamaseqid/#equalsobject","title":"Equals(Object)","text":"<pre><code>bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#parameters","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.native.llamaseqid/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamaseqid/#equalsllamaseqid","title":"Equals(LLamaSeqId)","text":"<pre><code>bool Equals(LLamaSeqId other)\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#parameters_1","title":"Parameters","text":"<p><code>other</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.native.llamaseqid/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamatoken/","title":"LLamaToken","text":"<p>Namespace: LLama.Native</p> <p>A single token</p> <pre><code>public struct LLamaToken\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaToken Implements IEquatable&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.native.llamatoken/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamatoken/#tostring","title":"ToString()","text":"<pre><code>string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.llamatoken/#gethashcode","title":"GetHashCode()","text":"<pre><code>int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#returns_1","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamatoken/#equalsobject","title":"Equals(Object)","text":"<pre><code>bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#parameters","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.native.llamatoken/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamatoken/#equalsllamatoken","title":"Equals(LLamaToken)","text":"<pre><code>bool Equals(LLamaToken other)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#parameters_1","title":"Parameters","text":"<p><code>other</code> LLamaToken</p>"},{"location":"xmldocs/llama.native.llamatoken/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamatokendata/","title":"LLamaTokenData","text":"<p>Namespace: LLama.Native</p> <p>A single token along with probability of this token being selected</p> <pre><code>public struct LLamaTokenData\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaTokenData</p>"},{"location":"xmldocs/llama.native.llamatokendata/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamatokendata/#id","title":"id","text":"<p>token id</p> <pre><code>public LLamaToken id;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendata/#logit","title":"logit","text":"<p>log-odds of the token</p> <pre><code>public float logit;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendata/#p","title":"p","text":"<p>probability of the token</p> <pre><code>public float p;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendata/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.llamatokendata/#llamatokendatallamatoken-single-single","title":"LLamaTokenData(LLamaToken, Single, Single)","text":"<p>Create a new LLamaTokenData</p> <pre><code>LLamaTokenData(LLamaToken id, float logit, float p)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendata/#parameters","title":"Parameters","text":"<p><code>id</code> LLamaToken</p> <p><code>logit</code> Single</p> <p><code>p</code> Single</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/","title":"LLamaTokenDataArray","text":"<p>Namespace: LLama.Native</p> <p>Contains an array of LLamaTokenData, potentially sorted.</p> <pre><code>public struct LLamaTokenDataArray\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaTokenDataArray</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamatokendataarray/#data","title":"data","text":"<p>The LLamaTokenData</p> <pre><code>public Memory&lt;LLamaTokenData&gt; data;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#sorted","title":"sorted","text":"<p>Indicates if <code>data</code> is sorted by logits in descending order. If this is false the token data is in no particular order.</p> <pre><code>public bool sorted;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.llamatokendataarray/#llamatokendataarraymemoryllamatokendata-boolean","title":"LLamaTokenDataArray(Memory&lt;LLamaTokenData&gt;, Boolean)","text":"<p>Create a new LLamaTokenDataArray</p> <pre><code>LLamaTokenDataArray(Memory&lt;LLamaTokenData&gt; tokens, bool isSorted)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters","title":"Parameters","text":"<p><code>tokens</code> Memory&lt;LLamaTokenData&gt;</p> <p><code>isSorted</code> Boolean</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamatokendataarray/#createreadonlyspansingle","title":"Create(ReadOnlySpan&lt;Single&gt;)","text":"<p>Create a new LLamaTokenDataArray, copying the data from the given logits</p> <pre><code>LLamaTokenDataArray Create(ReadOnlySpan&lt;float&gt; logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_1","title":"Parameters","text":"<p><code>logits</code> ReadOnlySpan&lt;Single&gt;</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#returns","title":"Returns","text":"<p>LLamaTokenDataArray</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#overwritelogitsreadonlyspanvaluetuplellamatoken-single","title":"OverwriteLogits(ReadOnlySpan&lt;ValueTuple&lt;LLamaToken, Single&gt;&gt;)","text":"<p>Overwrite the logit values for all given tokens</p> <pre><code>void OverwriteLogits(ReadOnlySpan&lt;ValueTuple&lt;LLamaToken, float&gt;&gt; values)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_2","title":"Parameters","text":"<p><code>values</code> ReadOnlySpan&lt;ValueTuple&lt;LLamaToken, Single&gt;&gt; tuples of token and logit value to overwrite</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#applygrammarsafellamacontexthandle-safellamagrammarhandle","title":"ApplyGrammar(SafeLLamaContextHandle, SafeLLamaGrammarHandle)","text":"<p>Apply grammar rules to candidate tokens</p> <pre><code>void ApplyGrammar(SafeLLamaContextHandle ctx, SafeLLamaGrammarHandle grammar)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_3","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>grammar</code> SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#topksafellamacontexthandle-int32-uint64","title":"TopK(SafeLLamaContextHandle, Int32, UInt64)","text":"<p>Top-K sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751</p> <pre><code>void TopK(SafeLLamaContextHandle context, int k, ulong minKeep)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_4","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p> <p><code>k</code> Int32 Number of tokens to keep</p> <p><code>minKeep</code> UInt64 Minimum number to keep</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#toppsafellamacontexthandle-single-uint64","title":"TopP(SafeLLamaContextHandle, Single, UInt64)","text":"<p>Nucleus sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751</p> <pre><code>void TopP(SafeLLamaContextHandle context, float p, ulong minKeep)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_5","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p> <p><code>p</code> Single</p> <p><code>minKeep</code> UInt64</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#minpsafellamacontexthandle-single-uint64","title":"MinP(SafeLLamaContextHandle, Single, UInt64)","text":"<p>Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841</p> <pre><code>void MinP(SafeLLamaContextHandle context, float p, ulong minKeep)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_6","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p> <p><code>p</code> Single All tokens with probability greater than this will be kept</p> <p><code>minKeep</code> UInt64</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#tailfreesafellamacontexthandle-single-uint64","title":"TailFree(SafeLLamaContextHandle, Single, UInt64)","text":"<p>Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.</p> <pre><code>void TailFree(SafeLLamaContextHandle context, float z, ulong min_keep)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_7","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p> <p><code>z</code> Single</p> <p><code>min_keep</code> UInt64</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#locallytypicalsafellamacontexthandle-single-uint64","title":"LocallyTypical(SafeLLamaContextHandle, Single, UInt64)","text":"<p>Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.</p> <pre><code>void LocallyTypical(SafeLLamaContextHandle context, float p, ulong min_keep)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_8","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p> <p><code>p</code> Single</p> <p><code>min_keep</code> UInt64</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#repetitionpenaltysafellamacontexthandle-readonlyspanllamatoken-single-single-single","title":"RepetitionPenalty(SafeLLamaContextHandle, ReadOnlySpan&lt;LLamaToken&gt;, Single, Single, Single)","text":"<p>Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.  Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.</p> <pre><code>void RepetitionPenalty(SafeLLamaContextHandle context, ReadOnlySpan&lt;LLamaToken&gt; last_tokens, float penalty_repeat, float penalty_freq, float penalty_present)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_9","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p> <p><code>last_tokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p> <p><code>penalty_repeat</code> Single</p> <p><code>penalty_freq</code> Single</p> <p><code>penalty_present</code> Single</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#guidancesafellamacontexthandle-readonlyspansingle-single","title":"Guidance(SafeLLamaContextHandle, ReadOnlySpan&lt;Single&gt;, Single)","text":"<p>Apply classifier-free guidance to the logits as described in academic paper \"Stay on topic with Classifier-Free Guidance\" https://arxiv.org/abs/2306.17806</p> <pre><code>void Guidance(SafeLLamaContextHandle context, ReadOnlySpan&lt;float&gt; guidanceLogits, float guidance)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_10","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p> <p><code>guidanceLogits</code> ReadOnlySpan&lt;Single&gt; Logits extracted from a separate context from the same model.  Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.</p> <p><code>guidance</code> Single Guidance strength. 0 means no guidance, higher values applies stronger guidance</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#temperaturesafellamacontexthandle-single","title":"Temperature(SafeLLamaContextHandle, Single)","text":"<p>Sample with temperature.  As temperature increases, the prediction becomes more diverse but also vulnerable to hallucinations -- generating tokens that are sensible but not factual</p> <pre><code>void Temperature(SafeLLamaContextHandle context, float temp)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_11","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p> <p><code>temp</code> Single</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#softmaxsafellamacontexthandle","title":"Softmax(SafeLLamaContextHandle)","text":"<p>Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.</p> <pre><code>void Softmax(SafeLLamaContextHandle context)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_12","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#sampletokensafellamacontexthandle","title":"SampleToken(SafeLLamaContextHandle)","text":"<p>Randomly selects a token from the candidates based on their probabilities.</p> <pre><code>LLamaToken SampleToken(SafeLLamaContextHandle context)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_13","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#returns_1","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#sampletokengreedysafellamacontexthandle","title":"SampleTokenGreedy(SafeLLamaContextHandle)","text":"<p>Selects the token with the highest probability.</p> <pre><code>LLamaToken SampleTokenGreedy(SafeLLamaContextHandle context)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_14","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#returns_2","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#sampletokenmirostatsafellamacontexthandle-single-single-int32-single","title":"SampleTokenMirostat(SafeLLamaContextHandle, Single, Single, Int32, Single&amp;)","text":"<p>Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.</p> <pre><code>LLamaToken SampleTokenMirostat(SafeLLamaContextHandle context, float tau, float eta, int m, Single&amp; mu)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_15","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p> <p><code>tau</code> Single The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</p> <p><code>eta</code> Single The learning rate used to update <code>mu</code> based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause <code>mu</code> to be updated more quickly, while a smaller learning rate will result in slower updates.</p> <p><code>m</code> Int32 The number of tokens considered in the estimation of <code>s_hat</code>. This is an arbitrary value that is used to calculate <code>s_hat</code>, which in turn helps to calculate the value of <code>k</code>. In the paper, they use <code>m = 100</code>, but you can experiment with different values to see how it affects the performance of the algorithm.</p> <p><code>mu</code> Single&amp; Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (<code>2 * tau</code>) and is updated in the algorithm based on the error between the target and observed surprisal.</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#returns_3","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#sampletokenmirostat2safellamacontexthandle-single-single-single","title":"SampleTokenMirostat2(SafeLLamaContextHandle, Single, Single, Single&amp;)","text":"<p>Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.</p> <pre><code>LLamaToken SampleTokenMirostat2(SafeLLamaContextHandle context, float tau, float eta, Single&amp; mu)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_16","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p> <p><code>tau</code> Single The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</p> <p><code>eta</code> Single The learning rate used to update <code>mu</code> based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause <code>mu</code> to be updated more quickly, while a smaller learning rate will result in slower updates.</p> <p><code>mu</code> Single&amp; Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (<code>2 * tau</code>) and is updated in the algorithm based on the error between the target and observed surprisal.</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#returns_4","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/","title":"LLamaTokenDataArrayNative","text":"<p>Namespace: LLama.Native</p> <p>Contains a pointer to an array of LLamaTokenData which is pinned in memory.</p> <pre><code>public struct LLamaTokenDataArrayNative\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaTokenDataArrayNative</p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamatokendataarraynative/#data","title":"data","text":"<p>A pointer to an array of LlamaTokenData</p> <pre><code>public IntPtr data;\n</code></pre> <p>Remarks:</p> <p>Memory must be pinned in place for all the time this LLamaTokenDataArrayNative is in use</p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#size","title":"size","text":"<p>Number of LLamaTokenData in the array</p> <pre><code>public ulong size;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamatokendataarraynative/#sorted","title":"sorted","text":"<p>Indicates if the items in the array are sorted</p> <pre><code>public bool sorted { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamatokendataarraynative/#createllamatokendataarray-llamatokendataarraynative","title":"Create(LLamaTokenDataArray, LLamaTokenDataArrayNative&amp;)","text":"<p>Create a new LLamaTokenDataArrayNative around the data in the LLamaTokenDataArray</p> <pre><code>MemoryHandle Create(LLamaTokenDataArray array, LLamaTokenDataArrayNative&amp; native)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#parameters","title":"Parameters","text":"<p><code>array</code> LLamaTokenDataArray Data source</p> <p><code>native</code> LLamaTokenDataArrayNative&amp; Created native array</p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#returns","title":"Returns","text":"<p>MemoryHandle A memory handle, pinning the data in place until disposed</p>"},{"location":"xmldocs/llama.native.llamatokentype/","title":"LLamaTokenType","text":"<p>Namespace: LLama.Native</p> <p>Token Types</p> <pre><code>public enum LLamaTokenType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaTokenType Implements IComparable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>C# equivalent of llama_token_get_type</p>"},{"location":"xmldocs/llama.native.llamatokentype/#fields","title":"Fields","text":"Name Value Description LLAMA_TOKEN_TYPE_UNDEFINED 0 No specific type has been set for this token LLAMA_TOKEN_TYPE_NORMAL 1 This is a \"normal\" token LLAMA_TOKEN_TYPE_UNKNOWN 2 An \"unknown\" character/text token e.g. &lt;unk&gt; LLAMA_TOKEN_TYPE_CONTROL 3 A special control token e.g. &lt;/s&gt;"},{"location":"xmldocs/llama.native.llamavocabtype/","title":"LLamaVocabType","text":"<p>Namespace: LLama.Native</p> <pre><code>public enum LLamaVocabType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaVocabType Implements IComparable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>llama_vocab_type</p>"},{"location":"xmldocs/llama.native.llamavocabtype/#fields","title":"Fields","text":"Name Value Description"},{"location":"xmldocs/llama.native.llavaimageembed/","title":"LLavaImageEmbed","text":"<p>Namespace: LLama.Native</p> <p>LLaVa Image embeddings</p> <pre><code>public struct LLavaImageEmbed\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLavaImageEmbed</p>"},{"location":"xmldocs/llama.native.llavaimageembed/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llavaimageembed/#embed","title":"embed","text":"<pre><code>public Single* embed;\n</code></pre>"},{"location":"xmldocs/llama.native.llavaimageembed/#n_image_pos","title":"n_image_pos","text":"<pre><code>public int n_image_pos;\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/","title":"NativeApi","text":"<p>Namespace: LLama.Native</p> <p>Direct translation of the llama.cpp API</p> <pre><code>public static class NativeApi\n</code></pre> <p>Inheritance Object \u2192 NativeApi</p>"},{"location":"xmldocs/llama.native.nativeapi/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_token_mirostatsafellamacontexthandle-llamatokendataarraynative-single-single-int32-single","title":"llama_sample_token_mirostat(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Int32, Single&amp;)","text":"<p>Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.</p> <pre><code>public static LLamaToken llama_sample_token_mirostat(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float tau, float eta, int m, Single&amp; mu)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; A vector of <code>llama_token_data</code> containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.</p> <p><code>tau</code> Single The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</p> <p><code>eta</code> Single The learning rate used to update <code>mu</code> based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause <code>mu</code> to be updated more quickly, while a smaller learning rate will result in slower updates.</p> <p><code>m</code> Int32 The number of tokens considered in the estimation of <code>s_hat</code>. This is an arbitrary value that is used to calculate <code>s_hat</code>, which in turn helps to calculate the value of <code>k</code>. In the paper, they use <code>m = 100</code>, but you can experiment with different values to see how it affects the performance of the algorithm.</p> <p><code>mu</code> Single&amp; Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (<code>2 * tau</code>) and is updated in the algorithm based on the error between the target and observed surprisal.</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_token_mirostat_v2safellamacontexthandle-llamatokendataarraynative-single-single-single","title":"llama_sample_token_mirostat_v2(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Single&amp;)","text":"<p>Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.</p> <pre><code>public static LLamaToken llama_sample_token_mirostat_v2(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float tau, float eta, Single&amp; mu)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_1","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; A vector of <code>llama_token_data</code> containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.</p> <p><code>tau</code> Single The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</p> <p><code>eta</code> Single The learning rate used to update <code>mu</code> based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause <code>mu</code> to be updated more quickly, while a smaller learning rate will result in slower updates.</p> <p><code>mu</code> Single&amp; Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (<code>2 * tau</code>) and is updated in the algorithm based on the error between the target and observed surprisal.</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_1","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_token_greedysafellamacontexthandle-llamatokendataarraynative","title":"llama_sample_token_greedy(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)","text":"<p>Selects the token with the highest probability.</p> <pre><code>public static LLamaToken llama_sample_token_greedy(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_2","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; Pointer to LLamaTokenDataArray</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_2","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_tokensafellamacontexthandle-llamatokendataarraynative","title":"llama_sample_token(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)","text":"<p>Randomly selects a token from the candidates based on their probabilities.</p> <pre><code>public static LLamaToken llama_sample_token(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_3","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; Pointer to LLamaTokenDataArray</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_3","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_get_embeddingsg__llama_get_embeddings_native30_0safellamacontexthandle","title":"&lt;llama_get_embeddings&gt;g__llama_get_embeddings_native|30_0(SafeLLamaContextHandle)","text":"<pre><code>internal static Single* &lt;llama_get_embeddings&gt;g__llama_get_embeddings_native|30_0(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_4","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_4","title":"Returns","text":"<p>Single*</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_to_pieceg__llama_token_to_piece_native44_0safellamamodelhandle-llamatoken-byte-int32","title":"&lt;llama_token_to_piece&gt;g__llama_token_to_piece_native|44_0(SafeLlamaModelHandle, LLamaToken, Byte*, Int32)","text":"<pre><code>internal static int &lt;llama_token_to_piece&gt;g__llama_token_to_piece_native|44_0(SafeLlamaModelHandle model, LLamaToken llamaToken, Byte* buffer, int length)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_5","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>llamaToken</code> LLamaToken</p> <p><code>buffer</code> Byte*</p> <p><code>length</code> Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_5","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#tryloadlibrariesg__tryload84_0string","title":"&lt;TryLoadLibraries&gt;g__TryLoad|84_0(String)","text":"<pre><code>internal static IntPtr &lt;TryLoadLibraries&gt;g__TryLoad|84_0(string path)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_6","title":"Parameters","text":"<p><code>path</code> String</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_6","title":"Returns","text":"<p>IntPtr</p>"},{"location":"xmldocs/llama.native.nativeapi/#tryloadlibrariesg__tryfindpath84_1string-c__displayclass84_0","title":"&lt;TryLoadLibraries&gt;g__TryFindPath|84_1(String, &lt;&gt;c__DisplayClass84_0&amp;)","text":"<pre><code>internal static string &lt;TryLoadLibraries&gt;g__TryFindPath|84_1(string filename, &lt;&gt;c__DisplayClass84_0&amp; )\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_7","title":"Parameters","text":"<p><code>filename</code> String</p> <p>`` &lt;&gt;c__DisplayClass84_0&amp;</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_7","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_set_n_threadssafellamacontexthandle-uint32-uint32","title":"llama_set_n_threads(SafeLLamaContextHandle, UInt32, UInt32)","text":"<p>Set the number of threads used for decoding</p> <pre><code>public static void llama_set_n_threads(SafeLLamaContextHandle ctx, uint n_threads, uint n_threads_batch)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_8","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>n_threads</code> UInt32 n_threads is the number of threads used for generation (single token)</p> <p><code>n_threads_batch</code> UInt32 n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_vocab_typesafellamamodelhandle","title":"llama_vocab_type(SafeLlamaModelHandle)","text":"<pre><code>public static LLamaVocabType llama_vocab_type(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_9","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_8","title":"Returns","text":"<p>LLamaVocabType</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_rope_typesafellamamodelhandle","title":"llama_rope_type(SafeLlamaModelHandle)","text":"<pre><code>public static LLamaRopeType llama_rope_type(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_10","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_9","title":"Returns","text":"<p>LLamaRopeType</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_grammar_initllamagrammarelement-uint64-uint64","title":"llama_grammar_init(LLamaGrammarElement, UInt64, UInt64)**","text":"<p>Create a new grammar from the given set of grammar rules</p> <pre><code>public static IntPtr llama_grammar_init(LLamaGrammarElement** rules, ulong n_rules, ulong start_rule_index)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_11","title":"Parameters","text":"<p><code>rules</code> LLamaGrammarElement**</p> <p><code>n_rules</code> UInt64</p> <p><code>start_rule_index</code> UInt64</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_10","title":"Returns","text":"<p>IntPtr</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_grammar_freeintptr","title":"llama_grammar_free(IntPtr)","text":"<p>Free all memory from the given SafeLLamaGrammarHandle</p> <pre><code>public static void llama_grammar_free(IntPtr grammar)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_12","title":"Parameters","text":"<p><code>grammar</code> IntPtr</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_grammar_copysafellamagrammarhandle","title":"llama_grammar_copy(SafeLLamaGrammarHandle)","text":"<p>Create a copy of an existing grammar instance</p> <pre><code>public static IntPtr llama_grammar_copy(SafeLLamaGrammarHandle grammar)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_13","title":"Parameters","text":"<p><code>grammar</code> SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_11","title":"Returns","text":"<p>IntPtr</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_grammarsafellamacontexthandle-llamatokendataarraynative-safellamagrammarhandle","title":"llama_sample_grammar(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, SafeLLamaGrammarHandle)","text":"<p>Apply constraints from grammar</p> <pre><code>public static void llama_sample_grammar(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, SafeLLamaGrammarHandle grammar)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_14","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp;</p> <p><code>grammar</code> SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_grammar_accept_tokensafellamacontexthandle-safellamagrammarhandle-llamatoken","title":"llama_grammar_accept_token(SafeLLamaContextHandle, SafeLLamaGrammarHandle, LLamaToken)","text":"<p>Accepts the sampled token into the grammar</p> <pre><code>public static void llama_grammar_accept_token(SafeLLamaContextHandle ctx, SafeLLamaGrammarHandle grammar, LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_15","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>grammar</code> SafeLLamaGrammarHandle</p> <p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.native.nativeapi/#llava_validate_embed_sizesafellamacontexthandle-safellavamodelhandle","title":"llava_validate_embed_size(SafeLLamaContextHandle, SafeLlavaModelHandle)","text":"<p>Sanity check for clip &lt;-&gt; llava embed size match</p> <pre><code>public static bool llava_validate_embed_size(SafeLLamaContextHandle ctxLlama, SafeLlavaModelHandle ctxClip)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_16","title":"Parameters","text":"<p><code>ctxLlama</code> SafeLLamaContextHandle LLama Context</p> <p><code>ctxClip</code> SafeLlavaModelHandle Llava Model</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_12","title":"Returns","text":"<p>Boolean True if validate successfully</p>"},{"location":"xmldocs/llama.native.nativeapi/#llava_image_embed_make_with_bytessafellavamodelhandle-int32-byte-int32","title":"llava_image_embed_make_with_bytes(SafeLlavaModelHandle, Int32, Byte[], Int32)","text":"<p>Build an image embed from image file bytes</p> <pre><code>public static SafeLlavaImageEmbedHandle llava_image_embed_make_with_bytes(SafeLlavaModelHandle ctx_clip, int n_threads, Byte[] image_bytes, int image_bytes_length)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_17","title":"Parameters","text":"<p><code>ctx_clip</code> SafeLlavaModelHandle SafeHandle to the Clip Model</p> <p><code>n_threads</code> Int32 Number of threads</p> <p><code>image_bytes</code> Byte[] Binary image in jpeg format</p> <p><code>image_bytes_length</code> Int32 Bytes lenght of the image</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_13","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle SafeHandle to the Embeddings</p>"},{"location":"xmldocs/llama.native.nativeapi/#llava_image_embed_make_with_filenamesafellavamodelhandle-int32-string","title":"llava_image_embed_make_with_filename(SafeLlavaModelHandle, Int32, String)","text":"<p>Build an image embed from a path to an image filename</p> <pre><code>public static SafeLlavaImageEmbedHandle llava_image_embed_make_with_filename(SafeLlavaModelHandle ctx_clip, int n_threads, string image_path)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_18","title":"Parameters","text":"<p><code>ctx_clip</code> SafeLlavaModelHandle SafeHandle to the Clip Model</p> <p><code>n_threads</code> Int32 Number of threads</p> <p><code>image_path</code> String Image filename (jpeg) to generate embeddings</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_14","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle SafeHandel to the embeddings</p>"},{"location":"xmldocs/llama.native.nativeapi/#llava_image_embed_freeintptr","title":"llava_image_embed_free(IntPtr)","text":"<p>Free an embedding made with llava_image_embed_make_*</p> <pre><code>public static void llava_image_embed_free(IntPtr embed)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_19","title":"Parameters","text":"<p><code>embed</code> IntPtr Embeddings to release</p>"},{"location":"xmldocs/llama.native.nativeapi/#llava_eval_image_embedsafellamacontexthandle-safellavaimageembedhandle-int32-int32","title":"llava_eval_image_embed(SafeLLamaContextHandle, SafeLlavaImageEmbedHandle, Int32, Int32&amp;)","text":"<p>Write the image represented by embed into the llama context with batch size n_batch, starting at context  pos n_past. on completion, n_past points to the next position in the context after the image embed.</p> <pre><code>public static bool llava_eval_image_embed(SafeLLamaContextHandle ctx_llama, SafeLlavaImageEmbedHandle embed, int n_batch, Int32&amp; n_past)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_20","title":"Parameters","text":"<p><code>ctx_llama</code> SafeLLamaContextHandle Llama Context</p> <p><code>embed</code> SafeLlavaImageEmbedHandle Embedding handle</p> <p><code>n_batch</code> Int32</p> <p><code>n_past</code> Int32&amp;</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_15","title":"Returns","text":"<p>Boolean True on success</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_model_quantizestring-string-llamamodelquantizeparams","title":"llama_model_quantize(String, String, LLamaModelQuantizeParams*)","text":"<p>Returns 0 on success</p> <pre><code>public static uint llama_model_quantize(string fname_inp, string fname_out, LLamaModelQuantizeParams* param)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_21","title":"Parameters","text":"<p><code>fname_inp</code> String</p> <p><code>fname_out</code> String</p> <p><code>param</code> LLamaModelQuantizeParams*</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_16","title":"Returns","text":"<p>UInt32 Returns 0 on success</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_repetition_penaltiessafellamacontexthandle-llamatokendataarraynative-llamatoken-uint64-single-single-single","title":"llama_sample_repetition_penalties(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, LLamaToken*, UInt64, Single, Single, Single)","text":"<p>Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.  Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.</p> <pre><code>public static void llama_sample_repetition_penalties(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, LLamaToken* last_tokens, ulong last_tokens_size, float penalty_repeat, float penalty_freq, float penalty_present)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_22","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; Pointer to LLamaTokenDataArray</p> <p><code>last_tokens</code> LLamaToken*</p> <p><code>last_tokens_size</code> UInt64</p> <p><code>penalty_repeat</code> Single Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.</p> <p><code>penalty_freq</code> Single Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.</p> <p><code>penalty_present</code> Single Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_apply_guidancesafellamacontexthandle-spansingle-readonlyspansingle-single","title":"llama_sample_apply_guidance(SafeLLamaContextHandle, Span&lt;Single&gt;, ReadOnlySpan&lt;Single&gt;, Single)","text":"<p>Apply classifier-free guidance to the logits as described in academic paper \"Stay on topic with Classifier-Free Guidance\" https://arxiv.org/abs/2306.17806</p> <pre><code>public static void llama_sample_apply_guidance(SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, ReadOnlySpan&lt;float&gt; logits_guidance, float scale)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_23","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>logits</code> Span&lt;Single&gt; Logits extracted from the original generation context.</p> <p><code>logits_guidance</code> ReadOnlySpan&lt;Single&gt; Logits extracted from a separate context from the same model.  Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.</p> <p><code>scale</code> Single Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance.</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_apply_guidancesafellamacontexthandle-single-single-single","title":"llama_sample_apply_guidance(SafeLLamaContextHandle, Single, Single, Single)","text":"<p>Apply classifier-free guidance to the logits as described in academic paper \"Stay on topic with Classifier-Free Guidance\" https://arxiv.org/abs/2306.17806</p> <pre><code>public static void llama_sample_apply_guidance(SafeLLamaContextHandle ctx, Single* logits, Single* logits_guidance, float scale)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_24","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>logits</code> Single* Logits extracted from the original generation context.</p> <p><code>logits_guidance</code> Single* Logits extracted from a separate context from the same model.  Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.</p> <p><code>scale</code> Single Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance.</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_softmaxsafellamacontexthandle-llamatokendataarraynative","title":"llama_sample_softmax(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)","text":"<p>Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.</p> <pre><code>public static void llama_sample_softmax(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_25","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; Pointer to LLamaTokenDataArray</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_top_ksafellamacontexthandle-llamatokendataarraynative-int32-uint64","title":"llama_sample_top_k(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Int32, UInt64)","text":"<p>Top-K sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751</p> <pre><code>public static void llama_sample_top_k(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, int k, ulong min_keep)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_26","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; Pointer to LLamaTokenDataArray</p> <p><code>k</code> Int32</p> <p><code>min_keep</code> UInt64</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_top_psafellamacontexthandle-llamatokendataarraynative-single-uint64","title":"llama_sample_top_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)","text":"<p>Nucleus sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751</p> <pre><code>public static void llama_sample_top_p(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float p, ulong min_keep)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_27","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; Pointer to LLamaTokenDataArray</p> <p><code>p</code> Single</p> <p><code>min_keep</code> UInt64</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_min_psafellamacontexthandle-llamatokendataarraynative-single-uint64","title":"llama_sample_min_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)","text":"<p>Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841</p> <pre><code>public static void llama_sample_min_p(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float p, ulong min_keep)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_28","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; Pointer to LLamaTokenDataArray</p> <p><code>p</code> Single</p> <p><code>min_keep</code> UInt64</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_tail_freesafellamacontexthandle-llamatokendataarraynative-single-uint64","title":"llama_sample_tail_free(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)","text":"<p>Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.</p> <pre><code>public static void llama_sample_tail_free(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float z, ulong min_keep)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_29","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; Pointer to LLamaTokenDataArray</p> <p><code>z</code> Single</p> <p><code>min_keep</code> UInt64</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_typicalsafellamacontexthandle-llamatokendataarraynative-single-uint64","title":"llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)","text":"<p>Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.</p> <pre><code>public static void llama_sample_typical(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float p, ulong min_keep)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_30","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; Pointer to LLamaTokenDataArray</p> <p><code>p</code> Single</p> <p><code>min_keep</code> UInt64</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_typicalsafellamacontexthandle-llamatokendataarraynative-single-single-single","title":"llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Single)","text":"<p>Dynamic temperature implementation described in the paper https://arxiv.org/abs/2309.02772.</p> <pre><code>public static void llama_sample_typical(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float min_temp, float max_temp, float exponent_val)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_31","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp; Pointer to LLamaTokenDataArray</p> <p><code>min_temp</code> Single</p> <p><code>max_temp</code> Single</p> <p><code>exponent_val</code> Single</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_sample_tempsafellamacontexthandle-llamatokendataarraynative-single","title":"llama_sample_temp(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single)","text":"<p>Modify logits by temperature</p> <pre><code>public static void llama_sample_temp(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float temp)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_32","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArrayNative&amp;</p> <p><code>temp</code> Single</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_get_embeddingssafellamacontexthandle","title":"llama_get_embeddings(SafeLLamaContextHandle)","text":"<p>Get the embeddings for the input</p> <pre><code>public static Span&lt;float&gt; llama_get_embeddings(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_33","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_17","title":"Returns","text":"<p>Span&lt;Single&gt;</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_chat_apply_templatesafellamamodelhandle-char-llamachatmessage-intptr-boolean-char-int32","title":"llama_chat_apply_template(SafeLlamaModelHandle, Char, LLamaChatMessage, IntPtr, Boolean, Char*, Int32)","text":"<p>Apply chat template. Inspired by hf apply_chat_template() on python.  Both \"model\" and \"custom_template\" are optional, but at least one is required. \"custom_template\" has higher precedence than \"model\"  NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template</p> <pre><code>public static int llama_chat_apply_template(SafeLlamaModelHandle model, Char* tmpl, LLamaChatMessage* chat, IntPtr n_msg, bool add_ass, Char* buf, int length)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_34","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>tmpl</code> Char* A Jinja template to use for this chat. If this is nullptr, the model\u2019s default chat template will be used instead.</p> <p><code>chat</code> LLamaChatMessage* Pointer to a list of multiple llama_chat_message</p> <p><code>n_msg</code> IntPtr Number of llama_chat_message in this chat</p> <p><code>add_ass</code> Boolean Whether to end the prompt with the token(s) that indicate the start of an assistant message.</p> <p><code>buf</code> Char* A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)</p> <p><code>length</code> Int32 The size of the allocated buffer</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_18","title":"Returns","text":"<p>Int32 The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template.</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_bossafellamamodelhandle","title":"llama_token_bos(SafeLlamaModelHandle)","text":"<p>Get the \"Beginning of sentence\" token</p> <pre><code>public static LLamaToken llama_token_bos(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_35","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_19","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_eossafellamamodelhandle","title":"llama_token_eos(SafeLlamaModelHandle)","text":"<p>Get the \"End of sentence\" token</p> <pre><code>public static LLamaToken llama_token_eos(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_36","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_20","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_nlsafellamamodelhandle","title":"llama_token_nl(SafeLlamaModelHandle)","text":"<p>Get the \"new line\" token</p> <pre><code>public static LLamaToken llama_token_nl(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_37","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_21","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_add_bos_tokensafellamamodelhandle","title":"llama_add_bos_token(SafeLlamaModelHandle)","text":"<p>Returns -1 if unknown, 1 for true or 0 for false.</p> <pre><code>public static int llama_add_bos_token(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_38","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_22","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_add_eos_tokensafellamamodelhandle","title":"llama_add_eos_token(SafeLlamaModelHandle)","text":"<p>Returns -1 if unknown, 1 for true or 0 for false.</p> <pre><code>public static int llama_add_eos_token(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_39","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_23","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_prefixsafellamamodelhandle","title":"llama_token_prefix(SafeLlamaModelHandle)","text":"<p>codellama infill tokens, Beginning of infill prefix</p> <pre><code>public static int llama_token_prefix(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_40","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_24","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_middlesafellamamodelhandle","title":"llama_token_middle(SafeLlamaModelHandle)","text":"<p>codellama infill tokens, Beginning of infill middle</p> <pre><code>public static int llama_token_middle(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_41","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_25","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_suffixsafellamamodelhandle","title":"llama_token_suffix(SafeLlamaModelHandle)","text":"<p>codellama infill tokens, Beginning of infill suffix</p> <pre><code>public static int llama_token_suffix(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_42","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_26","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_eotsafellamamodelhandle","title":"llama_token_eot(SafeLlamaModelHandle)","text":"<p>codellama infill tokens, End of infill middle</p> <pre><code>public static int llama_token_eot(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_43","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_27","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_print_timingssafellamacontexthandle","title":"llama_print_timings(SafeLLamaContextHandle)","text":"<p>Print out timing information for this context</p> <pre><code>public static void llama_print_timings(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_44","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_reset_timingssafellamacontexthandle","title":"llama_reset_timings(SafeLLamaContextHandle)","text":"<p>Reset all collected timing information for this context</p> <pre><code>public static void llama_reset_timings(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_45","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_print_system_info","title":"llama_print_system_info()","text":"<p>Print system information</p> <pre><code>public static IntPtr llama_print_system_info()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_28","title":"Returns","text":"<p>IntPtr</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_to_piecesafellamamodelhandle-llamatoken-spanbyte","title":"llama_token_to_piece(SafeLlamaModelHandle, LLamaToken, Span&lt;Byte&gt;)","text":"<p>Convert a single token into text</p> <pre><code>public static int llama_token_to_piece(SafeLlamaModelHandle model, LLamaToken llamaToken, Span&lt;byte&gt; buffer)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_46","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>llamaToken</code> LLamaToken</p> <p><code>buffer</code> Span&lt;Byte&gt; buffer to write string into</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_29","title":"Returns","text":"<p>Int32 The length written, or if the buffer is too small a negative that indicates the length required</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_tokenizesafellamamodelhandle-byte-int32-llamatoken-int32-boolean-boolean","title":"llama_tokenize(SafeLlamaModelHandle, Byte, Int32, LLamaToken, Int32, Boolean, Boolean)","text":"<p>Convert text into tokens</p> <pre><code>public static int llama_tokenize(SafeLlamaModelHandle model, Byte* text, int text_len, LLamaToken* tokens, int n_max_tokens, bool add_bos, bool special)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_47","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>text</code> Byte*</p> <p><code>text_len</code> Int32</p> <p><code>tokens</code> LLamaToken*</p> <p><code>n_max_tokens</code> Int32</p> <p><code>add_bos</code> Boolean</p> <p><code>special</code> Boolean Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext. Does not insert a leading space.</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_30","title":"Returns","text":"<p>Int32 Returns the number of tokens on success, no more than n_max_tokens.  Returns a negative number on failure - the number of tokens that would have been returned</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_log_setllamalogcallback","title":"llama_log_set(LLamaLogCallback)","text":"<p>Register a callback to receive llama log messages</p> <pre><code>public static void llama_log_set(LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_48","title":"Parameters","text":"<p><code>logCallback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_clearsafellamacontexthandle","title":"llama_kv_cache_clear(SafeLLamaContextHandle)","text":"<p>Clear the KV cache</p> <pre><code>public static void llama_kv_cache_clear(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_49","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_seq_rmsafellamacontexthandle-llamaseqid-llamapos-llamapos","title":"llama_kv_cache_seq_rm(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos)","text":"<p>Removes all tokens that belong to the specified sequence and have positions in [p0, p1)</p> <pre><code>public static void llama_kv_cache_seq_rm(SafeLLamaContextHandle ctx, LLamaSeqId seq, LLamaPos p0, LLamaPos p1)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_50","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>seq</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_seq_cpsafellamacontexthandle-llamaseqid-llamaseqid-llamapos-llamapos","title":"llama_kv_cache_seq_cp(SafeLLamaContextHandle, LLamaSeqId, LLamaSeqId, LLamaPos, LLamaPos)","text":"<p>Copy all tokens that belong to the specified sequence to another sequence  Note that this does not allocate extra KV cache memory - it simply assigns the tokens to the new sequence</p> <pre><code>public static void llama_kv_cache_seq_cp(SafeLLamaContextHandle ctx, LLamaSeqId src, LLamaSeqId dest, LLamaPos p0, LLamaPos p1)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_51","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>src</code> LLamaSeqId</p> <p><code>dest</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_seq_keepsafellamacontexthandle-llamaseqid","title":"llama_kv_cache_seq_keep(SafeLLamaContextHandle, LLamaSeqId)","text":"<p>Removes all tokens that do not belong to the specified sequence</p> <pre><code>public static void llama_kv_cache_seq_keep(SafeLLamaContextHandle ctx, LLamaSeqId seq)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_52","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>seq</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_seq_addsafellamacontexthandle-llamaseqid-llamapos-llamapos-int32","title":"llama_kv_cache_seq_add(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)","text":"<p>Adds relative position \"delta\" to all tokens that belong to the specified sequence and have positions in [p0, p1)  If the KV cache is RoPEd, the KV data is updated accordingly:  - lazily on next llama_decode()  - explicitly with llama_kv_cache_update()</p> <pre><code>public static void llama_kv_cache_seq_add(SafeLLamaContextHandle ctx, LLamaSeqId seq, LLamaPos p0, LLamaPos p1, int delta)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_53","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>seq</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p> <p><code>delta</code> Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_seq_divsafellamacontexthandle-llamaseqid-llamapos-llamapos-int32","title":"llama_kv_cache_seq_div(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)","text":"<p>Integer division of the positions by factor of <code>d &amp;gt; 1</code>  If the KV cache is RoPEd, the KV data is updated accordingly:  - lazily on next llama_decode()  - explicitly with llama_kv_cache_update()    p0 &lt; 0 : [0, p1]    p1 &lt; 0 : [p0, inf)</p> <pre><code>public static void llama_kv_cache_seq_div(SafeLLamaContextHandle ctx, LLamaSeqId seq, LLamaPos p0, LLamaPos p1, int d)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_54","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>seq</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p> <p><code>d</code> Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_seq_pos_maxsafellamacontexthandle-llamaseqid","title":"llama_kv_cache_seq_pos_max(SafeLLamaContextHandle, LLamaSeqId)","text":"<p>Returns the largest position present in the KV cache for the specified sequence</p> <pre><code>public static LLamaPos llama_kv_cache_seq_pos_max(SafeLLamaContextHandle ctx, LLamaSeqId seq)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_55","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>seq</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_31","title":"Returns","text":"<p>LLamaPos</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_defragsafellamacontexthandle","title":"llama_kv_cache_defrag(SafeLLamaContextHandle)","text":"<p>Defragment the KV cache. This will be applied:  - lazily on next llama_decode()  - explicitly with llama_kv_cache_update()</p> <pre><code>public static LLamaPos llama_kv_cache_defrag(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_56","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_32","title":"Returns","text":"<p>LLamaPos</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_updatesafellamacontexthandle","title":"llama_kv_cache_update(SafeLLamaContextHandle)","text":"<p>Apply the KV cache updates (such as K-shifts, defragmentation, etc.)</p> <pre><code>public static void llama_kv_cache_update(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_57","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_batch_initint32-int32-int32","title":"llama_batch_init(Int32, Int32, Int32)","text":"<p>Allocates a batch of tokens on the heap  Each token can be assigned up to n_seq_max sequence ids  The batch has to be freed with llama_batch_free()  If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)  Otherwise, llama_batch.token will be allocated to store n_tokens llama_token  The rest of the llama_batch members are allocated with size n_tokens  All members are left uninitialized</p> <pre><code>public static LLamaNativeBatch llama_batch_init(int n_tokens, int embd, int n_seq_max)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_58","title":"Parameters","text":"<p><code>n_tokens</code> Int32</p> <p><code>embd</code> Int32</p> <p><code>n_seq_max</code> Int32 Each token can be assigned up to n_seq_max sequence ids</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_33","title":"Returns","text":"<p>LLamaNativeBatch</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_batch_freellamanativebatch","title":"llama_batch_free(LLamaNativeBatch)","text":"<p>Frees a batch of tokens allocated with llama_batch_init()</p> <pre><code>public static void llama_batch_free(LLamaNativeBatch batch)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_59","title":"Parameters","text":"<p><code>batch</code> LLamaNativeBatch</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_decodesafellamacontexthandle-llamanativebatch","title":"llama_decode(SafeLLamaContextHandle, LLamaNativeBatch)","text":"<pre><code>public static int llama_decode(SafeLLamaContextHandle ctx, LLamaNativeBatch batch)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_60","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>batch</code> LLamaNativeBatch</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_34","title":"Returns","text":"<p>Int32 Positive return values does not mean a fatal error, but rather a warning:  - 0: success  - 1: could not find a KV slot for the batch (try reducing the size of the batch or increase the context)  - &lt; 0: error</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_view_initsafellamacontexthandle-int32","title":"llama_kv_cache_view_init(SafeLLamaContextHandle, Int32)","text":"<p>Create an empty KV cache view. (use only for debugging purposes)</p> <pre><code>public static LLamaKvCacheView llama_kv_cache_view_init(SafeLLamaContextHandle ctx, int n_max_seq)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_61","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>n_max_seq</code> Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_35","title":"Returns","text":"<p>LLamaKvCacheView</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_view_freellamakvcacheview","title":"llama_kv_cache_view_free(LLamaKvCacheView&amp;)","text":"<p>Free a KV cache view. (use only for debugging purposes)</p> <pre><code>public static void llama_kv_cache_view_free(LLamaKvCacheView&amp; view)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_62","title":"Parameters","text":"<p><code>view</code> LLamaKvCacheView&amp;</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_cache_view_updatesafellamacontexthandle-llamakvcacheview","title":"llama_kv_cache_view_update(SafeLLamaContextHandle, LLamaKvCacheView&amp;)","text":"<p>Update the KV cache view structure with the current state of the KV cache. (use only for debugging purposes)</p> <pre><code>public static void llama_kv_cache_view_update(SafeLLamaContextHandle ctx, LLamaKvCacheView&amp; view)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_63","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>view</code> LLamaKvCacheView&amp;</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_get_kv_cache_token_countsafellamacontexthandle","title":"llama_get_kv_cache_token_count(SafeLLamaContextHandle)","text":"<p>Returns the number of tokens in the KV cache (slow, use only for debug)  If a KV cell has multiple sequences assigned to it, it will be counted multiple times</p> <pre><code>public static int llama_get_kv_cache_token_count(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_64","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_36","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_get_kv_cache_used_cellssafellamacontexthandle","title":"llama_get_kv_cache_used_cells(SafeLLamaContextHandle)","text":"<p>Returns the number of used KV cells (i.e. have at least one sequence assigned to them)</p> <pre><code>public static int llama_get_kv_cache_used_cells(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_65","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_37","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_beam_searchsafellamacontexthandle-llamabeamsearchcallback-intptr-uint64-int32-int32-int32","title":"llama_beam_search(SafeLLamaContextHandle, LLamaBeamSearchCallback, IntPtr, UInt64, Int32, Int32, Int32)","text":"<p>Deterministically returns entire sentence constructed by a beam search.</p> <pre><code>public static void llama_beam_search(SafeLLamaContextHandle ctx, LLamaBeamSearchCallback callback, IntPtr callback_data, ulong n_beams, int n_past, int n_predict, int n_threads)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_66","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle Pointer to the llama_context.</p> <p><code>callback</code> LLamaBeamSearchCallback Invoked for each iteration of the beam_search loop, passing in beams_state.</p> <p><code>callback_data</code> IntPtr A pointer that is simply passed back to callback.</p> <p><code>n_beams</code> UInt64 Number of beams to use.</p> <p><code>n_past</code> Int32 Number of tokens already evaluated.</p> <p><code>n_predict</code> Int32 Maximum number of tokens to predict. EOS may occur earlier.</p> <p><code>n_threads</code> Int32 Number of threads.</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_empty_call","title":"llama_empty_call()","text":"<p>A method that does nothing. This is a native method, calling it will force the llama native dependencies to be loaded.</p> <pre><code>public static void llama_empty_call()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#llama_max_devices","title":"llama_max_devices()","text":"<p>Get the maximum number of devices supported by llama.cpp</p> <pre><code>public static long llama_max_devices()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_38","title":"Returns","text":"<p>Int64</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_model_default_params","title":"llama_model_default_params()","text":"<p>Create a LLamaModelParams with default values</p> <pre><code>public static LLamaModelParams llama_model_default_params()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_39","title":"Returns","text":"<p>LLamaModelParams</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_context_default_params","title":"llama_context_default_params()","text":"<p>Create a LLamaContextParams with default values</p> <pre><code>public static LLamaContextParams llama_context_default_params()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_40","title":"Returns","text":"<p>LLamaContextParams</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_model_quantize_default_params","title":"llama_model_quantize_default_params()","text":"<p>Create a LLamaModelQuantizeParams with default values</p> <pre><code>public static LLamaModelQuantizeParams llama_model_quantize_default_params()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_41","title":"Returns","text":"<p>LLamaModelQuantizeParams</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_supports_mmap","title":"llama_supports_mmap()","text":"<p>Check if memory mapping is supported</p> <pre><code>public static bool llama_supports_mmap()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_42","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_supports_mlock","title":"llama_supports_mlock()","text":"<p>Check if memory locking is supported</p> <pre><code>public static bool llama_supports_mlock()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_43","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_supports_gpu_offload","title":"llama_supports_gpu_offload()","text":"<p>Check if GPU offload is supported</p> <pre><code>public static bool llama_supports_gpu_offload()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_44","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_set_rng_seedsafellamacontexthandle-uint32","title":"llama_set_rng_seed(SafeLLamaContextHandle, UInt32)","text":"<p>Sets the current rng seed.</p> <pre><code>public static void llama_set_rng_seed(SafeLLamaContextHandle ctx, uint seed)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_67","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>seed</code> UInt32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_get_state_sizesafellamacontexthandle","title":"llama_get_state_size(SafeLLamaContextHandle)","text":"<p>Returns the maximum size in bytes of the state (rng, logits, embedding  and kv_cache) - will often be smaller after compacting tokens</p> <pre><code>public static ulong llama_get_state_size(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_68","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_45","title":"Returns","text":"<p>UInt64</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_copy_state_datasafellamacontexthandle-byte","title":"llama_copy_state_data(SafeLLamaContextHandle, Byte*)","text":"<p>Copies the state to the specified destination address.  Destination needs to have allocated enough memory.</p> <pre><code>public static ulong llama_copy_state_data(SafeLLamaContextHandle ctx, Byte* dest)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_69","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>dest</code> Byte*</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_46","title":"Returns","text":"<p>UInt64 the number of bytes copied</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_set_state_datasafellamacontexthandle-byte","title":"llama_set_state_data(SafeLLamaContextHandle, Byte*)","text":"<p>Set the state reading from the specified address</p> <pre><code>public static ulong llama_set_state_data(SafeLLamaContextHandle ctx, Byte* src)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_70","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>src</code> Byte*</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_47","title":"Returns","text":"<p>UInt64 the number of bytes read</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_load_session_filesafellamacontexthandle-string-llamatoken-uint64-uint64","title":"llama_load_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64, UInt64&amp;)","text":"<p>Load session file</p> <pre><code>public static bool llama_load_session_file(SafeLLamaContextHandle ctx, string path_session, LLamaToken[] tokens_out, ulong n_token_capacity, UInt64&amp; n_token_count_out)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_71","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>path_session</code> String</p> <p><code>tokens_out</code> LLamaToken[]</p> <p><code>n_token_capacity</code> UInt64</p> <p><code>n_token_count_out</code> UInt64&amp;</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_48","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_save_session_filesafellamacontexthandle-string-llamatoken-uint64","title":"llama_save_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64)","text":"<p>Save session file</p> <pre><code>public static bool llama_save_session_file(SafeLLamaContextHandle ctx, string path_session, LLamaToken[] tokens, ulong n_token_count)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_72","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>path_session</code> String</p> <p><code>tokens</code> LLamaToken[]</p> <p><code>n_token_count</code> UInt64</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_49","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_get_textsafellamamodelhandle-llamatoken","title":"llama_token_get_text(SafeLlamaModelHandle, LLamaToken)","text":"<pre><code>public static Byte* llama_token_get_text(SafeLlamaModelHandle model, LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_73","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_50","title":"Returns","text":"<p>Byte*</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_get_scoresafellamamodelhandle-llamatoken","title":"llama_token_get_score(SafeLlamaModelHandle, LLamaToken)","text":"<pre><code>public static float llama_token_get_score(SafeLlamaModelHandle model, LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_74","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_51","title":"Returns","text":"<p>Single</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_get_typesafellamamodelhandle-llamatoken","title":"llama_token_get_type(SafeLlamaModelHandle, LLamaToken)","text":"<pre><code>public static LLamaTokenType llama_token_get_type(SafeLlamaModelHandle model, LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_75","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_52","title":"Returns","text":"<p>LLamaTokenType</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_n_ctxsafellamacontexthandle","title":"llama_n_ctx(SafeLLamaContextHandle)","text":"<p>Get the size of the context window for the model for this context</p> <pre><code>public static uint llama_n_ctx(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_76","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_53","title":"Returns","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_n_batchsafellamacontexthandle","title":"llama_n_batch(SafeLLamaContextHandle)","text":"<p>Get the batch size for this context</p> <pre><code>public static uint llama_n_batch(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_77","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_54","title":"Returns","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_get_logitssafellamacontexthandle","title":"llama_get_logits(SafeLLamaContextHandle)","text":"<p>Token logits obtained from the last call to llama_decode  The logits for the last token are stored in the last row  Can be mutated in order to change the probabilities of the next token.  Rows: n_tokens  Cols: n_vocab</p> <pre><code>public static Single* llama_get_logits(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_78","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_55","title":"Returns","text":"<p>Single*</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_get_logits_ithsafellamacontexthandle-int32","title":"llama_get_logits_ith(SafeLLamaContextHandle, Int32)","text":"<p>Logits for the ith token. Equivalent to: llama_get_logits(ctx) + i*n_vocab</p> <pre><code>public static Single* llama_get_logits_ith(SafeLLamaContextHandle ctx, int i)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_79","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>i</code> Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_56","title":"Returns","text":"<p>Single*</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_get_embeddings_ithsafellamacontexthandle-int32","title":"llama_get_embeddings_ith(SafeLLamaContextHandle, Int32)","text":"<p>Get the embeddings for the ith sequence. Equivalent to: llama_get_embeddings(ctx) + i*n_embd</p> <pre><code>public static Single* llama_get_embeddings_ith(SafeLLamaContextHandle ctx, int i)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_80","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>i</code> Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_57","title":"Returns","text":"<p>Single*</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/","title":"NativeLibraryConfig","text":"<p>Namespace: LLama.Native</p> <p>Allows configuration of the native llama.cpp libraries to load and use.  All configuration must be done before using any other LLamaSharp methods!</p> <pre><code>public sealed class NativeLibraryConfig\n</code></pre> <p>Inheritance Object \u2192 NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.nativelibraryconfig/#instance","title":"Instance","text":"<p>Get the config instance</p> <pre><code>public static NativeLibraryConfig Instance { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#property-value","title":"Property Value","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#libraryhasloaded","title":"LibraryHasLoaded","text":"<p>Check if the native library has already been loaded. Configuration cannot be modified if this is true.</p> <pre><code>public static bool LibraryHasLoaded { get; internal set; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativelibraryconfig/#withlibrarystring-string","title":"WithLibrary(String, String)","text":"<p>Load a specified native library as backend for LLamaSharp.  When this method is called, all the other configurations will be ignored.</p> <pre><code>public NativeLibraryConfig WithLibrary(string llamaPath, string llavaPath)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters","title":"Parameters","text":"<p><code>llamaPath</code> String The full path to the llama library to load.</p> <p><code>llavaPath</code> String The full path to the llava library to load.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withcudaboolean","title":"WithCuda(Boolean)","text":"<p>Configure whether to use cuda backend if possible.</p> <pre><code>public NativeLibraryConfig WithCuda(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_1","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_1","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_1","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withavxavxlevel","title":"WithAvx(AvxLevel)","text":"<p>Configure the prefferred avx support level of the backend.</p> <pre><code>public NativeLibraryConfig WithAvx(AvxLevel level)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_2","title":"Parameters","text":"<p><code>level</code> AvxLevel</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_2","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_2","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withautofallbackboolean","title":"WithAutoFallback(Boolean)","text":"<p>Configure whether to allow fallback when there's no match for preferred settings.</p> <pre><code>public NativeLibraryConfig WithAutoFallback(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_3","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_3","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_3","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#skipcheckboolean","title":"SkipCheck(Boolean)","text":"<p>Whether to skip the check when you don't allow fallback. This option   may be useful under some complex conditions. For example, you're sure   you have your cublas configured but LLamaSharp take it as invalid by mistake.</p> <pre><code>public NativeLibraryConfig SkipCheck(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_4","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_4","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_4","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withlogsboolean","title":"WithLogs(Boolean)","text":"<p>Whether to output the logs to console when loading the native library with your configuration.</p> <pre><code>public NativeLibraryConfig WithLogs(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_5","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_5","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_5","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withlogsllamaloglevel","title":"WithLogs(LLamaLogLevel)","text":"<p>Enable console logging with the specified log logLevel.</p> <pre><code>public NativeLibraryConfig WithLogs(LLamaLogLevel logLevel)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_6","title":"Parameters","text":"<p><code>logLevel</code> LLamaLogLevel</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_6","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_6","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withsearchdirectoriesienumerablestring","title":"WithSearchDirectories(IEnumerable&lt;String&gt;)","text":"<p>Add self-defined search directories. Note that the file stucture of the added   directories must be the same as the default directory. Besides, the directory   won't be used recursively.</p> <pre><code>public NativeLibraryConfig WithSearchDirectories(IEnumerable&lt;string&gt; directories)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_7","title":"Parameters","text":"<p><code>directories</code> IEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_7","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withsearchdirectorystring","title":"WithSearchDirectory(String)","text":"<p>Add self-defined search directories. Note that the file stucture of the added   directories must be the same as the default directory. Besides, the directory   won't be used recursively.</p> <pre><code>public NativeLibraryConfig WithSearchDirectory(string directory)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_8","title":"Parameters","text":"<p><code>directory</code> String</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_8","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#checkandgatherdescriptionlibraryname","title":"CheckAndGatherDescription(LibraryName)","text":"<pre><code>internal static Description CheckAndGatherDescription(LibraryName library)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_9","title":"Parameters","text":"<p><code>library</code> LibraryName</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_9","title":"Returns","text":"<p>Description</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#avxleveltostringavxlevel","title":"AvxLevelToString(AvxLevel)","text":"<pre><code>internal static string AvxLevelToString(AvxLevel level)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_10","title":"Parameters","text":"<p><code>level</code> AvxLevel</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_10","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.ropescalingtype/","title":"RopeScalingType","text":"<p>Namespace: LLama.Native</p> <p>RoPE scaling type.</p> <pre><code>public enum RopeScalingType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 RopeScalingType Implements IComparable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>C# equivalent of llama_rope_scaling_type</p>"},{"location":"xmldocs/llama.native.ropescalingtype/#fields","title":"Fields","text":"Name Value Description Unspecified -1 No particular scaling type has been specified None 0 Do not apply any RoPE scaling Linear 1 Positional linear interpolation, as described by kaikendev: https://kaiokendev.github.io/til#extending-context-to-8k Yarn 2 YaRN scaling: https://arxiv.org/pdf/2309.00071.pdf"},{"location":"xmldocs/llama.native.safellamacontexthandle/","title":"SafeLLamaContextHandle","text":"<p>Namespace: LLama.Native</p> <p>A safe wrapper around a llama_context</p> <pre><code>public sealed class SafeLLamaContextHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 SafeLLamaContextHandle Implements IDisposable</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellamacontexthandle/#vocabcount","title":"VocabCount","text":"<p>Total number of tokens in vocabulary of this model</p> <pre><code>public int VocabCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#contextsize","title":"ContextSize","text":"<p>Total number of tokens in the context</p> <pre><code>public uint ContextSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_1","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#embeddingsize","title":"EmbeddingSize","text":"<p>Dimension of embedding vectors</p> <pre><code>public int EmbeddingSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#batchsize","title":"BatchSize","text":"<p>Get the maximum batch size for this context</p> <pre><code>public uint BatchSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_3","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#modelhandle","title":"ModelHandle","text":"<p>Get the model which this context is using</p> <pre><code>public SafeLlamaModelHandle ModelHandle { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_4","title":"Property Value","text":"<p>SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_5","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_6","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.safellamacontexthandle/#safellamacontexthandle_1","title":"SafeLLamaContextHandle()","text":"<pre><code>public SafeLLamaContextHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellamacontexthandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#createsafellamamodelhandle-llamacontextparams","title":"Create(SafeLlamaModelHandle, LLamaContextParams)","text":"<p>Create a new llama_state for the given model</p> <pre><code>public static SafeLLamaContextHandle Create(SafeLlamaModelHandle model, LLamaContextParams lparams)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>lparams</code> LLamaContextParams</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_1","title":"Returns","text":"<p>SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#exceptions","title":"Exceptions","text":"<p>RuntimeError</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getlogits","title":"GetLogits()","text":"<p>Token logits obtained from the last call to llama_decode  The logits for the last token are stored in the last row  Can be mutated in order to change the probabilities of the next token.  Rows: n_tokens  Cols: n_vocab</p> <pre><code>public Span&lt;float&gt; GetLogits()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_2","title":"Returns","text":"<p>Span&lt;Single&gt;</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getlogitsithint32","title":"GetLogitsIth(Int32)","text":"<p>Logits for the ith token. Equivalent to: llama_get_logits(ctx) + i*n_vocab</p> <pre><code>public Span&lt;float&gt; GetLogitsIth(int i)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_1","title":"Parameters","text":"<p><code>i</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_3","title":"Returns","text":"<p>Span&lt;Single&gt;</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#tokenizestring-boolean-boolean-encoding","title":"Tokenize(String, Boolean, Boolean, Encoding)","text":"<p>Convert the given text into tokens</p> <pre><code>public LLamaToken[] Tokenize(string text, bool add_bos, bool special, Encoding encoding)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_2","title":"Parameters","text":"<p><code>text</code> String The text to tokenize</p> <p><code>add_bos</code> Boolean Whether the \"BOS\" token should be added</p> <p><code>special</code> Boolean Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.</p> <p><code>encoding</code> Encoding Encoding to use for the text</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_4","title":"Returns","text":"<p>LLamaToken[]</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#exceptions_1","title":"Exceptions","text":"<p>RuntimeError</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#tokentospanllamatoken-spanbyte","title":"TokenToSpan(LLamaToken, Span&lt;Byte&gt;)","text":"<p>Convert a single llama token into bytes</p> <pre><code>public uint TokenToSpan(LLamaToken token, Span&lt;byte&gt; dest)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_3","title":"Parameters","text":"<p><code>token</code> LLamaToken Token to decode</p> <p><code>dest</code> Span&lt;Byte&gt; A span to attempt to write into. If this is too small nothing will be written</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_5","title":"Returns","text":"<p>UInt32 The size of this token. nothing will be written if this is larger than <code>dest</code></p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#decodellamabatch","title":"Decode(LLamaBatch)","text":"<pre><code>public DecodeResult Decode(LLamaBatch batch)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_4","title":"Parameters","text":"<p><code>batch</code> LLamaBatch</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_6","title":"Returns","text":"<p>DecodeResult Positive return values does not mean a fatal error, but rather a warning:  - 0: success  - 1: could not find a KV slot for the batch (try reducing the size of the batch or increase the context)  - &lt; 0: error</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#decodelistllamatoken-llamaseqid-llamabatch-int32","title":"Decode(List&lt;LLamaToken&gt;, LLamaSeqId, LLamaBatch, Int32&amp;)","text":"<p>Decode a set of tokens in batch-size chunks.</p> <pre><code>internal ValueTuple&lt;DecodeResult, int&gt; Decode(List&lt;LLamaToken&gt; tokens, LLamaSeqId id, LLamaBatch batch, Int32&amp; n_past)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_5","title":"Parameters","text":"<p><code>tokens</code> List&lt;LLamaToken&gt;</p> <p><code>id</code> LLamaSeqId</p> <p><code>batch</code> LLamaBatch</p> <p><code>n_past</code> Int32&amp;</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_7","title":"Returns","text":"<p>ValueTuple&lt;DecodeResult, Int32&gt; A tuple, containing the decode result and the number of tokens that have not been decoded yet.</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getstatesize","title":"GetStateSize()","text":"<p>Get the size of the state, when saved as bytes</p> <pre><code>public ulong GetStateSize()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_8","title":"Returns","text":"<p>UInt64</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getstatebyte-uint64","title":"GetState(Byte*, UInt64)","text":"<p>Get the raw state of this context, encoded as bytes. Data is written into the <code>dest</code> pointer.</p> <pre><code>public ulong GetState(Byte* dest, ulong size)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_6","title":"Parameters","text":"<p><code>dest</code> Byte* Destination to write to</p> <p><code>size</code> UInt64 Number of bytes available to write to in dest (check required size with <code>GetStateSize()</code>)</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_9","title":"Returns","text":"<p>UInt64 The number of bytes written to dest</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#exceptions_2","title":"Exceptions","text":"<p>ArgumentOutOfRangeException Thrown if dest is too small</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getstateintptr-uint64","title":"GetState(IntPtr, UInt64)","text":"<p>Get the raw state of this context, encoded as bytes. Data is written into the <code>dest</code> pointer.</p> <pre><code>public ulong GetState(IntPtr dest, ulong size)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_7","title":"Parameters","text":"<p><code>dest</code> IntPtr Destination to write to</p> <p><code>size</code> UInt64 Number of bytes available to write to in dest (check required size with <code>GetStateSize()</code>)</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_10","title":"Returns","text":"<p>UInt64 The number of bytes written to dest</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#exceptions_3","title":"Exceptions","text":"<p>ArgumentOutOfRangeException Thrown if dest is too small</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#setstatebyte","title":"SetState(Byte*)","text":"<p>Set the raw state of this context</p> <pre><code>public ulong SetState(Byte* src)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_8","title":"Parameters","text":"<p><code>src</code> Byte* The pointer to read the state from</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_11","title":"Returns","text":"<p>UInt64 Number of bytes read from the src pointer</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#setstateintptr","title":"SetState(IntPtr)","text":"<p>Set the raw state of this context</p> <pre><code>public ulong SetState(IntPtr src)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_9","title":"Parameters","text":"<p><code>src</code> IntPtr The pointer to read the state from</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_12","title":"Returns","text":"<p>UInt64 Number of bytes read from the src pointer</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#setseeduint32","title":"SetSeed(UInt32)","text":"<p>Set the RNG seed</p> <pre><code>public void SetSeed(uint seed)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_10","title":"Parameters","text":"<p><code>seed</code> UInt32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#setthreadsuint32-uint32","title":"SetThreads(UInt32, UInt32)","text":"<p>Set the number of threads used for decoding</p> <pre><code>public void SetThreads(uint threads, uint threadsBatch)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_11","title":"Parameters","text":"<p><code>threads</code> UInt32 n_threads is the number of threads used for generation (single token)</p> <p><code>threadsBatch</code> UInt32 n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachegetdebugviewint32","title":"KvCacheGetDebugView(Int32)","text":"<p>Get a new KV cache view that can be used to debug the KV cache</p> <pre><code>public LLamaKvCacheViewSafeHandle KvCacheGetDebugView(int maxSequences)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_12","title":"Parameters","text":"<p><code>maxSequences</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_13","title":"Returns","text":"<p>LLamaKvCacheViewSafeHandle</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachecountcells","title":"KvCacheCountCells()","text":"<p>Count the number of used cells in the KV cache (i.e. have at least one sequence assigned to them)</p> <pre><code>public int KvCacheCountCells()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_14","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachecounttokens","title":"KvCacheCountTokens()","text":"<p>Returns the number of tokens in the KV cache (slow, use only for debug)  If a KV cell has multiple sequences assigned to it, it will be counted multiple times</p> <pre><code>public int KvCacheCountTokens()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_15","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcacheclear","title":"KvCacheClear()","text":"<p>Clear the KV cache</p> <pre><code>public void KvCacheClear()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcacheremovellamaseqid-llamapos-llamapos","title":"KvCacheRemove(LLamaSeqId, LLamaPos, LLamaPos)","text":"<p>Removes all tokens that belong to the specified sequence and have positions in [p0, p1)</p> <pre><code>public void KvCacheRemove(LLamaSeqId seq, LLamaPos p0, LLamaPos p1)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_13","title":"Parameters","text":"<p><code>seq</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachesequencecopyllamaseqid-llamaseqid-llamapos-llamapos","title":"KvCacheSequenceCopy(LLamaSeqId, LLamaSeqId, LLamaPos, LLamaPos)","text":"<p>Copy all tokens that belong to the specified sequence to another sequence. Note that  this does not allocate extra KV cache memory - it simply assigns the tokens to the  new sequence</p> <pre><code>public void KvCacheSequenceCopy(LLamaSeqId src, LLamaSeqId dest, LLamaPos p0, LLamaPos p1)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_14","title":"Parameters","text":"<p><code>src</code> LLamaSeqId</p> <p><code>dest</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachesequencekeepllamaseqid","title":"KvCacheSequenceKeep(LLamaSeqId)","text":"<p>Removes all tokens that do not belong to the specified sequence</p> <pre><code>public void KvCacheSequenceKeep(LLamaSeqId seq)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_15","title":"Parameters","text":"<p><code>seq</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachesequenceaddllamaseqid-llamapos-llamapos-int32","title":"KvCacheSequenceAdd(LLamaSeqId, LLamaPos, LLamaPos, Int32)","text":"<p>Adds relative position \"delta\" to all tokens that belong to the specified sequence  and have positions in [p0, p1. If the KV cache is RoPEd, the KV data is updated  accordingly</p> <pre><code>public void KvCacheSequenceAdd(LLamaSeqId seq, LLamaPos p0, LLamaPos p1, int delta)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_16","title":"Parameters","text":"<p><code>seq</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p> <p><code>delta</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachesequencedividellamaseqid-llamapos-llamapos-int32","title":"KvCacheSequenceDivide(LLamaSeqId, LLamaPos, LLamaPos, Int32)","text":"<p>Integer division of the positions by factor of <code>d &amp;gt; 1</code>.  If the KV cache is RoPEd, the KV data is updated accordingly.  p0 &lt; 0 : [0, p1]  p1 &lt; 0 : [p0, inf)</p> <pre><code>public void KvCacheSequenceDivide(LLamaSeqId seq, LLamaPos p0, LLamaPos p1, int divisor)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_17","title":"Parameters","text":"<p><code>seq</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p> <p><code>divisor</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/","title":"SafeLLamaGrammarHandle","text":"<p>Namespace: LLama.Native</p> <p>A safe reference to a <code>llama_grammar</code></p> <pre><code>public class SafeLLamaGrammarHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 SafeLLamaGrammarHandle Implements IDisposable</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellamagrammarhandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellamagrammarhandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#createireadonlylistgrammarrule-uint64","title":"Create(IReadOnlyList&lt;GrammarRule&gt;, UInt64)","text":"<p>Create a new llama_grammar</p> <pre><code>public static SafeLLamaGrammarHandle Create(IReadOnlyList&lt;GrammarRule&gt; rules, ulong start_rule_index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#parameters","title":"Parameters","text":"<p><code>rules</code> IReadOnlyList&lt;GrammarRule&gt; A list of list of elements, each inner list makes up one grammar rule</p> <p><code>start_rule_index</code> UInt64 The index (in the outer list) of the start rule</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#returns_1","title":"Returns","text":"<p>SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#exceptions","title":"Exceptions","text":"<p>RuntimeError</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#createllamagrammarelement-uint64-uint64","title":"Create(LLamaGrammarElement, UInt64, UInt64)**","text":"<p>Create a new llama_grammar</p> <pre><code>public static SafeLLamaGrammarHandle Create(LLamaGrammarElement** rules, ulong nrules, ulong start_rule_index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#parameters_1","title":"Parameters","text":"<p><code>rules</code> LLamaGrammarElement** rules list, each rule is a list of rule elements (terminated by a LLamaGrammarElementType.END element)</p> <p><code>nrules</code> UInt64 total number of rules</p> <p><code>start_rule_index</code> UInt64 index of the start rule of the grammar</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#returns_2","title":"Returns","text":"<p>SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#exceptions_1","title":"Exceptions","text":"<p>RuntimeError</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#clone","title":"Clone()","text":"<p>Create a copy of this grammar instance</p> <pre><code>public SafeLLamaGrammarHandle Clone()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#returns_3","title":"Returns","text":"<p>SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#accepttokensafellamacontexthandle-llamatoken","title":"AcceptToken(SafeLLamaContextHandle, LLamaToken)","text":"<p>Accepts the sampled token into the grammar</p> <pre><code>public void AcceptToken(SafeLLamaContextHandle ctx, LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamagrammarhandle/#parameters_2","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.native.safellamahandlebase/","title":"SafeLLamaHandleBase","text":"<p>Namespace: LLama.Native</p> <p>Base class for all llama handles to native resources</p> <pre><code>public abstract class SafeLLamaHandleBase : System.Runtime.InteropServices.SafeHandle, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase Implements IDisposable</p>"},{"location":"xmldocs/llama.native.safellamahandlebase/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellamahandlebase/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamahandlebase/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamahandlebase/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamahandlebase/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamahandlebase/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellamahandlebase/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamahandlebase/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/","title":"SafeLlamaModelHandle","text":"<p>Namespace: LLama.Native</p> <p>A reference to a set of llama model weights</p> <pre><code>public sealed class SafeLlamaModelHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 SafeLlamaModelHandle Implements IDisposable</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellamamodelhandle/#vocabcount","title":"VocabCount","text":"<p>Total number of tokens in vocabulary of this model</p> <pre><code>public int VocabCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#contextsize","title":"ContextSize","text":"<p>Total number of tokens in the context</p> <pre><code>public int ContextSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#ropefrequency","title":"RopeFrequency","text":"<p>Get the rope frequency this model was trained with</p> <pre><code>public float RopeFrequency { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_2","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#embeddingsize","title":"EmbeddingSize","text":"<p>Dimension of embedding vectors</p> <pre><code>public int EmbeddingSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_3","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#sizeinbytes","title":"SizeInBytes","text":"<p>Get the size of this model in bytes</p> <pre><code>public ulong SizeInBytes { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_4","title":"Property Value","text":"<p>UInt64</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parametercount","title":"ParameterCount","text":"<p>Get the number of parameters in this model</p> <pre><code>public ulong ParameterCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_5","title":"Property Value","text":"<p>UInt64</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#description","title":"Description","text":"<p>Get a description of this model</p> <pre><code>public string Description { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_6","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#metadatacount","title":"MetadataCount","text":"<p>Get the number of metadata key/value pairs</p> <pre><code>public int MetadataCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_7","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_8","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_9","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.safellamamodelhandle/#safellamamodelhandle_1","title":"SafeLlamaModelHandle()","text":"<pre><code>public SafeLlamaModelHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellamamodelhandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#loadfromfilestring-llamamodelparams","title":"LoadFromFile(String, LLamaModelParams)","text":"<p>Load a model from the given file path into memory</p> <pre><code>public static SafeLlamaModelHandle LoadFromFile(string modelPath, LLamaModelParams lparams)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters","title":"Parameters","text":"<p><code>modelPath</code> String</p> <p><code>lparams</code> LLamaModelParams</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_1","title":"Returns","text":"<p>SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#exceptions","title":"Exceptions","text":"<p>RuntimeError</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#llama_model_apply_lora_from_filesafellamamodelhandle-string-single-string-int32","title":"llama_model_apply_lora_from_file(SafeLlamaModelHandle, String, Single, String, Int32)","text":"<p>Apply a LoRA adapter to a loaded model  path_base_model is the path to a higher quality model to use as a base for  the layers modified by the adapter. Can be NULL to use the current loaded model.  The model needs to be reloaded before applying a new adapter, otherwise the adapter  will be applied on top of the previous one</p> <pre><code>public static int llama_model_apply_lora_from_file(SafeLlamaModelHandle model_ptr, string path_lora, float scale, string path_base_model, int n_threads)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_1","title":"Parameters","text":"<p><code>model_ptr</code> SafeLlamaModelHandle</p> <p><code>path_lora</code> String</p> <p><code>scale</code> Single</p> <p><code>path_base_model</code> String</p> <p><code>n_threads</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_2","title":"Returns","text":"<p>Int32 Returns 0 on success</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#llama_model_meta_val_strsafellamamodelhandle-byte-byte-int64","title":"llama_model_meta_val_str(SafeLlamaModelHandle, Byte, Byte, Int64)","text":"<p>Get metadata value as a string by key name</p> <pre><code>public static int llama_model_meta_val_str(SafeLlamaModelHandle model, Byte* key, Byte* buf, long buf_size)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_2","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>key</code> Byte*</p> <p><code>buf</code> Byte*</p> <p><code>buf_size</code> Int64</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_3","title":"Returns","text":"<p>Int32 The length of the string on success, or -1 on failure</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#applylorafromfilestring-single-string-nullableint32","title":"ApplyLoraFromFile(String, Single, String, Nullable&lt;Int32&gt;)","text":"<p>Apply a LoRA adapter to a loaded model</p> <pre><code>public void ApplyLoraFromFile(string lora, float scale, string modelBase, Nullable&lt;int&gt; threads)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_3","title":"Parameters","text":"<p><code>lora</code> String</p> <p><code>scale</code> Single</p> <p><code>modelBase</code> String A path to a higher quality model to use as a base for the layers modified by the  adapter. Can be NULL to use the current loaded model.</p> <p><code>threads</code> Nullable&lt;Int32&gt;</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#exceptions_1","title":"Exceptions","text":"<p>RuntimeError</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#tokentospanllamatoken-spanbyte","title":"TokenToSpan(LLamaToken, Span&lt;Byte&gt;)","text":"<p>Convert a single llama token into bytes</p> <pre><code>public uint TokenToSpan(LLamaToken token, Span&lt;byte&gt; dest)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_4","title":"Parameters","text":"<p><code>token</code> LLamaToken Token to decode</p> <p><code>dest</code> Span&lt;Byte&gt; A span to attempt to write into. If this is too small nothing will be written</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_4","title":"Returns","text":"<p>UInt32 The size of this token. nothing will be written if this is larger than <code>dest</code></p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#tokenstospanireadonlylistllamatoken-spanchar-encoding","title":"TokensToSpan(IReadOnlyList&lt;LLamaToken&gt;, Span&lt;Char&gt;, Encoding)","text":""},{"location":"xmldocs/llama.native.safellamamodelhandle/#caution","title":"Caution","text":"<p>Use a StreamingTokenDecoder instead</p> <p>Convert a sequence of tokens into characters.</p> <pre><code>internal Span&lt;char&gt; TokensToSpan(IReadOnlyList&lt;LLamaToken&gt; tokens, Span&lt;char&gt; dest, Encoding encoding)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_5","title":"Parameters","text":"<p><code>tokens</code> IReadOnlyList&lt;LLamaToken&gt;</p> <p><code>dest</code> Span&lt;Char&gt;</p> <p><code>encoding</code> Encoding</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_5","title":"Returns","text":"<p>Span&lt;Char&gt; The section of the span which has valid data in it.  If there was insufficient space in the output span this will be  filled with as many characters as possible, starting from the last token.</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#tokenizestring-boolean-boolean-encoding","title":"Tokenize(String, Boolean, Boolean, Encoding)","text":"<p>Convert a string of text into tokens</p> <pre><code>public LLamaToken[] Tokenize(string text, bool add_bos, bool special, Encoding encoding)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_6","title":"Parameters","text":"<p><code>text</code> String</p> <p><code>add_bos</code> Boolean</p> <p><code>special</code> Boolean Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.</p> <p><code>encoding</code> Encoding</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_6","title":"Returns","text":"<p>LLamaToken[]</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#createcontextllamacontextparams","title":"CreateContext(LLamaContextParams)","text":"<p>Create a new context for this model</p> <pre><code>public SafeLLamaContextHandle CreateContext(LLamaContextParams params)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_7","title":"Parameters","text":"<p><code>params</code> LLamaContextParams</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_7","title":"Returns","text":"<p>SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#metadatakeybyindexint32","title":"MetadataKeyByIndex(Int32)","text":"<p>Get the metadata key for the given index</p> <pre><code>public Nullable&lt;Memory&lt;byte&gt;&gt; MetadataKeyByIndex(int index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_8","title":"Parameters","text":"<p><code>index</code> Int32 The index to get</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_8","title":"Returns","text":"<p>Nullable&lt;Memory&lt;Byte&gt;&gt; The key, null if there is no such key or if the buffer was too small</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#metadatavaluebyindexint32","title":"MetadataValueByIndex(Int32)","text":"<p>Get the metadata value for the given index</p> <pre><code>public Nullable&lt;Memory&lt;byte&gt;&gt; MetadataValueByIndex(int index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_9","title":"Parameters","text":"<p><code>index</code> Int32 The index to get</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_9","title":"Returns","text":"<p>Nullable&lt;Memory&lt;Byte&gt;&gt; The value, null if there is no such value or if the buffer was too small</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#readmetadata","title":"ReadMetadata()","text":"<pre><code>internal IReadOnlyDictionary&lt;string, string&gt; ReadMetadata()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_10","title":"Returns","text":"<p>IReadOnlyDictionary&lt;String, String&gt;</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#llama_model_meta_key_by_indexg__llama_model_meta_key_by_index_native23_0safellamamodelhandle-int32-byte-int64","title":"&lt;llama_model_meta_key_by_index&gt;g__llama_model_meta_key_by_index_native|23_0(SafeLlamaModelHandle, Int32, Byte*, Int64)","text":"<pre><code>internal static int &lt;llama_model_meta_key_by_index&gt;g__llama_model_meta_key_by_index_native|23_0(SafeLlamaModelHandle model, int index, Byte* buf, long buf_size)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_10","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>index</code> Int32</p> <p><code>buf</code> Byte*</p> <p><code>buf_size</code> Int64</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_11","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#llama_model_meta_val_str_by_indexg__llama_model_meta_val_str_by_index_native24_0safellamamodelhandle-int32-byte-int64","title":"&lt;llama_model_meta_val_str_by_index&gt;g__llama_model_meta_val_str_by_index_native|24_0(SafeLlamaModelHandle, Int32, Byte*, Int64)","text":"<pre><code>internal static int &lt;llama_model_meta_val_str_by_index&gt;g__llama_model_meta_val_str_by_index_native|24_0(SafeLlamaModelHandle model, int index, Byte* buf, long buf_size)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_11","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>index</code> Int32</p> <p><code>buf</code> Byte*</p> <p><code>buf_size</code> Int64</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_12","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/","title":"SafeLlavaImageEmbedHandle","text":"<p>Namespace: LLama.Native</p> <p>A Reference to a llava Image Embed handle</p> <pre><code>public sealed class SafeLlavaImageEmbedHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 SafeLlavaImageEmbedHandle Implements IDisposable</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#createfromfilenamesafellavamodelhandle-llamacontext-string","title":"CreateFromFileName(SafeLlavaModelHandle, LLamaContext, String)","text":"<p>Create an image embed from an image file</p> <pre><code>public static SafeLlavaImageEmbedHandle CreateFromFileName(SafeLlavaModelHandle ctxLlava, LLamaContext ctxLlama, string image)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#parameters","title":"Parameters","text":"<p><code>ctxLlava</code> SafeLlavaModelHandle</p> <p><code>ctxLlama</code> LLamaContext</p> <p><code>image</code> String Path to the image file. Supported formats:  JPGPNGBMPTGA</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#returns","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#exceptions","title":"Exceptions","text":"<p>InvalidOperationException</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#createfrommemorysafellavamodelhandle-llamacontext-byte","title":"CreateFromMemory(SafeLlavaModelHandle, LLamaContext, Byte[])","text":"<p>Create an image embed from the bytes of an image.</p> <pre><code>public static SafeLlavaImageEmbedHandle CreateFromMemory(SafeLlavaModelHandle ctxLlava, LLamaContext ctxLlama, Byte[] image)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#parameters_1","title":"Parameters","text":"<p><code>ctxLlava</code> SafeLlavaModelHandle</p> <p><code>ctxLlama</code> LLamaContext</p> <p><code>image</code> Byte[] Image bytes. Supported formats:  JPGPNGBMPTGA</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#returns_1","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/","title":"SafeLlavaModelHandle","text":"<p>Namespace: LLama.Native</p> <p>A reference to a set of llava model weights.</p> <pre><code>public sealed class SafeLlavaModelHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 SafeLlavaModelHandle Implements IDisposable</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellavamodelhandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellavamodelhandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#loadfromfilestring-int32","title":"LoadFromFile(String, Int32)","text":"<p>Load a model from the given file path into memory</p> <pre><code>public static SafeLlavaModelHandle LoadFromFile(string modelPath, int verbosity)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#parameters","title":"Parameters","text":"<p><code>modelPath</code> String MMP File (Multi-Modal Projections)</p> <p><code>verbosity</code> Int32 Verbosity level</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns_1","title":"Returns","text":"<p>SafeLlavaModelHandle SafeHandle of the Clip Model</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#exceptions","title":"Exceptions","text":"<p>InvalidOperationException</p> <p>RuntimeError</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#createimageembeddingsllamacontext-string","title":"CreateImageEmbeddings(LLamaContext, String)","text":"<p>Create the Image Embeddings.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(LLamaContext ctxLlama, string image)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#parameters_1","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext LLama Context</p> <p><code>image</code> String Image filename (it supports jpeg format only)</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns_2","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle return the SafeHandle of these embeddings</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#createimageembeddingsllamacontext-byte","title":"CreateImageEmbeddings(LLamaContext, Byte[])","text":"<p>Create the Image Embeddings.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(LLamaContext ctxLlama, Byte[] image)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#parameters_2","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext LLama Context</p> <p><code>image</code> Byte[] Image in binary format (it supports jpeg format only)</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns_3","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle return the SafeHandle of these embeddings</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#evalimageembedllamacontext-safellavaimageembedhandle-int32","title":"EvalImageEmbed(LLamaContext, SafeLlavaImageEmbedHandle, Int32&amp;)","text":"<p>Evaluates the image embeddings.</p> <pre><code>public bool EvalImageEmbed(LLamaContext ctxLlama, SafeLlavaImageEmbedHandle imageEmbed, Int32&amp; n_past)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#parameters_3","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext Llama Context</p> <p><code>imageEmbed</code> SafeLlavaImageEmbedHandle The current embeddings to evaluate</p> <p><code>n_past</code> Int32&amp;</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns_4","title":"Returns","text":"<p>Boolean True on success</p>"},{"location":"xmldocs/llama.quantizer/","title":"Quantizer","text":"<p>Namespace: LLama</p> <pre><code>public class Quantizer\n</code></pre> <p>Inheritance Object \u2192 Quantizer</p>"},{"location":"xmldocs/llama.quantizer/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.quantizer/#quantizer_1","title":"Quantizer()","text":"<pre><code>public Quantizer()\n</code></pre>"},{"location":"xmldocs/llama.quantizer/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.quantizer/#quantizestring-string-llamaftype-int32-boolean","title":"Quantize(String, String, LLamaFtype, Int32, Boolean)","text":"<pre><code>public static bool Quantize(string srcFileName, string dstFilename, LLamaFtype ftype, int nthread, bool printInfo)\n</code></pre>"},{"location":"xmldocs/llama.quantizer/#parameters","title":"Parameters","text":"<p><code>srcFileName</code> String</p> <p><code>dstFilename</code> String</p> <p><code>ftype</code> LLamaFtype</p> <p><code>nthread</code> Int32</p> <p><code>printInfo</code> Boolean</p>"},{"location":"xmldocs/llama.quantizer/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.quantizer/#quantizestring-string-string-int32-boolean","title":"Quantize(String, String, String, Int32, Boolean)","text":"<pre><code>public static bool Quantize(string srcFileName, string dstFilename, string ftype, int nthread, bool printInfo)\n</code></pre>"},{"location":"xmldocs/llama.quantizer/#parameters_1","title":"Parameters","text":"<p><code>srcFileName</code> String</p> <p><code>dstFilename</code> String</p> <p><code>ftype</code> String</p> <p><code>nthread</code> Int32</p> <p><code>printInfo</code> Boolean</p>"},{"location":"xmldocs/llama.quantizer/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/","title":"BaseSamplingPipeline","text":"<p>Namespace: LLama.Sampling</p> <p>Base class for implementing custom sampling pipelines. This provides a helpful framework for implementing <code>ISamplingPipeline</code>.</p> <pre><code>public abstract class BaseSamplingPipeline : ISamplingPipeline, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 BaseSamplingPipeline Implements ISamplingPipeline, IDisposable</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#grammar","title":"Grammar","text":"<p>Grammar to constrain valid tokens</p> <pre><code>public SafeLLamaGrammarHandle Grammar { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#property-value","title":"Property Value","text":"<p>SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#samplesafellamacontexthandle-spansingle-readonlyspanllamatoken","title":"Sample(SafeLLamaContextHandle, Span&lt;Single&gt;, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<pre><code>public LLamaToken Sample(SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#parameters","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>logits</code> Span&lt;Single&gt;</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#returns","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#acceptsafellamacontexthandle-llamatoken","title":"Accept(SafeLLamaContextHandle, LLamaToken)","text":"<pre><code>public void Accept(SafeLLamaContextHandle ctx, LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#parameters_1","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#processlogitssafellamacontexthandle-spansingle-readonlyspanllamatoken","title":"ProcessLogits(SafeLLamaContextHandle, Span&lt;Single&gt;, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<p>Process the raw logit values</p> <pre><code>protected abstract void ProcessLogits(SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#parameters_2","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle The context being sampled from</p> <p><code>logits</code> Span&lt;Single&gt; The logits produced by the model</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt; A list of tokens recently returned by the model</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#processtokendataarraysafellamacontexthandle-llamatokendataarray-readonlyspanllamatoken","title":"ProcessTokenDataArray(SafeLLamaContextHandle, LLamaTokenDataArray, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<p>Process the LLamaTokenDataArray and select a single token</p> <pre><code>protected abstract LLamaToken ProcessTokenDataArray(SafeLLamaContextHandle ctx, LLamaTokenDataArray candidates, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#parameters_3","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle The context being sampled from</p> <p><code>candidates</code> LLamaTokenDataArray The LLamaTokenDataArray data produced by the model</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt; A list of tokens recently returned by the model</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#returns_1","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#reset","title":"Reset()","text":"<pre><code>public void Reset()\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#clone","title":"Clone()","text":"<pre><code>public abstract ISamplingPipeline Clone()\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#returns_2","title":"Returns","text":"<p>ISamplingPipeline</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#dispose","title":"Dispose()","text":"<pre><code>public void Dispose()\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/","title":"DefaultSamplingPipeline","text":"<p>Namespace: LLama.Sampling</p> <p>An implementation of ISamplePipeline which mimics the default llama.cpp sampling</p> <pre><code>public sealed class DefaultSamplingPipeline : BaseSamplingPipeline, ISamplingPipeline, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 BaseSamplingPipeline \u2192 DefaultSamplingPipeline Implements ISamplingPipeline, IDisposable</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#logitbias","title":"LogitBias","text":"<p>Bias values to add to certain logits</p> <pre><code>public Dictionary&lt;int, float&gt; LogitBias { get; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value","title":"Property Value","text":"<p>Dictionary&lt;Int32, Single&gt;</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#repeatpenalty","title":"RepeatPenalty","text":"<p>Repetition penalty, as described in https://arxiv.org/abs/1909.05858</p> <pre><code>public float RepeatPenalty { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_1","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#alphafrequency","title":"AlphaFrequency","text":"<p>Frequency penalty as described by OpenAI: https://platform.openai.com/docs/api-reference/chat/create  Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text  so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <pre><code>public float AlphaFrequency { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_2","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#alphapresence","title":"AlphaPresence","text":"<p>Presence penalty as described by OpenAI: https://platform.openai.com/docs/api-reference/chat/create  Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the  text so far, increasing the model's likelihood to talk about new topics.</p> <pre><code>public float AlphaPresence { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_3","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#temperature","title":"Temperature","text":"<p>Temperature to apply (higher temperature is more \"creative\")</p> <pre><code>public float Temperature { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_4","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#topk","title":"TopK","text":"<p>Number of tokens to keep in TopK sampling</p> <pre><code>public int TopK { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_5","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#tailfreez","title":"TailFreeZ","text":"<p>Z value for tail free sampling</p> <pre><code>public float TailFreeZ { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_6","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#typicalp","title":"TypicalP","text":"<p>P value for locally typical sampling</p> <pre><code>public float TypicalP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_7","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#topp","title":"TopP","text":"<p>P value for TopP sampling</p> <pre><code>public float TopP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_8","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#minp","title":"MinP","text":"<p>P value for MinP sampling</p> <pre><code>public float MinP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_9","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#penalizenewline","title":"PenalizeNewline","text":"<p>Whether the newline value should be protected from being modified by logit bias and repeat penalty</p> <pre><code>public bool PenalizeNewline { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_10","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#grammar","title":"Grammar","text":"<p>Grammar to constrain valid tokens</p> <pre><code>public SafeLLamaGrammarHandle Grammar { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_11","title":"Property Value","text":"<p>SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#defaultsamplingpipeline_1","title":"DefaultSamplingPipeline()","text":"<pre><code>public DefaultSamplingPipeline()\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#processlogitssafellamacontexthandle-spansingle-readonlyspanllamatoken","title":"ProcessLogits(SafeLLamaContextHandle, Span&lt;Single&gt;, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<pre><code>protected void ProcessLogits(SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#parameters","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>logits</code> Span&lt;Single&gt;</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#processtokendataarraysafellamacontexthandle-llamatokendataarray-readonlyspanllamatoken","title":"ProcessTokenDataArray(SafeLLamaContextHandle, LLamaTokenDataArray, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<pre><code>protected LLamaToken ProcessTokenDataArray(SafeLLamaContextHandle ctx, LLamaTokenDataArray candidates, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#parameters_1","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArray</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#returns","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#acceptsafellamacontexthandle-llamatoken","title":"Accept(SafeLLamaContextHandle, LLamaToken)","text":"<pre><code>public void Accept(SafeLLamaContextHandle ctx, LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#parameters_2","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#clone","title":"Clone()","text":"<pre><code>public ISamplingPipeline Clone()\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#returns_1","title":"Returns","text":"<p>ISamplingPipeline</p>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/","title":"GreedySamplingPipeline","text":"<p>Namespace: LLama.Sampling</p> <p>A sampling pipeline which always selects the most likely token</p> <pre><code>public class GreedySamplingPipeline : BaseSamplingPipeline, ISamplingPipeline, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 BaseSamplingPipeline \u2192 GreedySamplingPipeline Implements ISamplingPipeline, IDisposable</p>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#grammar","title":"Grammar","text":"<p>Grammar to constrain valid tokens</p> <pre><code>public SafeLLamaGrammarHandle Grammar { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#property-value","title":"Property Value","text":"<p>SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#greedysamplingpipeline_1","title":"GreedySamplingPipeline()","text":"<pre><code>public GreedySamplingPipeline()\n</code></pre>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#processlogitssafellamacontexthandle-spansingle-readonlyspanllamatoken","title":"ProcessLogits(SafeLLamaContextHandle, Span&lt;Single&gt;, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<pre><code>protected void ProcessLogits(SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#parameters","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>logits</code> Span&lt;Single&gt;</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#processtokendataarraysafellamacontexthandle-llamatokendataarray-readonlyspanllamatoken","title":"ProcessTokenDataArray(SafeLLamaContextHandle, LLamaTokenDataArray, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<pre><code>protected LLamaToken ProcessTokenDataArray(SafeLLamaContextHandle ctx, LLamaTokenDataArray candidates, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#parameters_1","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArray</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#returns","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#clone","title":"Clone()","text":"<pre><code>public ISamplingPipeline Clone()\n</code></pre>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#returns_1","title":"Returns","text":"<p>ISamplingPipeline</p>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/","title":"ISamplingPipeline","text":"<p>Namespace: LLama.Sampling</p> <p>Convert a span of logits into a single sampled token. This interface can be implemented to completely customise the sampling process.</p> <pre><code>public interface ISamplingPipeline : System.IDisposable\n</code></pre> <p>Implements IDisposable</p>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.isamplingpipeline/#samplesafellamacontexthandle-spansingle-readonlyspanllamatoken","title":"Sample(SafeLLamaContextHandle, Span&lt;Single&gt;, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<p>Sample a single token from the given logits</p> <pre><code>LLamaToken Sample(SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#parameters","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle The context being sampled from</p> <p><code>logits</code> Span&lt;Single&gt; The logits produced by the model</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt; A span of tokens recently returned by the model</p>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#returns","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#acceptsafellamacontexthandle-llamatoken","title":"Accept(SafeLLamaContextHandle, LLamaToken)","text":"<p>Update the pipeline, with knowledge that a particular token was just accepted</p> <pre><code>void Accept(SafeLLamaContextHandle ctx, LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#parameters_1","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#reset","title":"Reset()","text":"<p>Reset all internal state of the sampling pipeline</p> <pre><code>void Reset()\n</code></pre>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#clone","title":"Clone()","text":"<p>Create a copy of this sampling pipeline</p> <pre><code>ISamplingPipeline Clone()\n</code></pre>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#returns_1","title":"Returns","text":"<p>ISamplingPipeline</p>"},{"location":"xmldocs/llama.sampling.isamplingpipelineextensions/","title":"ISamplingPipelineExtensions","text":"<p>Namespace: LLama.Sampling</p> <p>Extensions methods for ISamplingPipeline</p> <pre><code>public static class ISamplingPipelineExtensions\n</code></pre> <p>Inheritance Object \u2192 ISamplingPipelineExtensions</p>"},{"location":"xmldocs/llama.sampling.isamplingpipelineextensions/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.isamplingpipelineextensions/#sampleisamplingpipeline-safellamacontexthandle-spansingle-listllamatoken","title":"Sample(ISamplingPipeline, SafeLLamaContextHandle, Span&lt;Single&gt;, List&lt;LLamaToken&gt;)","text":"<p>Sample a single token from the given logits</p> <pre><code>public static LLamaToken Sample(ISamplingPipeline pipeline, SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, List&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.isamplingpipelineextensions/#parameters","title":"Parameters","text":"<p><code>pipeline</code> ISamplingPipeline</p> <p><code>ctx</code> SafeLLamaContextHandle The context being sampled from</p> <p><code>logits</code> Span&lt;Single&gt; The logits produced by the model</p> <p><code>lastTokens</code> List&lt;LLamaToken&gt; A list of tokens recently returned by the model</p>"},{"location":"xmldocs/llama.sampling.isamplingpipelineextensions/#returns","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/","title":"Mirostate2SamplingPipeline","text":"<p>Namespace: LLama.Sampling</p> <p>A sampling pipeline which uses mirostat (v2) to select tokens</p> <pre><code>public class Mirostate2SamplingPipeline : BaseSamplingPipeline, ISamplingPipeline, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 BaseSamplingPipeline \u2192 Mirostate2SamplingPipeline Implements ISamplingPipeline, IDisposable</p>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#mu","title":"Mu","text":"<p>Currently learned mu value</p> <pre><code>public float Mu { get; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#property-value","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#tau","title":"Tau","text":"<p>target entropy</p> <pre><code>public float Tau { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#property-value_1","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#eta","title":"Eta","text":"<p>learning rate</p> <pre><code>public float Eta { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#property-value_2","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#grammar","title":"Grammar","text":"<p>Grammar to constrain valid tokens</p> <pre><code>public SafeLLamaGrammarHandle Grammar { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#property-value_3","title":"Property Value","text":"<p>SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#mirostate2samplingpipeline_1","title":"Mirostate2SamplingPipeline()","text":"<pre><code>public Mirostate2SamplingPipeline()\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#processlogitssafellamacontexthandle-spansingle-readonlyspanllamatoken","title":"ProcessLogits(SafeLLamaContextHandle, Span&lt;Single&gt;, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<pre><code>protected void ProcessLogits(SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#parameters","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>logits</code> Span&lt;Single&gt;</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#processtokendataarraysafellamacontexthandle-llamatokendataarray-readonlyspanllamatoken","title":"ProcessTokenDataArray(SafeLLamaContextHandle, LLamaTokenDataArray, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<pre><code>protected LLamaToken ProcessTokenDataArray(SafeLLamaContextHandle ctx, LLamaTokenDataArray candidates, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#parameters_1","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArray</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#returns","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#reset","title":"Reset()","text":"<pre><code>public void Reset()\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#clone","title":"Clone()","text":"<pre><code>public ISamplingPipeline Clone()\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostate2samplingpipeline/#returns_1","title":"Returns","text":"<p>ISamplingPipeline</p>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/","title":"MirostateSamplingPipeline","text":"<p>Namespace: LLama.Sampling</p> <p>A sampling pipeline which uses mirostat (v1) to select tokens</p> <pre><code>public class MirostateSamplingPipeline : BaseSamplingPipeline, ISamplingPipeline, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 BaseSamplingPipeline \u2192 MirostateSamplingPipeline Implements ISamplingPipeline, IDisposable</p>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#mu","title":"Mu","text":"<p>Currently learned mu value</p> <pre><code>public float Mu { get; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#property-value","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#tau","title":"Tau","text":"<p>target entropy</p> <pre><code>public float Tau { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#property-value_1","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#eta","title":"Eta","text":"<p>learning rate</p> <pre><code>public float Eta { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#property-value_2","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#grammar","title":"Grammar","text":"<p>Grammar to constrain valid tokens</p> <pre><code>public SafeLLamaGrammarHandle Grammar { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#property-value_3","title":"Property Value","text":"<p>SafeLLamaGrammarHandle</p>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#mirostatesamplingpipeline_1","title":"MirostateSamplingPipeline()","text":"<pre><code>public MirostateSamplingPipeline()\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#processlogitssafellamacontexthandle-spansingle-readonlyspanllamatoken","title":"ProcessLogits(SafeLLamaContextHandle, Span&lt;Single&gt;, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<pre><code>protected void ProcessLogits(SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#parameters","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>logits</code> Span&lt;Single&gt;</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#processtokendataarraysafellamacontexthandle-llamatokendataarray-readonlyspanllamatoken","title":"ProcessTokenDataArray(SafeLLamaContextHandle, LLamaTokenDataArray, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<pre><code>protected LLamaToken ProcessTokenDataArray(SafeLLamaContextHandle ctx, LLamaTokenDataArray candidates, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#parameters_1","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>candidates</code> LLamaTokenDataArray</p> <p><code>lastTokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#returns","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#reset","title":"Reset()","text":"<pre><code>public void Reset()\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#clone","title":"Clone()","text":"<pre><code>public ISamplingPipeline Clone()\n</code></pre>"},{"location":"xmldocs/llama.sampling.mirostatesamplingpipeline/#returns_1","title":"Returns","text":"<p>ISamplingPipeline</p>"},{"location":"xmldocs/llama.sessionstate/","title":"SessionState","text":"<p>Namespace: LLama</p> <p>The state of a chat session in-memory.</p> <pre><code>public class SessionState : System.IEquatable`1[[LLama.SessionState, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 SessionState Implements IEquatable&lt;SessionState&gt;</p>"},{"location":"xmldocs/llama.sessionstate/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.sessionstate/#executorstate","title":"ExecutorState","text":"<p>Saved executor state for the session in JSON format.</p> <pre><code>public ExecutorBaseState ExecutorState { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value","title":"Property Value","text":"<p>ExecutorBaseState</p>"},{"location":"xmldocs/llama.sessionstate/#contextstate","title":"ContextState","text":"<p>Saved context state (KV cache) for the session.</p> <pre><code>public State ContextState { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value_1","title":"Property Value","text":"<p>State</p>"},{"location":"xmldocs/llama.sessionstate/#inputtransformpipeline","title":"InputTransformPipeline","text":"<p>The input transform pipeline used in this session.</p> <pre><code>public ITextTransform[] InputTransformPipeline { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value_2","title":"Property Value","text":"<p>ITextTransform[]</p>"},{"location":"xmldocs/llama.sessionstate/#outputtransform","title":"OutputTransform","text":"<p>The output transform used in this session.</p> <pre><code>public ITextStreamTransform OutputTransform { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value_3","title":"Property Value","text":"<p>ITextStreamTransform</p>"},{"location":"xmldocs/llama.sessionstate/#historytransform","title":"HistoryTransform","text":"<p>The history transform used in this session.</p> <pre><code>public IHistoryTransform HistoryTransform { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value_4","title":"Property Value","text":"<p>IHistoryTransform</p>"},{"location":"xmldocs/llama.sessionstate/#history","title":"History","text":"<p>The the chat history messages for this session.</p> <pre><code>public Message[] History { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value_5","title":"Property Value","text":"<p>Message[]</p>"},{"location":"xmldocs/llama.sessionstate/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.sessionstate/#sessionstatestate-executorbasestate-chathistory-listitexttransform-itextstreamtransform-ihistorytransform","title":"SessionState(State, ExecutorBaseState, ChatHistory, List&lt;ITextTransform&gt;, ITextStreamTransform, IHistoryTransform)","text":"<p>Create a new session state.</p> <pre><code>public SessionState(State contextState, ExecutorBaseState executorState, ChatHistory history, List&lt;ITextTransform&gt; inputTransformPipeline, ITextStreamTransform outputTransform, IHistoryTransform historyTransform)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters","title":"Parameters","text":"<p><code>contextState</code> State</p> <p><code>executorState</code> ExecutorBaseState</p> <p><code>history</code> ChatHistory</p> <p><code>inputTransformPipeline</code> List&lt;ITextTransform&gt;</p> <p><code>outputTransform</code> ITextStreamTransform</p> <p><code>historyTransform</code> IHistoryTransform</p>"},{"location":"xmldocs/llama.sessionstate/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sessionstate/#savestring","title":"Save(String)","text":"<p>Save the session state to folder.</p> <pre><code>public void Save(string path)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters_1","title":"Parameters","text":"<p><code>path</code> String</p>"},{"location":"xmldocs/llama.sessionstate/#loadstring","title":"Load(String)","text":"<p>Load the session state from folder.</p> <pre><code>public static SessionState Load(string path)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters_2","title":"Parameters","text":"<p><code>path</code> String</p>"},{"location":"xmldocs/llama.sessionstate/#returns","title":"Returns","text":"<p>SessionState</p>"},{"location":"xmldocs/llama.sessionstate/#exceptions","title":"Exceptions","text":"<p>ArgumentException Throws when session state is incorrect</p>"},{"location":"xmldocs/llama.sessionstate/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#returns_1","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.sessionstate/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters_3","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.sessionstate/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sessionstate/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#returns_3","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.sessionstate/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters_4","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.sessionstate/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sessionstate/#equalssessionstate","title":"Equals(SessionState)","text":"<pre><code>public bool Equals(SessionState other)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters_5","title":"Parameters","text":"<p><code>other</code> SessionState</p>"},{"location":"xmldocs/llama.sessionstate/#returns_5","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sessionstate/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public SessionState &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#returns_6","title":"Returns","text":"<p>SessionState</p>"},{"location":"xmldocs/llama.streamingtokendecoder/","title":"StreamingTokenDecoder","text":"<p>Namespace: LLama</p> <p>Decodes a stream of tokens into a stream of characters</p> <pre><code>public sealed class StreamingTokenDecoder\n</code></pre> <p>Inheritance Object \u2192 StreamingTokenDecoder</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.streamingtokendecoder/#availablecharacters","title":"AvailableCharacters","text":"<p>The number of decoded characters waiting to be read</p> <pre><code>public int AvailableCharacters { get; }\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.streamingtokendecoder/#streamingtokendecoderencoding-llamaweights","title":"StreamingTokenDecoder(Encoding, LLamaWeights)","text":"<p>Create a new decoder</p> <pre><code>public StreamingTokenDecoder(Encoding encoding, LLamaWeights weights)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters","title":"Parameters","text":"<p><code>encoding</code> Encoding Text encoding to use</p> <p><code>weights</code> LLamaWeights Model weights</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#streamingtokendecoderllamacontext","title":"StreamingTokenDecoder(LLamaContext)","text":"<p>Create a new decoder</p> <pre><code>public StreamingTokenDecoder(LLamaContext context)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_1","title":"Parameters","text":"<p><code>context</code> LLamaContext Context to retrieve encoding and model weights from</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#streamingtokendecoderencoding-safellamacontexthandle","title":"StreamingTokenDecoder(Encoding, SafeLLamaContextHandle)","text":"<p>Create a new decoder</p> <pre><code>public StreamingTokenDecoder(Encoding encoding, SafeLLamaContextHandle context)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_2","title":"Parameters","text":"<p><code>encoding</code> Encoding Text encoding to use</p> <p><code>context</code> SafeLLamaContextHandle Context to retrieve model weights from</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#streamingtokendecoderencoding-safellamamodelhandle","title":"StreamingTokenDecoder(Encoding, SafeLlamaModelHandle)","text":"<p>Create a new decoder</p> <pre><code>public StreamingTokenDecoder(Encoding encoding, SafeLlamaModelHandle weights)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_3","title":"Parameters","text":"<p><code>encoding</code> Encoding Text encoding to use</p> <p><code>weights</code> SafeLlamaModelHandle Models weights to use</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.streamingtokendecoder/#addllamatoken","title":"Add(LLamaToken)","text":"<p>Add a single token to the decoder</p> <pre><code>public void Add(LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_4","title":"Parameters","text":"<p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#addint32","title":"Add(Int32)","text":"<p>Add a single token to the decoder</p> <pre><code>public void Add(int token)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_5","title":"Parameters","text":"<p><code>token</code> Int32</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#addrangett","title":"AddRange&lt;T&gt;(T)","text":"<p>Add all tokens in the given enumerable</p> <pre><code>public void AddRange&lt;T&gt;(T tokens)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#type-parameters","title":"Type Parameters","text":"<p><code>T</code></p>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_6","title":"Parameters","text":"<p><code>tokens</code> T</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#addrangereadonlyspanllamatoken","title":"AddRange(ReadOnlySpan&lt;LLamaToken&gt;)","text":"<p>Add all tokens in the given span</p> <pre><code>public void AddRange(ReadOnlySpan&lt;LLamaToken&gt; tokens)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_7","title":"Parameters","text":"<p><code>tokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#readlistchar","title":"Read(List&lt;Char&gt;)","text":"<p>Read all decoded characters and clear the buffer</p> <pre><code>public void Read(List&lt;char&gt; dest)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_8","title":"Parameters","text":"<p><code>dest</code> List&lt;Char&gt;</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#read","title":"Read()","text":"<p>Read all decoded characters as a string and clear the buffer</p> <pre><code>public string Read()\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#reset","title":"Reset()","text":"<p>Set the decoder back to its initial state</p> <pre><code>public void Reset()\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#addg__tokentobytes9_0byte-llamatoken-safellamamodelhandle","title":"&lt;Add&gt;g__TokenToBytes|9_0(Byte[]&amp;, LLamaToken, SafeLlamaModelHandle)","text":"<pre><code>internal static Span&lt;byte&gt; &lt;Add&gt;g__TokenToBytes|9_0(Byte[]&amp; bytes, LLamaToken token, SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_9","title":"Parameters","text":"<p><code>bytes</code> Byte[]&amp;</p> <p><code>token</code> LLamaToken</p> <p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#returns_1","title":"Returns","text":"<p>Span&lt;Byte&gt;</p>"},{"location":"xmldocs/llama.types.chatcompletion/","title":"ChatCompletion","text":"<p>Namespace: LLama.Types</p> <pre><code>public class ChatCompletion : System.IEquatable`1[[LLama.Types.ChatCompletion, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 ChatCompletion Implements IEquatable&lt;ChatCompletion&gt;</p>"},{"location":"xmldocs/llama.types.chatcompletion/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.chatcompletion/#id","title":"Id","text":"<pre><code>public string Id { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletion/#object","title":"Object","text":"<pre><code>public string Object { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletion/#created","title":"Created","text":"<pre><code>public int Created { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.chatcompletion/#model","title":"Model","text":"<pre><code>public string Model { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#property-value_3","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletion/#choices","title":"Choices","text":"<pre><code>public ChatCompletionChoice[] Choices { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#property-value_4","title":"Property Value","text":"<p>ChatCompletionChoice[]</p>"},{"location":"xmldocs/llama.types.chatcompletion/#usage","title":"Usage","text":"<pre><code>public CompletionUsage Usage { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#property-value_5","title":"Property Value","text":"<p>CompletionUsage</p>"},{"location":"xmldocs/llama.types.chatcompletion/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.chatcompletion/#chatcompletionstring-string-int32-string-chatcompletionchoice-completionusage","title":"ChatCompletion(String, String, Int32, String, ChatCompletionChoice[], CompletionUsage)","text":"<pre><code>public ChatCompletion(string Id, string Object, int Created, string Model, ChatCompletionChoice[] Choices, CompletionUsage Usage)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#parameters","title":"Parameters","text":"<p><code>Id</code> String</p> <p><code>Object</code> String</p> <p><code>Created</code> Int32</p> <p><code>Model</code> String</p> <p><code>Choices</code> ChatCompletionChoice[]</p> <p><code>Usage</code> CompletionUsage</p>"},{"location":"xmldocs/llama.types.chatcompletion/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.chatcompletion/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletion/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.chatcompletion/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletion/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.chatcompletion/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.chatcompletion/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletion/#equalschatcompletion","title":"Equals(ChatCompletion)","text":"<pre><code>public bool Equals(ChatCompletion other)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#parameters_3","title":"Parameters","text":"<p><code>other</code> ChatCompletion</p>"},{"location":"xmldocs/llama.types.chatcompletion/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletion/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public ChatCompletion &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#returns_5","title":"Returns","text":"<p>ChatCompletion</p>"},{"location":"xmldocs/llama.types.chatcompletion/#deconstructstring-string-int32-string-chatcompletionchoice-completionusage","title":"Deconstruct(String&amp;, String&amp;, Int32&amp;, String&amp;, ChatCompletionChoice[]&amp;, CompletionUsage&amp;)","text":"<pre><code>public void Deconstruct(String&amp; Id, String&amp; Object, Int32&amp; Created, String&amp; Model, ChatCompletionChoice[]&amp; Choices, CompletionUsage&amp; Usage)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletion/#parameters_4","title":"Parameters","text":"<p><code>Id</code> String&amp;</p> <p><code>Object</code> String&amp;</p> <p><code>Created</code> Int32&amp;</p> <p><code>Model</code> String&amp;</p> <p><code>Choices</code> ChatCompletionChoice[]&amp;</p> <p><code>Usage</code> CompletionUsage&amp;</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/","title":"ChatCompletionChoice","text":"<p>Namespace: LLama.Types</p> <pre><code>public class ChatCompletionChoice : System.IEquatable`1[[LLama.Types.ChatCompletionChoice, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 ChatCompletionChoice Implements IEquatable&lt;ChatCompletionChoice&gt;</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.chatcompletionchoice/#index","title":"Index","text":"<pre><code>public int Index { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#message","title":"Message","text":"<pre><code>public ChatCompletionMessage Message { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#property-value_1","title":"Property Value","text":"<p>ChatCompletionMessage</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#finishreason","title":"FinishReason","text":"<pre><code>public string FinishReason { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#property-value_2","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.chatcompletionchoice/#chatcompletionchoiceint32-chatcompletionmessage-string","title":"ChatCompletionChoice(Int32, ChatCompletionMessage, String)","text":"<pre><code>public ChatCompletionChoice(int Index, ChatCompletionMessage Message, string FinishReason)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#parameters","title":"Parameters","text":"<p><code>Index</code> Int32</p> <p><code>Message</code> ChatCompletionMessage</p> <p><code>FinishReason</code> String</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.chatcompletionchoice/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#equalschatcompletionchoice","title":"Equals(ChatCompletionChoice)","text":"<pre><code>public bool Equals(ChatCompletionChoice other)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#parameters_3","title":"Parameters","text":"<p><code>other</code> ChatCompletionChoice</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public ChatCompletionChoice &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#returns_5","title":"Returns","text":"<p>ChatCompletionChoice</p>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#deconstructint32-chatcompletionmessage-string","title":"Deconstruct(Int32&amp;, ChatCompletionMessage&amp;, String&amp;)","text":"<pre><code>public void Deconstruct(Int32&amp; Index, ChatCompletionMessage&amp; Message, String&amp; FinishReason)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchoice/#parameters_4","title":"Parameters","text":"<p><code>Index</code> Int32&amp;</p> <p><code>Message</code> ChatCompletionMessage&amp;</p> <p><code>FinishReason</code> String&amp;</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/","title":"ChatCompletionChunk","text":"<p>Namespace: LLama.Types</p> <pre><code>public class ChatCompletionChunk : System.IEquatable`1[[LLama.Types.ChatCompletionChunk, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 ChatCompletionChunk Implements IEquatable&lt;ChatCompletionChunk&gt;</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.chatcompletionchunk/#id","title":"Id","text":"<pre><code>public string Id { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#model","title":"Model","text":"<pre><code>public string Model { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#object","title":"Object","text":"<pre><code>public string Object { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#property-value_2","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#created","title":"Created","text":"<pre><code>public int Created { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#property-value_3","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#choices","title":"Choices","text":"<pre><code>public ChatCompletionChunkChoice[] Choices { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#property-value_4","title":"Property Value","text":"<p>ChatCompletionChunkChoice[]</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.chatcompletionchunk/#chatcompletionchunkstring-string-string-int32-chatcompletionchunkchoice","title":"ChatCompletionChunk(String, String, String, Int32, ChatCompletionChunkChoice[])","text":"<pre><code>public ChatCompletionChunk(string Id, string Model, string Object, int Created, ChatCompletionChunkChoice[] Choices)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#parameters","title":"Parameters","text":"<p><code>Id</code> String</p> <p><code>Model</code> String</p> <p><code>Object</code> String</p> <p><code>Created</code> Int32</p> <p><code>Choices</code> ChatCompletionChunkChoice[]</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.chatcompletionchunk/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#equalschatcompletionchunk","title":"Equals(ChatCompletionChunk)","text":"<pre><code>public bool Equals(ChatCompletionChunk other)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#parameters_3","title":"Parameters","text":"<p><code>other</code> ChatCompletionChunk</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public ChatCompletionChunk &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#returns_5","title":"Returns","text":"<p>ChatCompletionChunk</p>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#deconstructstring-string-string-int32-chatcompletionchunkchoice","title":"Deconstruct(String&amp;, String&amp;, String&amp;, Int32&amp;, ChatCompletionChunkChoice[]&amp;)","text":"<pre><code>public void Deconstruct(String&amp; Id, String&amp; Model, String&amp; Object, Int32&amp; Created, ChatCompletionChunkChoice[]&amp; Choices)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunk/#parameters_4","title":"Parameters","text":"<p><code>Id</code> String&amp;</p> <p><code>Model</code> String&amp;</p> <p><code>Object</code> String&amp;</p> <p><code>Created</code> Int32&amp;</p> <p><code>Choices</code> ChatCompletionChunkChoice[]&amp;</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/","title":"ChatCompletionChunkChoice","text":"<p>Namespace: LLama.Types</p> <pre><code>public class ChatCompletionChunkChoice : System.IEquatable`1[[LLama.Types.ChatCompletionChunkChoice, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 ChatCompletionChunkChoice Implements IEquatable&lt;ChatCompletionChunkChoice&gt;</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#index","title":"Index","text":"<pre><code>public int Index { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#delta","title":"Delta","text":"<pre><code>public ChatCompletionChunkDelta Delta { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#property-value_1","title":"Property Value","text":"<p>ChatCompletionChunkDelta</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#finishreason","title":"FinishReason","text":"<pre><code>public string FinishReason { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#property-value_2","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#chatcompletionchunkchoiceint32-chatcompletionchunkdelta-string","title":"ChatCompletionChunkChoice(Int32, ChatCompletionChunkDelta, String)","text":"<pre><code>public ChatCompletionChunkChoice(int Index, ChatCompletionChunkDelta Delta, string FinishReason)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#parameters","title":"Parameters","text":"<p><code>Index</code> Int32</p> <p><code>Delta</code> ChatCompletionChunkDelta</p> <p><code>FinishReason</code> String</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#equalschatcompletionchunkchoice","title":"Equals(ChatCompletionChunkChoice)","text":"<pre><code>public bool Equals(ChatCompletionChunkChoice other)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#parameters_3","title":"Parameters","text":"<p><code>other</code> ChatCompletionChunkChoice</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public ChatCompletionChunkChoice &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#returns_5","title":"Returns","text":"<p>ChatCompletionChunkChoice</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#deconstructint32-chatcompletionchunkdelta-string","title":"Deconstruct(Int32&amp;, ChatCompletionChunkDelta&amp;, String&amp;)","text":"<pre><code>public void Deconstruct(Int32&amp; Index, ChatCompletionChunkDelta&amp; Delta, String&amp; FinishReason)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkchoice/#parameters_4","title":"Parameters","text":"<p><code>Index</code> Int32&amp;</p> <p><code>Delta</code> ChatCompletionChunkDelta&amp;</p> <p><code>FinishReason</code> String&amp;</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/","title":"ChatCompletionChunkDelta","text":"<p>Namespace: LLama.Types</p> <pre><code>public class ChatCompletionChunkDelta : System.IEquatable`1[[LLama.Types.ChatCompletionChunkDelta, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 ChatCompletionChunkDelta Implements IEquatable&lt;ChatCompletionChunkDelta&gt;</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#role","title":"Role","text":"<pre><code>public string Role { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#content","title":"Content","text":"<pre><code>public string Content { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#chatcompletionchunkdeltastring-string","title":"ChatCompletionChunkDelta(String, String)","text":"<pre><code>public ChatCompletionChunkDelta(string Role, string Content)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#parameters","title":"Parameters","text":"<p><code>Role</code> String</p> <p><code>Content</code> String</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#equalschatcompletionchunkdelta","title":"Equals(ChatCompletionChunkDelta)","text":"<pre><code>public bool Equals(ChatCompletionChunkDelta other)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#parameters_3","title":"Parameters","text":"<p><code>other</code> ChatCompletionChunkDelta</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public ChatCompletionChunkDelta &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#returns_5","title":"Returns","text":"<p>ChatCompletionChunkDelta</p>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#deconstructstring-string","title":"Deconstruct(String&amp;, String&amp;)","text":"<pre><code>public void Deconstruct(String&amp; Role, String&amp; Content)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionchunkdelta/#parameters_4","title":"Parameters","text":"<p><code>Role</code> String&amp;</p> <p><code>Content</code> String&amp;</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/","title":"ChatCompletionMessage","text":"<p>Namespace: LLama.Types</p> <pre><code>public class ChatCompletionMessage : System.IEquatable`1[[LLama.Types.ChatCompletionMessage, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 ChatCompletionMessage Implements IEquatable&lt;ChatCompletionMessage&gt;</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.chatcompletionmessage/#role","title":"Role","text":"<pre><code>public ChatRole Role { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#property-value","title":"Property Value","text":"<p>ChatRole</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#content","title":"Content","text":"<pre><code>public string Content { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#name","title":"Name","text":"<pre><code>public string Name { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#property-value_2","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.chatcompletionmessage/#chatcompletionmessagechatrole-string-string","title":"ChatCompletionMessage(ChatRole, String, String)","text":"<pre><code>public ChatCompletionMessage(ChatRole Role, string Content, string Name)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#parameters","title":"Parameters","text":"<p><code>Role</code> ChatRole</p> <p><code>Content</code> String</p> <p><code>Name</code> String</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.chatcompletionmessage/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#equalschatcompletionmessage","title":"Equals(ChatCompletionMessage)","text":"<pre><code>public bool Equals(ChatCompletionMessage other)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#parameters_3","title":"Parameters","text":"<p><code>other</code> ChatCompletionMessage</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public ChatCompletionMessage &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#returns_5","title":"Returns","text":"<p>ChatCompletionMessage</p>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#deconstructchatrole-string-string","title":"Deconstruct(ChatRole&amp;, String&amp;, String&amp;)","text":"<pre><code>public void Deconstruct(ChatRole&amp; Role, String&amp; Content, String&amp; Name)\n</code></pre>"},{"location":"xmldocs/llama.types.chatcompletionmessage/#parameters_4","title":"Parameters","text":"<p><code>Role</code> ChatRole&amp;</p> <p><code>Content</code> String&amp;</p> <p><code>Name</code> String&amp;</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/","title":"ChatMessageRecord","text":"<p>Namespace: LLama.Types</p> <pre><code>public class ChatMessageRecord : System.IEquatable`1[[LLama.Types.ChatMessageRecord, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 ChatMessageRecord Implements IEquatable&lt;ChatMessageRecord&gt;</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.chatmessagerecord/#message","title":"Message","text":"<pre><code>public ChatCompletionMessage Message { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatmessagerecord/#property-value","title":"Property Value","text":"<p>ChatCompletionMessage</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#time","title":"Time","text":"<pre><code>public DateTime Time { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.chatmessagerecord/#property-value_1","title":"Property Value","text":"<p>DateTime</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.chatmessagerecord/#chatmessagerecordchatcompletionmessage-datetime","title":"ChatMessageRecord(ChatCompletionMessage, DateTime)","text":"<pre><code>public ChatMessageRecord(ChatCompletionMessage Message, DateTime Time)\n</code></pre>"},{"location":"xmldocs/llama.types.chatmessagerecord/#parameters","title":"Parameters","text":"<p><code>Message</code> ChatCompletionMessage</p> <p><code>Time</code> DateTime</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.chatmessagerecord/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.chatmessagerecord/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.chatmessagerecord/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.chatmessagerecord/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.chatmessagerecord/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#equalschatmessagerecord","title":"Equals(ChatMessageRecord)","text":"<pre><code>public bool Equals(ChatMessageRecord other)\n</code></pre>"},{"location":"xmldocs/llama.types.chatmessagerecord/#parameters_3","title":"Parameters","text":"<p><code>other</code> ChatMessageRecord</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public ChatMessageRecord &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.chatmessagerecord/#returns_5","title":"Returns","text":"<p>ChatMessageRecord</p>"},{"location":"xmldocs/llama.types.chatmessagerecord/#deconstructchatcompletionmessage-datetime","title":"Deconstruct(ChatCompletionMessage&amp;, DateTime&amp;)","text":"<pre><code>public void Deconstruct(ChatCompletionMessage&amp; Message, DateTime&amp; Time)\n</code></pre>"},{"location":"xmldocs/llama.types.chatmessagerecord/#parameters_4","title":"Parameters","text":"<p><code>Message</code> ChatCompletionMessage&amp;</p> <p><code>Time</code> DateTime&amp;</p>"},{"location":"xmldocs/llama.types.chatrole/","title":"ChatRole","text":"<p>Namespace: LLama.Types</p> <pre><code>public enum ChatRole\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 ChatRole Implements IComparable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.types.chatrole/#fields","title":"Fields","text":"Name Value Description"},{"location":"xmldocs/llama.types.completion/","title":"Completion","text":"<p>Namespace: LLama.Types</p> <pre><code>public class Completion : System.IEquatable`1[[LLama.Types.Completion, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 Completion Implements IEquatable&lt;Completion&gt;</p>"},{"location":"xmldocs/llama.types.completion/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.completion/#id","title":"Id","text":"<pre><code>public string Id { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completion/#object","title":"Object","text":"<pre><code>public string Object { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completion/#created","title":"Created","text":"<pre><code>public int Created { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.completion/#model","title":"Model","text":"<pre><code>public string Model { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#property-value_3","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completion/#choices","title":"Choices","text":"<pre><code>public CompletionChoice[] Choices { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#property-value_4","title":"Property Value","text":"<p>CompletionChoice[]</p>"},{"location":"xmldocs/llama.types.completion/#usage","title":"Usage","text":"<pre><code>public CompletionUsage Usage { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#property-value_5","title":"Property Value","text":"<p>CompletionUsage</p>"},{"location":"xmldocs/llama.types.completion/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.completion/#completionstring-string-int32-string-completionchoice-completionusage","title":"Completion(String, String, Int32, String, CompletionChoice[], CompletionUsage)","text":"<pre><code>public Completion(string Id, string Object, int Created, string Model, CompletionChoice[] Choices, CompletionUsage Usage)\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#parameters","title":"Parameters","text":"<p><code>Id</code> String</p> <p><code>Object</code> String</p> <p><code>Created</code> Int32</p> <p><code>Model</code> String</p> <p><code>Choices</code> CompletionChoice[]</p> <p><code>Usage</code> CompletionUsage</p>"},{"location":"xmldocs/llama.types.completion/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.completion/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completion/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.completion/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completion/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.completion/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.completion/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completion/#equalscompletion","title":"Equals(Completion)","text":"<pre><code>public bool Equals(Completion other)\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#parameters_3","title":"Parameters","text":"<p><code>other</code> Completion</p>"},{"location":"xmldocs/llama.types.completion/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completion/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public Completion &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#returns_5","title":"Returns","text":"<p>Completion</p>"},{"location":"xmldocs/llama.types.completion/#deconstructstring-string-int32-string-completionchoice-completionusage","title":"Deconstruct(String&amp;, String&amp;, Int32&amp;, String&amp;, CompletionChoice[]&amp;, CompletionUsage&amp;)","text":"<pre><code>public void Deconstruct(String&amp; Id, String&amp; Object, Int32&amp; Created, String&amp; Model, CompletionChoice[]&amp; Choices, CompletionUsage&amp; Usage)\n</code></pre>"},{"location":"xmldocs/llama.types.completion/#parameters_4","title":"Parameters","text":"<p><code>Id</code> String&amp;</p> <p><code>Object</code> String&amp;</p> <p><code>Created</code> Int32&amp;</p> <p><code>Model</code> String&amp;</p> <p><code>Choices</code> CompletionChoice[]&amp;</p> <p><code>Usage</code> CompletionUsage&amp;</p>"},{"location":"xmldocs/llama.types.completionchoice/","title":"CompletionChoice","text":"<p>Namespace: LLama.Types</p> <pre><code>public class CompletionChoice : System.IEquatable`1[[LLama.Types.CompletionChoice, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 CompletionChoice Implements IEquatable&lt;CompletionChoice&gt;</p>"},{"location":"xmldocs/llama.types.completionchoice/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.completionchoice/#text","title":"Text","text":"<pre><code>public string Text { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completionchoice/#index","title":"Index","text":"<pre><code>public int Index { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.completionchoice/#logprobs","title":"Logprobs","text":"<pre><code>public CompletionLogprobs Logprobs { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#property-value_2","title":"Property Value","text":"<p>CompletionLogprobs</p>"},{"location":"xmldocs/llama.types.completionchoice/#finishreason","title":"FinishReason","text":"<pre><code>public string FinishReason { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#property-value_3","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completionchoice/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.completionchoice/#completionchoicestring-int32-completionlogprobs-string","title":"CompletionChoice(String, Int32, CompletionLogprobs, String)","text":"<pre><code>public CompletionChoice(string Text, int Index, CompletionLogprobs Logprobs, string FinishReason)\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#parameters","title":"Parameters","text":"<p><code>Text</code> String</p> <p><code>Index</code> Int32</p> <p><code>Logprobs</code> CompletionLogprobs</p> <p><code>FinishReason</code> String</p>"},{"location":"xmldocs/llama.types.completionchoice/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.completionchoice/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completionchoice/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.completionchoice/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionchoice/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.completionchoice/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.completionchoice/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionchoice/#equalscompletionchoice","title":"Equals(CompletionChoice)","text":"<pre><code>public bool Equals(CompletionChoice other)\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#parameters_3","title":"Parameters","text":"<p><code>other</code> CompletionChoice</p>"},{"location":"xmldocs/llama.types.completionchoice/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionchoice/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public CompletionChoice &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#returns_5","title":"Returns","text":"<p>CompletionChoice</p>"},{"location":"xmldocs/llama.types.completionchoice/#deconstructstring-int32-completionlogprobs-string","title":"Deconstruct(String&amp;, Int32&amp;, CompletionLogprobs&amp;, String&amp;)","text":"<pre><code>public void Deconstruct(String&amp; Text, Int32&amp; Index, CompletionLogprobs&amp; Logprobs, String&amp; FinishReason)\n</code></pre>"},{"location":"xmldocs/llama.types.completionchoice/#parameters_4","title":"Parameters","text":"<p><code>Text</code> String&amp;</p> <p><code>Index</code> Int32&amp;</p> <p><code>Logprobs</code> CompletionLogprobs&amp;</p> <p><code>FinishReason</code> String&amp;</p>"},{"location":"xmldocs/llama.types.completionchunk/","title":"CompletionChunk","text":"<p>Namespace: LLama.Types</p> <pre><code>public class CompletionChunk : System.IEquatable`1[[LLama.Types.CompletionChunk, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 CompletionChunk Implements IEquatable&lt;CompletionChunk&gt;</p>"},{"location":"xmldocs/llama.types.completionchunk/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.completionchunk/#id","title":"Id","text":"<pre><code>public string Id { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completionchunk/#object","title":"Object","text":"<pre><code>public string Object { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completionchunk/#created","title":"Created","text":"<pre><code>public int Created { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.completionchunk/#model","title":"Model","text":"<pre><code>public string Model { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#property-value_3","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completionchunk/#choices","title":"Choices","text":"<pre><code>public CompletionChoice[] Choices { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#property-value_4","title":"Property Value","text":"<p>CompletionChoice[]</p>"},{"location":"xmldocs/llama.types.completionchunk/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.completionchunk/#completionchunkstring-string-int32-string-completionchoice","title":"CompletionChunk(String, String, Int32, String, CompletionChoice[])","text":"<pre><code>public CompletionChunk(string Id, string Object, int Created, string Model, CompletionChoice[] Choices)\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#parameters","title":"Parameters","text":"<p><code>Id</code> String</p> <p><code>Object</code> String</p> <p><code>Created</code> Int32</p> <p><code>Model</code> String</p> <p><code>Choices</code> CompletionChoice[]</p>"},{"location":"xmldocs/llama.types.completionchunk/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.completionchunk/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completionchunk/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.completionchunk/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionchunk/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.completionchunk/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.completionchunk/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionchunk/#equalscompletionchunk","title":"Equals(CompletionChunk)","text":"<pre><code>public bool Equals(CompletionChunk other)\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#parameters_3","title":"Parameters","text":"<p><code>other</code> CompletionChunk</p>"},{"location":"xmldocs/llama.types.completionchunk/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionchunk/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public CompletionChunk &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#returns_5","title":"Returns","text":"<p>CompletionChunk</p>"},{"location":"xmldocs/llama.types.completionchunk/#deconstructstring-string-int32-string-completionchoice","title":"Deconstruct(String&amp;, String&amp;, Int32&amp;, String&amp;, CompletionChoice[]&amp;)","text":"<pre><code>public void Deconstruct(String&amp; Id, String&amp; Object, Int32&amp; Created, String&amp; Model, CompletionChoice[]&amp; Choices)\n</code></pre>"},{"location":"xmldocs/llama.types.completionchunk/#parameters_4","title":"Parameters","text":"<p><code>Id</code> String&amp;</p> <p><code>Object</code> String&amp;</p> <p><code>Created</code> Int32&amp;</p> <p><code>Model</code> String&amp;</p> <p><code>Choices</code> CompletionChoice[]&amp;</p>"},{"location":"xmldocs/llama.types.completionlogprobs/","title":"CompletionLogprobs","text":"<p>Namespace: LLama.Types</p> <pre><code>public class CompletionLogprobs : System.IEquatable`1[[LLama.Types.CompletionLogprobs, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 CompletionLogprobs Implements IEquatable&lt;CompletionLogprobs&gt;</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.completionlogprobs/#textoffset","title":"TextOffset","text":"<pre><code>public Int32[] TextOffset { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#property-value","title":"Property Value","text":"<p>Int32[]</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#tokenlogprobs","title":"TokenLogProbs","text":"<pre><code>public Single[] TokenLogProbs { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#property-value_1","title":"Property Value","text":"<p>Single[]</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#tokens","title":"Tokens","text":"<pre><code>public String[] Tokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#property-value_2","title":"Property Value","text":"<p>String[]</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#toplogprobs","title":"TopLogprobs","text":"<pre><code>public Dictionary`2[] TopLogprobs { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#property-value_3","title":"Property Value","text":"<p>Dictionary`2[]</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.completionlogprobs/#completionlogprobsint32-single-string-dictionary2","title":"CompletionLogprobs(Int32[], Single[], String[], Dictionary`2[])","text":"<pre><code>public CompletionLogprobs(Int32[] TextOffset, Single[] TokenLogProbs, String[] Tokens, Dictionary`2[] TopLogprobs)\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#parameters","title":"Parameters","text":"<p><code>TextOffset</code> Int32[]</p> <p><code>TokenLogProbs</code> Single[]</p> <p><code>Tokens</code> String[]</p> <p><code>TopLogprobs</code> Dictionary`2[]</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.completionlogprobs/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#equalscompletionlogprobs","title":"Equals(CompletionLogprobs)","text":"<pre><code>public bool Equals(CompletionLogprobs other)\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#parameters_3","title":"Parameters","text":"<p><code>other</code> CompletionLogprobs</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public CompletionLogprobs &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#returns_5","title":"Returns","text":"<p>CompletionLogprobs</p>"},{"location":"xmldocs/llama.types.completionlogprobs/#deconstructint32-single-string-dictionary2","title":"Deconstruct(Int32[]&amp;, Single[]&amp;, String[]&amp;, Dictionary`2[]&amp;)","text":"<pre><code>public void Deconstruct(Int32[]&amp; TextOffset, Single[]&amp; TokenLogProbs, String[]&amp; Tokens, Dictionary`2[]&amp; TopLogprobs)\n</code></pre>"},{"location":"xmldocs/llama.types.completionlogprobs/#parameters_4","title":"Parameters","text":"<p><code>TextOffset</code> Int32[]&amp;</p> <p><code>TokenLogProbs</code> Single[]&amp;</p> <p><code>Tokens</code> String[]&amp;</p> <p><code>TopLogprobs</code> Dictionary`2[]&amp;</p>"},{"location":"xmldocs/llama.types.completionusage/","title":"CompletionUsage","text":"<p>Namespace: LLama.Types</p> <pre><code>public class CompletionUsage : System.IEquatable`1[[LLama.Types.CompletionUsage, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 CompletionUsage Implements IEquatable&lt;CompletionUsage&gt;</p>"},{"location":"xmldocs/llama.types.completionusage/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.completionusage/#prompttokens","title":"PromptTokens","text":"<pre><code>public int PromptTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionusage/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.completionusage/#completiontokens","title":"CompletionTokens","text":"<pre><code>public int CompletionTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionusage/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.completionusage/#totaltokens","title":"TotalTokens","text":"<pre><code>public int TotalTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.completionusage/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.completionusage/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.completionusage/#completionusageint32-int32-int32","title":"CompletionUsage(Int32, Int32, Int32)","text":"<pre><code>public CompletionUsage(int PromptTokens, int CompletionTokens, int TotalTokens)\n</code></pre>"},{"location":"xmldocs/llama.types.completionusage/#parameters","title":"Parameters","text":"<p><code>PromptTokens</code> Int32</p> <p><code>CompletionTokens</code> Int32</p> <p><code>TotalTokens</code> Int32</p>"},{"location":"xmldocs/llama.types.completionusage/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.completionusage/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.completionusage/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.completionusage/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.completionusage/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.completionusage/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionusage/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.completionusage/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.completionusage/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.completionusage/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.completionusage/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionusage/#equalscompletionusage","title":"Equals(CompletionUsage)","text":"<pre><code>public bool Equals(CompletionUsage other)\n</code></pre>"},{"location":"xmldocs/llama.types.completionusage/#parameters_3","title":"Parameters","text":"<p><code>other</code> CompletionUsage</p>"},{"location":"xmldocs/llama.types.completionusage/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.completionusage/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public CompletionUsage &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.completionusage/#returns_5","title":"Returns","text":"<p>CompletionUsage</p>"},{"location":"xmldocs/llama.types.completionusage/#deconstructint32-int32-int32","title":"Deconstruct(Int32&amp;, Int32&amp;, Int32&amp;)","text":"<pre><code>public void Deconstruct(Int32&amp; PromptTokens, Int32&amp; CompletionTokens, Int32&amp; TotalTokens)\n</code></pre>"},{"location":"xmldocs/llama.types.completionusage/#parameters_4","title":"Parameters","text":"<p><code>PromptTokens</code> Int32&amp;</p> <p><code>CompletionTokens</code> Int32&amp;</p> <p><code>TotalTokens</code> Int32&amp;</p>"},{"location":"xmldocs/llama.types.embedding/","title":"Embedding","text":"<p>Namespace: LLama.Types</p> <pre><code>public class Embedding : System.IEquatable`1[[LLama.Types.Embedding, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 Embedding Implements IEquatable&lt;Embedding&gt;</p>"},{"location":"xmldocs/llama.types.embedding/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.embedding/#object","title":"Object","text":"<pre><code>public string Object { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.embedding/#model","title":"Model","text":"<pre><code>public string Model { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.embedding/#data","title":"Data","text":"<pre><code>public EmbeddingData[] Data { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#property-value_2","title":"Property Value","text":"<p>EmbeddingData[]</p>"},{"location":"xmldocs/llama.types.embedding/#usage","title":"Usage","text":"<pre><code>public EmbeddingUsage Usage { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#property-value_3","title":"Property Value","text":"<p>EmbeddingUsage</p>"},{"location":"xmldocs/llama.types.embedding/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.embedding/#embeddingstring-string-embeddingdata-embeddingusage","title":"Embedding(String, String, EmbeddingData[], EmbeddingUsage)","text":"<pre><code>public Embedding(string Object, string Model, EmbeddingData[] Data, EmbeddingUsage Usage)\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#parameters","title":"Parameters","text":"<p><code>Object</code> String</p> <p><code>Model</code> String</p> <p><code>Data</code> EmbeddingData[]</p> <p><code>Usage</code> EmbeddingUsage</p>"},{"location":"xmldocs/llama.types.embedding/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.embedding/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.embedding/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.embedding/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.embedding/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.embedding/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.embedding/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.embedding/#equalsembedding","title":"Equals(Embedding)","text":"<pre><code>public bool Equals(Embedding other)\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#parameters_3","title":"Parameters","text":"<p><code>other</code> Embedding</p>"},{"location":"xmldocs/llama.types.embedding/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.embedding/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public Embedding &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#returns_5","title":"Returns","text":"<p>Embedding</p>"},{"location":"xmldocs/llama.types.embedding/#deconstructstring-string-embeddingdata-embeddingusage","title":"Deconstruct(String&amp;, String&amp;, EmbeddingData[]&amp;, EmbeddingUsage&amp;)","text":"<pre><code>public void Deconstruct(String&amp; Object, String&amp; Model, EmbeddingData[]&amp; Data, EmbeddingUsage&amp; Usage)\n</code></pre>"},{"location":"xmldocs/llama.types.embedding/#parameters_4","title":"Parameters","text":"<p><code>Object</code> String&amp;</p> <p><code>Model</code> String&amp;</p> <p><code>Data</code> EmbeddingData[]&amp;</p> <p><code>Usage</code> EmbeddingUsage&amp;</p>"},{"location":"xmldocs/llama.types.embeddingdata/","title":"EmbeddingData","text":"<p>Namespace: LLama.Types</p> <pre><code>public class EmbeddingData : System.IEquatable`1[[LLama.Types.EmbeddingData, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 EmbeddingData Implements IEquatable&lt;EmbeddingData&gt;</p>"},{"location":"xmldocs/llama.types.embeddingdata/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.embeddingdata/#index","title":"Index","text":"<pre><code>public int Index { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingdata/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.embeddingdata/#object","title":"Object","text":"<pre><code>public string Object { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingdata/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.types.embeddingdata/#embedding","title":"Embedding","text":"<pre><code>public Single[] Embedding { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingdata/#property-value_2","title":"Property Value","text":"<p>Single[]</p>"},{"location":"xmldocs/llama.types.embeddingdata/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.embeddingdata/#embeddingdataint32-string-single","title":"EmbeddingData(Int32, String, Single[])","text":"<pre><code>public EmbeddingData(int Index, string Object, Single[] Embedding)\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingdata/#parameters","title":"Parameters","text":"<p><code>Index</code> Int32</p> <p><code>Object</code> String</p> <p><code>Embedding</code> Single[]</p>"},{"location":"xmldocs/llama.types.embeddingdata/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.embeddingdata/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingdata/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.embeddingdata/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingdata/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.embeddingdata/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.embeddingdata/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingdata/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.embeddingdata/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingdata/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.embeddingdata/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.embeddingdata/#equalsembeddingdata","title":"Equals(EmbeddingData)","text":"<pre><code>public bool Equals(EmbeddingData other)\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingdata/#parameters_3","title":"Parameters","text":"<p><code>other</code> EmbeddingData</p>"},{"location":"xmldocs/llama.types.embeddingdata/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.embeddingdata/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public EmbeddingData &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingdata/#returns_5","title":"Returns","text":"<p>EmbeddingData</p>"},{"location":"xmldocs/llama.types.embeddingdata/#deconstructint32-string-single","title":"Deconstruct(Int32&amp;, String&amp;, Single[]&amp;)","text":"<pre><code>public void Deconstruct(Int32&amp; Index, String&amp; Object, Single[]&amp; Embedding)\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingdata/#parameters_4","title":"Parameters","text":"<p><code>Index</code> Int32&amp;</p> <p><code>Object</code> String&amp;</p> <p><code>Embedding</code> Single[]&amp;</p>"},{"location":"xmldocs/llama.types.embeddingusage/","title":"EmbeddingUsage","text":"<p>Namespace: LLama.Types</p> <pre><code>public class EmbeddingUsage : System.IEquatable`1[[LLama.Types.EmbeddingUsage, LLamaSharp, Version=0.2.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 EmbeddingUsage Implements IEquatable&lt;EmbeddingUsage&gt;</p>"},{"location":"xmldocs/llama.types.embeddingusage/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.types.embeddingusage/#prompttokens","title":"PromptTokens","text":"<pre><code>public int PromptTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingusage/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.embeddingusage/#totaltokens","title":"TotalTokens","text":"<pre><code>public int TotalTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingusage/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.embeddingusage/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.types.embeddingusage/#embeddingusageint32-int32","title":"EmbeddingUsage(Int32, Int32)","text":"<pre><code>public EmbeddingUsage(int PromptTokens, int TotalTokens)\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingusage/#parameters","title":"Parameters","text":"<p><code>PromptTokens</code> Int32</p> <p><code>TotalTokens</code> Int32</p>"},{"location":"xmldocs/llama.types.embeddingusage/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.types.embeddingusage/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingusage/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.types.embeddingusage/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingusage/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.types.embeddingusage/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.embeddingusage/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingusage/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.types.embeddingusage/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingusage/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.types.embeddingusage/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.embeddingusage/#equalsembeddingusage","title":"Equals(EmbeddingUsage)","text":"<pre><code>public bool Equals(EmbeddingUsage other)\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingusage/#parameters_3","title":"Parameters","text":"<p><code>other</code> EmbeddingUsage</p>"},{"location":"xmldocs/llama.types.embeddingusage/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.types.embeddingusage/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public EmbeddingUsage &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingusage/#returns_5","title":"Returns","text":"<p>EmbeddingUsage</p>"},{"location":"xmldocs/llama.types.embeddingusage/#deconstructint32-int32","title":"Deconstruct(Int32&amp;, Int32&amp;)","text":"<pre><code>public void Deconstruct(Int32&amp; PromptTokens, Int32&amp; TotalTokens)\n</code></pre>"},{"location":"xmldocs/llama.types.embeddingusage/#parameters_4","title":"Parameters","text":"<p><code>PromptTokens</code> Int32&amp;</p> <p><code>TotalTokens</code> Int32&amp;</p>"},{"location":"xmldocs/logger/","title":"Logger","text":"<p>Namespace:</p> <pre><code>public sealed class Logger\n</code></pre> <p>Inheritance Object \u2192 Logger</p>"},{"location":"xmldocs/logger/#properties","title":"Properties","text":""},{"location":"xmldocs/logger/#default","title":"Default","text":"<pre><code>public static Logger Default { get; }\n</code></pre>"},{"location":"xmldocs/logger/#property-value","title":"Property Value","text":"<p>Logger</p>"},{"location":"xmldocs/logger/#methods","title":"Methods","text":""},{"location":"xmldocs/logger/#toconsole","title":"ToConsole()","text":"<pre><code>public void ToConsole()\n</code></pre>"},{"location":"xmldocs/logger/#tofilestring","title":"ToFile(String)","text":"<pre><code>public void ToFile(string filename)\n</code></pre>"},{"location":"xmldocs/logger/#parameters","title":"Parameters","text":"<p><code>filename</code> String</p>"},{"location":"xmldocs/logger/#infostring","title":"Info(String)","text":"<pre><code>public void Info(string message)\n</code></pre>"},{"location":"xmldocs/logger/#parameters_1","title":"Parameters","text":"<p><code>message</code> String</p>"},{"location":"xmldocs/logger/#warnstring","title":"Warn(String)","text":"<pre><code>public void Warn(string message)\n</code></pre>"},{"location":"xmldocs/logger/#parameters_2","title":"Parameters","text":"<p><code>message</code> String</p>"},{"location":"xmldocs/logger/#errorstring","title":"Error(String)","text":"<pre><code>public void Error(string message)\n</code></pre>"},{"location":"xmldocs/logger/#parameters_3","title":"Parameters","text":"<p><code>message</code> String</p>"}]}