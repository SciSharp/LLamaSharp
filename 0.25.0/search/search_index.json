{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>LLamaSharp is a cross-platform library to run \ud83e\udd99LLaMA/LLaVA model (and others) in local device. Based on llama.cpp, inference with LLamaSharp is efficient on both CPU and GPU. With the higher-level APIs and RAG support, it's convenient to deploy LLM (Large Language Model) in your application with LLamaSharp.</p>"},{"location":"#essential-insights-for-novice-learners","title":"Essential insights for novice learners","text":"<p>If you are new to LLM, here're some tips for you to help you to get start with <code>LLamaSharp</code>. If you are experienced in this field, we'd still recommend you to take a few minutes to read it because some things perform differently compared to cpp/python.</p> <ol> <li>The main ability of LLamaSharp is to provide an efficient way to run inference of LLM on your device (and fine-tune model in the future). The model weights, however, need to be downloaded from other resources such as huggingface.</li> <li>To gain high performance, LLamaSharp interacts with a native library compiled from c++, which is called <code>backend</code>. We provide backend packages for Windows, Linux and MAC with CPU, Cuda, Metal and OpenCL. You don't need to handle anything about c++ but just install the backend packages. If no published backend match your device, please open an issue to let us know. If compiling c++ code is not difficult for you, you could also follow this guide to compile a backend and run LLamaSharp with it.</li> <li><code>LLaMA</code> originally refers to the weights released by Meta (Facebook Research). After that, many models are fine-tuned based on it, such as <code>Vicuna</code>, <code>GPT4All</code>, and <code>Pyglion</code>. There are two popular file format of these model now, which are PyTorch format (.pth) and Huggingface format (.bin). LLamaSharp uses <code>GGUF</code> format file, which could be converted from these two formats. There are two options for you to get GGUF format file. a) Search model name + 'gguf' in Huggingface, you will find lots of model files that have already been converted to GGUF format. Please take care of the publishing time of them because some old ones could only work with old version of LLamaSharp. b) Convert PyTorch or Huggingface format to GGUF format yourself. Please follow the instructions of this part of llama.cpp readme to convert them with the python scripts.</li> <li>LLamaSharp supports multi-modal, which means that the model could take both text and image as input. Note that there are two model files required for using multi-modal (LLaVA), which are main model and mm-proj model. Here is a huggingface repo which shows that: link.</li> </ol>"},{"location":"#integrations","title":"Integrations","text":"<p>There are integrations for the following libraries, which help to expand the application of LLamaSharp. Integrations for semantic-kernel and kernel-memory are developed in LLamaSharp repository, while others are developed in their own repositories.</p> <ul> <li>semantic-kernel: an SDK that integrates LLM like OpenAI, Azure OpenAI, and Hugging Face.</li> <li>kernel-memory: a multi-modal AI Service specialized in the efficient indexing of datasets through custom continuous data hybrid pipelines, with support for RAG (Retrieval Augmented Generation), synthetic memory, prompt engineering, and custom semantic memory processing.</li> <li>BotSharp: an open source machine learning framework for AI Bot platform builder.</li> <li>Langchain: a framework for developing applications powered by language models.</li> </ul> <p></p>"},{"location":"#welcome-to-join-the-development","title":"Welcome to join the development!","text":"<p>Community effort is always one of the most important things in open-source projects. Any contribution in any way is welcomed here. For example, the following things mean a lot for LLamaSharp:</p> <ol> <li>Open an issue when you find something wrong.</li> <li>Open a PR if you've fixed something. Even if just correcting a typo, it also makes great sense.</li> <li>Help to optimize the documentation. </li> <li>Write an example or blog about how to integrate LLamaSharp with your APPs.</li> <li>Ask for a missing feature and discuss with us.</li> </ol> <p>If you'd like to get deeply involved in development, please touch us in discord channel or send email to <code>AsakusaRinne@gmail.com</code>. \ud83e\udd17</p>"},{"location":"Architecture/","title":"Architecture","text":""},{"location":"Architecture/#architecture-of-main-functions","title":"Architecture of main functions","text":"<p>The figure below shows the core framework structure of LLamaSharp.</p> <ul> <li>Native APIs: LLamaSharp calls the exported C APIs to load and run the model. The APIs defined in LLamaSharp specially for calling C APIs are named <code>Native APIs</code>. We have made all the native APIs public under namespace <code>LLama.Native</code>. However, it's strongly recommended not to use them unless you know what you are doing.</li> <li>LLamaWeights: The holder of the model weight.</li> <li>LLamaContext: A context which directly interacts with the native library and provides some basic APIs such as tokenization and embedding. It takes use of <code>LLamaWeights</code>.</li> <li>LLamaExecutors: Executors which define the way to run the LLama model. It provides text-to-text and image-to-text APIs to make it easy to use. Currently we provide four kinds of executors: <code>InteractiveExecutor</code>, <code>InstructExecutor</code>, <code>StatelessExecutor</code> and <code>BatchedExecutor</code>. </li> <li>ChatSession: A wrapping for <code>InteractiveExecutor</code> and <code>LLamaContext</code>, which supports interactive tasks and saving/re-loading sessions. It also provides a flexible way to customize the text process by <code>IHistoryTransform</code>, <code>ITextTransform</code> and <code>ITextStreamTransform</code>.</li> <li>Integrations: Integrations with other libraries to expand the application of LLamaSharp. For example, if you want to do RAG (Retrieval Augmented Generation), kernel-memory integration is a good option for you.</li> </ul> <p></p>"},{"location":"ContributingGuide/","title":"LLamaSharp Contributing Guide","text":"<p>Hi, welcome to develop LLamaSharp with us together! We are always open for every contributor and any format of contributions! If you want to maintain this library actively together, please contact us to get the write access after some PRs. (Email: AsakusaRinne@gmail.com)</p> <p>In this page, we introduce how to make contributions here easily. \ud83d\ude0a</p>"},{"location":"ContributingGuide/#the-goal-of-llamasharp","title":"The goal of LLamaSharp","text":"<p>At the beginning, LLamaSharp is a C# binding of llama.cpp. It provided only some wrappers for llama.cpp to let C#/.NET users could run LLM models on their local device efficiently even if without any experience with C++. After around a year of development, more tools and integrations has been added to LLamaSharp, significantly expanding the application of LLamaSharp. Though llama.cpp is still the only backend of LLamaSharp, the goal of this repository is more likely to be an efficient and easy-to-use library of LLM inference, rather than just a binding of llama.cpp.</p> <p>In this way, our development of LLamaSharp is divided into two main directions:</p> <ol> <li>To make LLamaSharp more efficient. For example, <code>BatchedExecutor</code> could accept multiple queries and generate the response for them at the same time, which significantly improves the throughput. This part is always related with native APIs and executors in LLamaSharp.</li> <li>To make it easier to use LLamaSharp. We believe the best library is to let users build powerful functionalities with simple code. Higher-level APIs and integrations with other libraries are the key points of it.</li> </ol>"},{"location":"ContributingGuide/#how-to-compile-the-native-library-from-source","title":"How to compile the native library from source","text":"<p>If you want to contribute to the first direction of our goal, you may need to compile the native library yourself.</p> <p>Firstly, please follow the instructions in llama.cpp readme to configure your local environment. Most importantly, CMake with version higher than 3.14 should be installed on your device.</p> <p>Secondly, clone the llama.cpp repositories. You could manually clone it and checkout to the right commit according to Map of LLamaSharp and llama.cpp versions, or use clone the submodule of LLamaSharp when cloning LLamaSharp.</p> <pre><code>git clone --recursive https://github.com/SciSharp/LLamaSharp.git\n</code></pre> <p>If you want to support cublas in the compilation, please make sure that you've installed it. If you are using Intel CPU, please check the highest AVX (Advanced Vector Extensions) level that is supported by your device.</p> <p>As shown in llama.cpp cmake file, which also includes llama.cpp/ggml cmake file, there are many options that could be enabled or disabled when building the library. The following ones are commonly used when using it as a native library of LLamaSharp.</p> <pre><code>option(BUILD_SHARED_LIBS                \"build shared libraries\") // Please always enable it \noption(LLAMA_BUILD_TESTS                \"llama: build tests\") // Please disable it.\noption(LLAMA_BUILD_EXAMPLES             \"llama: build examples\") // Please disable it.\noption(LLAMA_BUILD_SERVER               \"llama: build server example\")// Please disable it.\n\noption(GGML_NATIVE                      \"llama: enable -march=native flag\") // Could be disabled\noption(GGML_AVX                         \"ggml: enable AVX\") // Enable it if the highest supported avx level is AVX\noption(GGML_AVX2                        \"ggml: enable AVX2\") // Enable it if the highest supported avx level is AVX2\noption(GGML_AVX512                      \"ggml: enable AVX512\") // Enable it if the highest supported avx level is AVX512\noption(GGML_CUDA                        \"ggml: use CUDA\") // Enable it if you have CUDA device\noption(GGML_BLAS                        \"ggml: use BLAS\") // Enable it if you want to use BLAS library to accelerate the computation on CPU\noption(GGML_VULKAN                      \"ggml: use Vulkan\") // Enable it if you have a device with Vulkan support\noption(GGML_METAL                       \"ggml: use Metal\") // Enable it if you are using a MAC with Metal device.\n</code></pre> <p>Most importantly, <code>-DBUILD_SHARED_LIBS=ON</code> must be added to the cmake instruction and other options depends on you. For example, when building with CUDA, use the following instruction:</p> <pre><code>mkdir build &amp;&amp; cd build\ncmake .. -DGGML_CUDA=ON -DBUILD_SHARED_LIBS=ON\ncmake --build . --config Release\n</code></pre> <p>Now you could find the <code>llama.dll</code>, <code>libllama.so</code> or <code>llama.dylib</code> in <code>build/src</code>. </p> <p>To load the compiled native library, please add the following code to the very beginning of your code.</p> <pre><code>NativeLibraryConfig.Instance.WithLibrary(\"&lt;Your native library path&gt;\");\n</code></pre>"},{"location":"ContributingGuide/#add-a-new-feature-to-llamasharp","title":"Add a new feature to LLamaSharp","text":"<p>After refactoring the framework in <code>v0.4.0</code>, LLamaSharp will try to maintain the backward compatibility. However, in the following cases a breaking change will be required:</p> <ol> <li>Due to some break changes in llama.cpp, making a breaking change will help to maintain the good abstraction and friendly user APIs.</li> <li>An important feature cannot be implemented unless refactoring some parts.</li> <li>After some discussions, an agreement was reached that making the break change is reasonable.</li> </ol> <p>If a new feature could be added without introducing any break change, please open a PR rather than open an issue first. We will never refuse the PR but help to improve it, unless it's malicious.</p> <p>When adding the feature, please take care of the namespace and the naming convention. For example, if you are adding an integration for WPF, please put the code under namespace <code>LLama.WPF</code> or <code>LLama.Integration.WPF</code> instead of putting it under the root namespace. The naming convention of LLamaSharp follows the pascal naming convention, but in some parts that are invisible to users, you can do whatever you want.</p>"},{"location":"ContributingGuide/#find-the-problem-and-fix-the-bug","title":"Find the problem and fix the BUG","text":"<p>If the issue is related to the LLM internal behaviour, such as endless generating the response, the best way to find the problem is to do comparison test between llama.cpp and LLamaSharp.</p> <p>You could use exactly the same prompt, the same model and the same parameters to run the inference in llama.cpp and LLamaSharp respectively to see if it's really a problem caused by the implementation in LLamaSharp.</p> <p>If the experiment showed that it worked well in llama.cpp but didn't in LLamaSharp, a search for the problem could be started. While the reason of the problem could be various, the best way I think is to add log-print in the code of llama.cpp and use it in LLamaSharp after compilation. Thus, when running LLamaSharp, you could see what happened in the native library.</p> <p>During the BUG fix process, please don't hesitate to discuss together when you are blocked.</p>"},{"location":"ContributingGuide/#add-integrations","title":"Add integrations","text":"<p>All kinds of integration are welcomed here! Currently the following integrations have been added but still need improvement:</p> <ol> <li>semantic-kernel</li> <li>kernel-memory</li> <li>BotSharp (maintained in SciSharp/BotSharp repo)</li> <li>Langchain (maintained in tryAGI/LangChain repo)</li> </ol> <p>If you find another library that is good to be integrated, please open an issue to let us know!</p>"},{"location":"ContributingGuide/#add-examples","title":"Add examples","text":"<p>There're mainly two ways to add an example:</p> <ol> <li>Add the example to <code>LLama.Examples</code> of the repository.</li> <li>Put the example in another repository and add the link to the readme or docs of LLamaSharp.</li> </ol>"},{"location":"ContributingGuide/#add-documents","title":"Add documents","text":"<p>LLamaSharp uses mkdocs to build the documentation, please follow the tutorial of mkdocs to add or modify documents in LLamaSharp.</p> <p>For API references, LLamaSharp takes use of xmldoc2md to generate the documentation of markdown format. If you want to update the files in <code>docs/xmldocs</code>, please run the following commands.</p> <pre><code>dotnet tool install -g XMLDoc2Markdown\ncd LLama/bin/Debug/net8 # change the path to your bin path.\ndotnet xmldoc2md LLamaSharp.dll -o ../../../../docs/xmldocs --back-button\n</code></pre> <p>Specifically, if the xmldoc2md cannot be found by dotnet cli, please replace the <code>dotnet xmldoc2md</code> with the executable file, like below.</p> <pre><code>C:\\Users\\liu_y\\.dotnet\\tools\\xmldoc2md.exe LLamaSharp.dll  -o ../../../../docs/xmldocs --back-button\n</code></pre>"},{"location":"FAQ/","title":"Frequently asked questions","text":"<p>Sometimes, your application with LLM and LLamaSharp may have unexpected behaviours. Here are some frequently asked questions, which may help you to deal with your problem.</p>"},{"location":"FAQ/#why-gpu-is-not-used-when-i-have-installed-cuda","title":"Why GPU is not used when I have installed CUDA","text":"<ol> <li>If you are using backend packages, please make sure you have installed the cuda backend package which matches the cuda version of your device. Please note that before LLamaSharp v0.10.0, only one backend package should be installed.</li> <li>Add <code>NativeLibraryConfig.Instance.WithLogs(LLamaLogLevel.Info)</code> to the very beginning of your code. The log will show which native library file is loaded. If the CPU library is loaded, please try to compile the native library yourself and open an issue for that. If the CUDA library is loaded, please check if <code>GpuLayerCount &gt; 0</code> when loading the model weight.</li> </ol>"},{"location":"FAQ/#why-the-inference-is-slow","title":"Why the inference is slow","text":"<p>Firstly, due to the large size of LLM models, it requires more time to generate outputs than other models, especially when you are using models larger than 30B.</p> <p>To see if that's a LLamaSharp performance issue, please follow the two tips below.</p> <ol> <li>If you are using CUDA, Metal or OpenCL, please set <code>GpuLayerCount</code> as large as possible.</li> <li>If it's still slower than you expect it to be, please try to run the same model with same setting in llama.cpp examples. If llama.cpp outperforms LLamaSharp significantly, it's likely a LLamaSharp BUG and please report us for that.</li> </ol>"},{"location":"FAQ/#why-the-program-crashes-before-any-output-is-generated","title":"Why the program crashes before any output is generated","text":"<p>Generally, there are two possible cases for this problem:</p> <ol> <li>The native library (backend) you are using is not compatible with the LLamaSharp version. If you compiled the native library yourself, please make sure you have checkouted llama.cpp to the corresponding commit of LLamaSharp, which could be found at the bottom of README.</li> <li>The model file you are using is not compatible with the backend. If you are using a GGUF file downloaded from huggingface, please check its publishing time.</li> </ol>"},{"location":"FAQ/#why-my-model-is-generating-output-infinitely","title":"Why my model is generating output infinitely","text":"<p>Please set anti-prompt or max-length when executing the inference.</p> <p>Anti-prompt can also be called as \"Stop-keyword\", which decides when to stop the response generation. Under interactive mode, the maximum tokens count is always not set, which makes the LLM generate responses infinitively. Therefore, setting anti-prompt correctly helps a lot to avoid the strange behaviours. For example, the prompt file <code>chat-with-bob.txt</code> has the following content:</p> <pre><code>Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\n\nUser: Hello, Bob.\nBob: Hello. How may I help you today?\nUser: Please tell me the largest city in Europe.\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\nUser:\n</code></pre> <p>Therefore, the anti-prompt should be set as \"User:\". If the last line of the prompt is removed, LLM will automatically generate a question (user) and a response (bob) for one time when running the chat session. Therefore, the antiprompt is suggested to be appended to the prompt when starting a chat session.</p> <p>What if an extra line is appended? The string \"User:\" in the prompt will be followed with a char \"\\n\". Thus when running the model, the automatic generation of a pair of question and response may appear because the anti-prompt is \"User:\" but the last token is \"User:\\n\". As for whether it will appear, it's an undefined behaviour, which depends on the implementation inside the <code>LLamaExecutor</code>. Anyway, since it may lead to unexpected behaviors, it's recommended to trim your prompt or carefully keep consistent with your anti-prompt.</p>"},{"location":"FAQ/#how-to-run-llm-with-non-english-languages","title":"How to run LLM with non-English languages","text":"<p>English is the most popular language in the world, and in the region of LLM. If you want to accept inputs and generate outputs of other languages, please follow the two tips below.</p> <ol> <li>Ensure the model you selected is well-trained with data of your language. For example, LLaMA (original) used few Chinese text during the pretrain, while Chinese-LLaMA-Alpaca finetuned LLaMA with a large amount of Chinese text data. Therefore, the quality of the output of Chinese-LLaMA-Alpaca is much better than that of LLaMA.</li> </ol>"},{"location":"FAQ/#pay-attention-to-the-length-of-prompt","title":"Pay attention to the length of prompt","text":"<p>Sometimes we want to input a long prompt to execute a task. However, the context size may limit the inference of LLama model. Please ensure the inequality below holds.</p> \\[ len(prompt) + len(response) &lt; len(context) \\] <p>In this inequality, <code>len(response)</code> refers to the expected tokens for LLM to generate.</p>"},{"location":"FAQ/#choose-models-weight-depending-on-your-task","title":"Choose models weight depending on your task","text":"<p>The differences between modes may lead to much different behaviours under the same task. For example, if you're building a chat bot with non-English, a fine-tuned model specially for the language you want to use will have huge effect on the performance.</p>"},{"location":"QuickStart/","title":"Quick start","text":""},{"location":"QuickStart/#installation","title":"Installation","text":"<p>To gain high performance, LLamaSharp interacts with a native library compiled from c++, which is called <code>backend</code>. We provide backend packages for Windows, Linux and MAC with CPU, Cuda, Metal and OpenCL. You don't need to handle anything about c++ but just install the backend packages.</p> <p>If no published backend match your device, please open an issue to let us know. If compiling c++ code is not difficult for you, you could also follow this guide to compile a backend and run LLamaSharp with it.</p> <ol> <li>Install LLamaSharp package on NuGet:</li> </ol> <pre><code>PM&gt; Install-Package LLamaSharp\n</code></pre> <ol> <li> <p>Install one or more of these backends, or use self-compiled backend.</p> </li> <li> <p><code>LLamaSharp.Backend.Cpu</code>: Pure CPU for Windows &amp; Linux &amp; MAC. Metal (GPU) support for MAC.</p> </li> <li><code>LLamaSharp.Backend.Cuda11</code>: CUDA11 for Windows &amp; Linux.</li> <li><code>LLamaSharp.Backend.Cuda12</code>: CUDA 12 for Windows &amp; Linux.</li> <li> <p><code>LLamaSharp.Backend.OpenCL</code>: OpenCL for Windows &amp; Linux.</p> </li> <li> <p>(optional) For Microsoft semantic-kernel integration, install the LLamaSharp.semantic-kernel package.</p> </li> <li>(optional) To enable RAG support, install the LLamaSharp.kernel-memory package (this package only supports <code>net6.0</code> or higher yet), which is based on Microsoft kernel-memory integration.</li> </ol>"},{"location":"QuickStart/#model-preparation","title":"Model preparation","text":"<p>There are two popular formats of model file of LLM now, which are PyTorch format (.pth) and Huggingface format (.bin). LLamaSharp uses <code>GGUF</code> format file, which could be converted from these two formats. To get <code>GGUF</code> file, there are two options:</p> <ol> <li> <p>Search model name + 'gguf' in Huggingface, you will find lots of model files that have already been converted to GGUF format. Please take care of the publishing time of them because some old ones could only work with old version of LLamaSharp.</p> </li> <li> <p>Convert PyTorch or Huggingface format to GGUF format yourself. Please follow the instructions of this part of llama.cpp readme to convert them with the python scripts.</p> </li> </ol> <p>Generally, we recommend downloading models with quantization rather than fp16, because it significantly reduce the required memory size while only slightly impact on its generation quality.</p>"},{"location":"QuickStart/#example-of-llama-chat-session","title":"Example of LLaMA chat session","text":"<p>Here is a simple example to chat with bot based on LLM in LLamaSharp. Please replace the model path with yours.</p> <p></p> <pre><code>using LLama.Common;\nusing LLama;\n\nstring modelPath = @\"&lt;Your Model Path&gt;\"; // change it to your own model path.\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024, // The longest length of chat as memory.\nGpuLayerCount = 5 // How many layers to offload to GPU. Please adjust it according to your GPU memory.\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\n// Add chat histories as prompt to tell AI how to act.\nvar chatHistory = new ChatHistory();\nchatHistory.AddMessage(AuthorRole.System, \"Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\");\nchatHistory.AddMessage(AuthorRole.User, \"Hello, Bob.\");\nchatHistory.AddMessage(AuthorRole.Assistant, \"Hello. How may I help you today?\");\n\nChatSession session = new(executor, chatHistory);\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nMaxTokens = 256, // No more than 256 tokens should appear in answer. Remove it if antiprompt is enough for control.\nAntiPrompts = new List&lt;string&gt; { \"User:\" } // Stop generation once antiprompts appear.\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.Write(\"The chat session has started.\\nUser: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\nawait foreach ( // Generate the response streamingly.\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n}\n</code></pre>"},{"location":"QuickStart/#examples-of-chatting-with-llava","title":"Examples of chatting with LLaVA","text":"<p>This example shows chatting with LLaVA to ask it to describe the picture. </p> <pre><code>using System.Text.RegularExpressions;\nusing LLama;\nusing LLama.Common;\n\nstring multiModalProj = @\"&lt;Your multi-modal proj file path&gt;\";\nstring modelPath = @\"&lt;Your LLaVA model file path&gt;\";\nstring modelImage = @\"&lt;Your image path&gt;\";\nconst int maxTokens = 1024; // The max tokens that could be generated.\n\nvar prompt = $\"{{{modelImage}}}\\nUSER:\\nProvide a full description of the image.\\nASSISTANT:\\n\";\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 4096,\nSeed = 1337,\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\n\n// Llava Init\nusing var clipModel = LLavaWeights.LoadFromFile(multiModalProj);\n\nvar ex = new InteractiveExecutor(context, clipModel);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the prompt is printed, the maximum tokens is set to {0} and the context size is {1}.\", maxTokens, parameters.ContextSize);\nConsole.WriteLine(\"To send an image, enter its filename in curly braces, like this {c:/image.jpg}.\");\n\nvar inferenceParams = new InferenceParams() { Temperature = 0.1f, AntiPrompts = new List&lt;string&gt; { \"\\nUSER:\" }, MaxTokens = maxTokens };\n\ndo\n{\n\n// Evaluate if we have images\n//\nvar imageMatches = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Value);\nvar imageCount = imageMatches.Count();\nvar hasImages = imageCount &gt; 0;\nbyte[][] imageBytes = null;\n\nif (hasImages)\n{\nvar imagePathsWithCurlyBraces = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Value);\nvar imagePaths = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Groups[1].Value);\n\ntry\n{\nimageBytes = imagePaths.Select(File.ReadAllBytes).ToArray();\n}\ncatch (IOException exception)\n{\nConsole.ForegroundColor = ConsoleColor.Red;\nConsole.Write(\n$\"Could not load your {(imageCount == 1 ? \"image\" : \"images\")}:\");\nConsole.Write($\"{exception.Message}\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Please try again.\");\nbreak;\n}\n\n\nint index = 0;\nforeach (var path in imagePathsWithCurlyBraces)\n{\n// First image replace to tag &lt;image, the rest of the images delete the tag\nif (index++ == 0)\nprompt = prompt.Replace(path, \"&lt;image&gt;\");\nelse\nprompt = prompt.Replace(path, \"\");\n}\nConsole.WriteLine();\n\n\n// Initialize Images in executor\n//\nex.ImagePaths = imagePaths.ToList();\n}\n\nConsole.ForegroundColor = ConsoleColor.White;\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\nConsole.Write(\" \");\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.WriteLine();\n\n// let the user finish with exit\n//\nif (prompt.Equals(\"/exit\", StringComparison.OrdinalIgnoreCase))\nbreak;\n\n}\nwhile (true);\n</code></pre> <p>For more examples, please refer to LLamaSharp.Examples.</p>"},{"location":"AdvancedTutorials/CustomizeNativeLibraryLoading/","title":"Customize the native library loading process","text":"<p>In the tutorial of native library config, we introduces how to auto-select a best native library as backend with your configurations. In this tutorial, we're taking a step further, to customize the native library loading process in a much more flexible way.</p>"},{"location":"AdvancedTutorials/CustomizeNativeLibraryLoading/#customize-the-policy-of-the-selection","title":"Customize the policy of the selection","text":"<p><code>INativeLibrarySelectingPolicy</code> is responsible for selecting the native libraries to load and sort them in order of priority. You could implement this interface and register your implementation by calling <code>NativeLibraryConfig.WithSelectingPolicy</code>.</p> <p>Note that when you're using your own implementation, the native libraries returned from <code>INativeLibrarySelectingPolicy.Apply</code> are all the libraries that are possibly used. It might spend lots of time of you to complete a robust implementation, but you can always fallback to the default behavior by calling <code>DefaultNativeLibrarySelectingPolicy</code> when no suitable library is found.</p> <pre><code>public interface INativeLibrarySelectingPolicy\n{\n/// &lt;summary&gt;\n/// Select the native library.\n/// &lt;/summary&gt;\n/// &lt;param name=\"description\"&gt;The description of the user's configuration.&lt;/param&gt;\n/// &lt;param name=\"systemInfo\"&gt;The system information of the current machine.&lt;/param&gt;\n/// &lt;param name=\"logCallback\"&gt;The log callback.&lt;/param&gt;\n/// &lt;returns&gt;The information of the selected native library files, in order by priority from the beginning to the end.&lt;/returns&gt;\nIEnumerable&lt;INativeLibrary&gt; Apply(NativeLibraryConfig.Description description, SystemInfo systemInfo, NativeLogConfig.LLamaLogCallback? logCallback = null);\n}\n</code></pre>"},{"location":"AdvancedTutorials/CustomizeNativeLibraryLoading/#customize-the-native-library","title":"Customize the native library","text":"<p>The native library in LLamaSharp is not just a file path. Instead, it's abstracted to <code>INativeLibrary</code>, which contains the information of it and allows finer-grained control of the loading process.</p> <pre><code>public interface INativeLibrary\n{\n/// &lt;summary&gt;\n/// Metadata of this library.\n/// &lt;/summary&gt;\nNativeLibraryMetadata? Metadata { get; }\n\n/// &lt;summary&gt;\n/// Prepare the native library file and returns the local path of it.\n/// If it's a relative path, LLamaSharp will search the path in the search directies you set.\n/// &lt;/summary&gt;\n/// &lt;param name=\"systemInfo\"&gt;The system information of the current machine.&lt;/param&gt;\n/// &lt;param name=\"logCallback\"&gt;The log callback.&lt;/param&gt;\n/// &lt;returns&gt;\n/// The relative paths of the library. You could return multiple paths to try them one by one. If no file is available, please return an empty array.\n/// &lt;/returns&gt;\nIEnumerable&lt;string&gt; Prepare(SystemInfo systemInfo, NativeLogConfig.LLamaLogCallback? logCallback = null);\n}\n</code></pre> <p><code>INativeLibrary.Metadata</code> contains the information of the native library. When you implement the interface, you can ignore it and return <code>null</code>. However, the best practice will always include implemenbting this peoperty because it will provide much more information when there's an anexpected behavior.</p> <p><code>INativeLibrary.Prepare</code> is a method which allows you to do some preparations before loading the library. For example, you can move the file, download the file, output some logs, etc. It is supposed to return a list of file paths in order of priority. </p> <p>There are several implementations inside LLamaSharp, which are <code>NativeLibraryFromPath</code>, <code>NativeLibraryWithAvx</code>, <code>NativeLibraryWithCuda</code> and <code>NativeLibraryWithMacOrFallback</code>. Please refer to their implementations if you find it difficult to implement <code>INativeLibrary</code>.</p>"},{"location":"AdvancedTutorials/CustomizeNativeLibraryLoading/#how-does-the-loading-work","title":"How does the loading work","text":"<p>Here're the steps for the native library loading in LLamaSharp.</p> <ol> <li>Gather and validate the user's configuration for native library loading. (<code>NativeLibraryConfig</code>)</li> <li>Gather the system information, mainly including platform and device.</li> <li>Apply the selecting policy to get the list of native libraries to load.</li> <li>For each of the selected native library, call the <code>INativeLibrary.Prepare</code> method to get the file paths. Then Try to load the library from the paths in turn. If the library is loaded successfully with any path of them, the loop will be broken.</li> </ol>"},{"location":"AdvancedTutorials/CustomizeNativeLibraryLoading/#example-use-remote-library-files","title":"Example: use remote library files","text":"<p>Though the native library downloading feature will be introduced as discussed in https://github.com/SciSharp/LLamaSharp/issues/670, it might have some security concerns. Here's an example to implement the remote native library downloading.</p> <p>Firstly, the native library with downloading process should be implemented.</p> <pre><code>public class AutoDownloadedLibraries\n{\n// Wrap a cuda native library\npublic class Cuda: INativeLibrary\n{\n// the default cuda native library implementation in LLamaSHarp\nprivate NativeLibraryWithCuda _cudaLibrary;\n// Some download settings\nprivate NativeLibraryDownloadSettings _settings;\n\npublic Cuda(NativeLibraryWithCuda cudaLibrary, NativeLibraryDownloadSettings settings)\n{\n_cudaLibrary = cudaLibrary;\n_settings = settings;\n}\n\npublic NativeLibraryMetadata? Metadata =&gt; _cudaLibrary.Metadata;\n\npublic IEnumerable&lt;string&gt; Prepare(SystemInfo systemInfo, NativeLogConfig.LLamaLogCallback? logCallback = null)\n{\nforeach(var relativePath in _cudaLibrary.Prepare(systemInfo, logCallback))\n{\n// try to use the default path first. If loaded successfully, the download will not be triggered.\nyield return relativePath;\n// download the file.\n// NOTE: be sure to complete the downloading process here. You CANNOT make `Prepare` as an async method.\nvar path = NativeLibraryDownloader.DownloadLibraryFile(_settings, relativePath, logCallback).Result;\n// if the downloading is successful, return the path of the downloaded file.\nif (path is not null)\n{\nyield return path;\n}\n}\n}\n}\n}\n</code></pre> <p>Then, implement the selecting policy for the native libraries above</p> <pre><code>public class SelectingPolicyWithAutoDownload: INativeLibrarySelectingPolicy\n{\n// making this class a wrapper for the default policy.\nprivate DefaultNativeLibrarySelectingPolicy _defaultPolicy = new();\n// record the download settings.\nprivate NativeLibraryDownloadSettings _downloadSettings;\n\ninternal SelectingPolicyWithAutoDownload(NativeLibraryDownloadSettings downloadSettings)\n{\n_downloadSettings = downloadSettings;\n}\n\npublic IEnumerable&lt;INativeLibrary&gt; Apply(NativeLibraryConfig.Description description, SystemInfo systemInfo, NativeLogConfig.LLamaLogCallback? logCallback)\n{\nforeach(var library in _defaultPolicy.Apply(description, systemInfo, logCallback))\n{\n// check the return type and returns the corresponding wrapper\nif(library is NativeLibraryWithCuda cudaLibrary)\n{\nyield return new AutoDownloadedLibraries.Cuda(cudaLibrary, _downloadSettings);\n}\nelse if(library is NativeLibraryWithAvx avxLibrary)\n{\nyield return new AutoDownloadedLibraries.Avx(avxLibrary, _downloadSettings);\n}\nelse if(library is NativeLibraryWithMacOrFallback macLibrary)\n{\nyield return new AutoDownloadedLibraries.MacOrFallback(macLibrary, _downloadSettings);\n}\n// Generally, you don't need to download the DLL if the user specify a path.\n// But if you want, you can certainly add a wrapper for it.\nelse if(library is NativeLibraryFromPath)\n{\nyield return library;\n}\n// It's also reasonable to throw an exception here.\nelse\n{\nyield return library;\n}\n}\n}\n}\n</code></pre> <p>Finally, add an extension to make an API for users to enable this feature.</p> <pre><code> public static class NativeLibraryAutoDownloadExtension\n{\npublic static NativeLibraryConfig WithAutoDownload(this NativeLibraryConfig config, bool enable = true, NativeLibraryDownloadSettings? settings = null)\n{\nif (config.LibraryHasLoaded)\n{\nthrow new Exception(\"The library has already loaded, you can't change the configurations. \" +\n\"Please finish the configuration setting before any call to LLamaSharp native APIs.\" +\n\"Please use NativeLibraryConfig.DryRun if you want to see whether it's loaded successfully \" +\n\"but still have chance to modify the configurations.\");\n}\nif (enable)\n{\nif(settings is null)\n{\nsettings = NativeLibraryDownloadSettings.Create();\n}\n// If you want to return an relative path in `INativeLibrary.Prepare`, \n// be sure to add the downloading directory to the search directories.\nvar defaultLocalDir = NativeLibraryDownloadSettings.GetDefaultLocalDir(settings);\n\n// When using auto-download, this should be the only search this directory.\nList&lt;string&gt; searchDirectoriesForDownload = [settings.LocalDir!];\n// unless extra search paths are added by the user.\nsearchDirectoriesForDownload.AddRange(settings.ExtraSearchDirectories ?? []);\nconfig.WithSearchDirectories(searchDirectoriesForDownload);\n\n// register you selecting policy.\nconfig.WithSelectingPolicy(new SelectingPolicyWithAutoDownload(settings));\n}\nreturn config;\n}\n}\n</code></pre> <p>Now your users are now able to enable this feature by calling <code>NativeLibraryConfig.All.WithAutoDownload</code>!</p>"},{"location":"Examples/BatchedExecutorFork/","title":"BatchedExecutor Fork - Generate Multiple Completions With Shared Memory","text":"<p>This example demonstrates using the <code>BatchedExecutor</code> to split one sequence into multiple sequences. See the source code here.</p> <p>Sequences share memory up to the point they were split, meaning no extra memory is consumed by creating a fork. Inference runs for all sequences simultaneously, this means that running two sequences does not take twice as much time as running one.</p> <p>An example output, starting with the prompt <code>Not many people know that</code>:</p> <pre><code>Not many people know that\n\u2514\u2500\u2500 , in the 17th century, a military band led by Captain Charles\n    \u251c\u2500\u2500  Bossler of Baden, Germany, composed and played a music suite titled\n    \u2502   \u251c\u2500\u2500  the \"Civil Psalm,\" in order to rally German Protestants during\n    \u2502   \u2502   \u251c\u2500\u2500  the Thirty Years' War.  This tune became popular among German soldiers,\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500  and its popularity continued long after the war\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500  and, eventually, reached France. The\n    \u2502   \u2502   \u2514\u2500\u2500  the Thirty Years' War.This music, with its clear call\n    \u2502   \u2502       \u251c\u2500\u2500  to arms and strong Christian themes, helped\n    \u2502   \u2502       \u2514\u2500\u2500  to arms and unwavering belief\n    \u2502   \u2514\u2500\u2500  \"Baden's First National Symphony,\" with lyrics by a young Wol\n    \u2502       \u251c\u2500\u2500 fgang Amadeus Mozart. The story of the composition's creation\n    \u2502       \u2502   \u251c\u2500\u2500  has long been forgotten. But the B\n    \u2502       \u2502   \u2514\u2500\u2500  was popularized by a novelty book\n    \u2502       \u2514\u2500\u2500 fgang Amadeus Mozart. It's said that this music brought\n    \u2502           \u251c\u2500\u2500  peace to Europe, at least for a\n    \u2502           \u2514\u2500\u2500  the troops together during difficult times. It\n    \u2514\u2500\u2500  Newdick played a mournful dirge to accompany the procession of\n        \u251c\u2500\u2500  the head of King Charles I. It is the scene that opens my latest book\n        \u2502   \u251c\u2500\u2500 , \"Death and Taxes.\" The book follows a British army captain named\n        \u2502   \u2502   \u251c\u2500\u2500  Marcus as he seeks revenge for his wife\n        \u2502   \u2502   \u2514\u2500\u2500  William Darnay who becomes involved in\n        \u2502   \u2514\u2500\u2500 , A King, A Pawn and a Prince. The murder of the king\n        \u2502       \u251c\u2500\u2500  and the civil war that followed are the\n        \u2502       \u2514\u2500\u2500  is a watershed moment in the political\n        \u2514\u2500\u2500  the coffin of William Shakespeare, as it was carried to its final resting place\n            \u251c\u2500\u2500 . That is the least that can be said for a man who is often regarded\n            \u2502   \u251c\u2500\u2500  as the greatest writer in the English language\n            \u2502   \u2514\u2500\u2500  as the greatest writer the English language has\n            \u2514\u2500\u2500  at Stratford-upon-Avon.  Shakespeare, of course\n                \u251c\u2500\u2500 , was a famous English poet and play\n                \u2514\u2500\u2500 , was one of the greatest playwright\n</code></pre> <p>Forked sequences can be used for many possible things. For example  - Evaluating the system prompt once and forking for each independent conversation.  - Saving a \"checkpoint\" in a conversation to return to later.  - Beam Search.  - Splitting a conversation, generating completions from several different \"agents\", and taking the best response.</p>"},{"location":"Examples/BatchedExecutorGuidance/","title":"BatchedExecutor Guidance - Classifier Free Guidance / Negative Prompting","text":"<p>This example demonstrates using <code>Classifier Free Guidance</code> (a.k.a. negative prompting) with a custom sampling pipeline. Negative prompting is a way of steering the model output away from certain topics. See the source code here.</p> <p>Two conversations are created. The <code>guided</code> conversation starts with the prompt that should be completed as shown as the output, for example <code>\"my favourite colour is\"</code>. The <code>guidance</code> conversation contains the negative prompt at the start, for example <code>\"I hate the colour red. My favourite colour is\"</code>. Note that this is a negative prompt, so therefore this guidance will make the model answer as if it likes the colour red.</p> <p>A custom sampler samples the <code>guidance</code> conversation and uses that output to influence the output of the <code>guided</code> conversation. Once a token is selected both conversations are continued with this token.</p>"},{"location":"Examples/BatchedExecutorRewind/","title":"BatchedExecutor - Rewind","text":"<p>This example demonstrates using the <code>BatchedExecutor</code> to split one sequence into multiple sequences. See the source code here.</p> <p>A single conversation is prompted and then continued for 24 tokens, after that it is re-wound by 12 tokens and continued from there. Rewinding simply sets the conversation back to an earlier state and requires no extra computation.</p>"},{"location":"Examples/ChatChineseGB2312/","title":"Chinese LLM - with GB2312 encoding","text":"<pre><code>using System.Text;\nusing LLama.Common;\n\nnamespace LLama.Examples.Examples;\n\n// This example shows how to deal with Chinese input with gb2312 encoding.\npublic class ChatChineseGB2312\n{\nprivate static string ConvertEncoding(string input, Encoding original, Encoding target)\n{\nbyte[] bytes = original.GetBytes(input);\nvar convertedBytes = Encoding.Convert(original, target, bytes);\nreturn target.GetString(convertedBytes);\n}\n\npublic static async Task Run()\n{\n// Register provider for GB2312 encoding\nEncoding.RegisterProvider(CodePagesEncodingProvider.Instance);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"This example shows how to use Chinese with gb2312 encoding, which is common in windows. It's recommended\" +\n\" to use https://huggingface.co/hfl/chinese-alpaca-2-7b-gguf/blob/main/ggml-model-q5_0.gguf, which has been verified by LLamaSharp developers.\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5,\nEncoding = Encoding.UTF8\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\nChatSession session;\nif (Directory.Exists(\"Assets/chat-with-kunkun-chinese\"))\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Loading session from disk.\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nsession = new ChatSession(executor);\nsession.LoadSession(\"Assets/chat-with-kunkun-chinese\");\n}\nelse\n{\nvar chatHistoryJson = File.ReadAllText(\"Assets/chat-with-kunkun-chinese.json\");\nChatHistory chatHistory = ChatHistory.FromJson(chatHistoryJson) ?? new ChatHistory();\n\nsession = new ChatSession(executor, chatHistory);\n}\n\nsession\n.WithHistoryTransform(new LLamaTransforms.DefaultHistoryTransform(\"\u7528\u6237\", \"\u5764\u5764\"));\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"\u7528\u6237\uff1a\" }\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started.\");\n\n// show the prompt\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(\"\u7528\u6237\uff1a\");\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\n// Convert the encoding from gb2312 to utf8 for the language model\n// and later saving to the history json file.\nuserInput = ConvertEncoding(userInput, Encoding.GetEncoding(\"gb2312\"), Encoding.UTF8);\n\nif (userInput == \"save\")\n{\nsession.SaveSession(\"Assets/chat-with-kunkun-chinese\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Session saved.\");\n}\nelse if (userInput == \"regenerate\")\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Regenerating last response ...\");\n\nawait foreach (\nvar text\nin session.RegenerateAssistantMessageAsync(\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\n\n// Convert the encoding from utf8 to gb2312 for the console output.\nConsole.Write(ConvertEncoding(text, Encoding.UTF8, Encoding.GetEncoding(\"gb2312\")));\n}\n}\nelse\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n</code></pre>"},{"location":"Examples/ChatSessionStripRoleName/","title":"ChatSession - stripping role names","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples;\n\n// When using chatsession, it's a common case that you want to strip the role names\n// rather than display them. This example shows how to use transforms to strip them.\npublic class ChatSessionStripRoleName\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\nvar chatHistoryJson = File.ReadAllText(\"Assets/chat-with-bob.json\");\nChatHistory chatHistory = ChatHistory.FromJson(chatHistoryJson) ?? new ChatHistory();\n\nChatSession session = new(executor, chatHistory);\nsession.WithOutputTransform(new LLamaTransforms.KeywordTextOutputStreamTransform(\nnew string[] { \"User:\", \"Assistant:\" },\nredundancyLength: 8));\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"User:\" }\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started.\");\n\n// show the prompt\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n</code></pre>"},{"location":"Examples/ChatSessionWithHistory/","title":"ChatSession - with history","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples;\n\n// This example shows how to save the state and history of chat session and load it again.\npublic class ChatSessionWithHistory\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\nChatSession session;\nif (Directory.Exists(\"Assets/chat-with-bob\"))\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Loading session from disk.\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nsession = new ChatSession(executor);\nsession.LoadSession(\"Assets/chat-with-bob\");\n}\nelse\n{\nvar chatHistoryJson = File.ReadAllText(\"Assets/chat-with-bob.json\");\nChatHistory chatHistory = ChatHistory.FromJson(chatHistoryJson) ?? new ChatHistory();\n\nsession = new ChatSession(executor, chatHistory);\n}\n\nsession.WithOutputTransform(new LLamaTransforms.KeywordTextOutputStreamTransform(\nnew string[] { \"User:\", \"Assistant:\" },\nredundancyLength: 8));\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"User:\" }\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started.\");\nConsole.WriteLine(\"Type 'exit' to end the chat session.\");\nConsole.WriteLine(\"Type 'save' to save the chat session to disk.\");\nConsole.WriteLine(\"Type 'load' to load the chat session from disk.\");\nConsole.WriteLine(\"Type 'regenerate' to regenerate the last response.\");\n\n// show the prompt\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\n// Save the chat state to disk\nif (userInput == \"save\")\n{\nsession.SaveSession(\"Assets/chat-with-bob\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Session saved.\");\n}\n// Load the chat state from disk\nelse if (userInput == \"load\")\n{\nsession.LoadSession(\"Assets/chat-with-bob\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Session loaded.\");\n}\nelse if (userInput == \"regenerate\")\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Regenerating last response ...\");\n\nawait foreach (\nvar text\nin session.RegenerateAssistantMessageAsync(\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n}\nelse\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n</code></pre>"},{"location":"Examples/ChatSessionWithRestart/","title":"ChatSession - restarting","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples;\n\n// This example shows how to restart the chat session\npublic class ChatSessionWithRestart\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\nvar chatHistoryJson = File.ReadAllText(\"Assets/chat-with-bob.json\");\nChatHistory chatHistory = ChatHistory.FromJson(chatHistoryJson) ?? new ChatHistory();\nChatSession prototypeSession = await ChatSession.InitializeSessionFromHistoryAsync(executor, chatHistory);\nprototypeSession.WithOutputTransform(new LLamaTransforms.KeywordTextOutputStreamTransform(\nnew string[] { \"User:\", \"Assistant:\" },\nredundancyLength: 8));\nvar resetState = prototypeSession.GetSessionState();\n\nChatSession session = new ChatSession(executor);\nsession.LoadSession(resetState);\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"User:\" }\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started. Starting point saved.\");\nConsole.WriteLine(\"Type 'exit' to end the chat session.\");\nConsole.WriteLine(\"Type 'save' to save chat session state in memory.\");\nConsole.WriteLine(\"Type 'reset' to reset the chat session to its saved state.\");\nConsole.WriteLine(\"Type 'answer for assistant' to add and process provided user and assistant messages.\");\n\n// show the prompt\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\n// Load the session state from the reset state\nif(userInput == \"reset\")\n{\nsession.LoadSession(resetState);\nConsole.WriteLine($\"Reset to history:\\n{session.HistoryTransform.HistoryToText(session.History)}\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Session reset.\");\n}\n// Assign new reset state.\nelse if (userInput == \"save\")\n{\nresetState = session.GetSessionState();\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Session saved.\");\n}\n// Provide user and override assistant answer with your own.\nelse if (userInput == \"answer for assistant\")\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Provide user input: \");\n\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInputOverride = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Provide assistant input: \");\n\nConsole.ForegroundColor = ConsoleColor.Green;\nstring assistantInputOverride = Console.ReadLine() ?? \"\";\n\nawait session.AddAndProcessUserMessage(userInputOverride);\nawait session.AddAndProcessAssistantMessage(assistantInputOverride);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"User and assistant messages processed. Provide next user message:\");\n}\nelse\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n</code></pre>"},{"location":"Examples/ChatSessionWithRoleName/","title":"ChatSession - Basic","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples;\n\n// The basic example for using ChatSession\npublic class ChatSessionWithRoleName\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\nvar chatHistoryJson = File.ReadAllText(\"Assets/chat-with-bob.json\");\nChatHistory chatHistory = ChatHistory.FromJson(chatHistoryJson) ?? new ChatHistory();\n\nChatSession session = new(executor, chatHistory);\n\nInferenceParams inferenceParams = new InferenceParams()\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"User:\" }\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started.\");\n\n// show the prompt\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, userInput),\ninferenceParams))\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(text);\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nuserInput = Console.ReadLine() ?? \"\";\n\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n</code></pre>"},{"location":"Examples/CodingAssistant/","title":"Coding assistant","text":"<pre><code>namespace LLama.Examples.Examples\n{\nusing LLama.Common;\nusing System;\n\n// This example shows how to apply code completion as a coding assistant\ninternal class CodingAssistant\n{\n// Source paper with example prompts:\n// https://doi.org/10.48550/arXiv.2308.12950\nconst string InstructionPrefix = \"[INST]\";\nconst string InstructionSuffix = \"[/INST]\";\nconst string SystemInstruction = \"You're an intelligent, concise coding assistant. \" +\n\"Wrap code in ``` for readability. Don't repeat yourself. \" +\n\"Use best practice and good coding standards.\";\n\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\nif (!modelPath.Contains(\"codellama\", StringComparison.InvariantCultureIgnoreCase))\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"WARNING: the model you selected is not a Code LLama model!\");\nConsole.WriteLine(\"For this example we specifically recommend 'codellama-7b-instruct.Q4_K_S.gguf'\");\nConsole.WriteLine(\"Press ENTER to continue...\");\nConsole.ReadLine();\n}\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 4096\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InstructExecutor(context, InstructionPrefix, InstructionSuffix, null);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the LLM will follow your instructions.\" +\n\"\\nIt's a 7B Code Llama, so it's trained for programming tasks like \\\"Write a C# function reading \" +\n\"a file name from a given URI\\\" or \\\"Write some programming interview questions\\\".\" +\n\"\\nWrite 'exit' to exit\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nvar inferenceParams = new InferenceParams()\n{\nTemperature = 0.8f,\nMaxTokens = -1,\n};\n\nstring instruction = $\"{SystemInstruction}\\n\\n\";\nawait Console.Out.WriteAsync(\"Instruction: \");\ninstruction += Console.ReadLine() ?? \"Ask me for instructions.\";\nwhile (instruction != \"exit\")\n{\n\nConsole.ForegroundColor = ConsoleColor.Green;\nawait foreach (var text in executor.InferAsync(instruction + Environment.NewLine, inferenceParams))\n{\nConsole.Write(text);\n}\nConsole.ForegroundColor = ConsoleColor.White;\n\nawait Console.Out.WriteAsync(\"Instruction: \");\ninstruction = Console.ReadLine() ?? \"Ask me for instructions.\";\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/GetEmbeddings/","title":"Get embeddings","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\n// This example shows how to get embeddings from a text prompt.\npublic class GetEmbeddings\n{\npublic static void Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nConsole.ForegroundColor = ConsoleColor.DarkGray;\nvar @params = new ModelParams(modelPath) { EmbeddingMode = true };\nusing var weights = LLamaWeights.LoadFromFile(@params);\nvar embedder = new LLamaEmbedder(weights, @params);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\n\"\"\"\nThis example displays embeddings from a text prompt.\nEmbeddings are numerical codes that represent information like words, images, or concepts.\nThese codes capture important relationships between those objects, like how similar words are in meaning or how close images are visually.\nThis allows machine learning models to efficiently understand and process complex data.\nEmbeddings of a text in LLM is sometimes useful, for example, to train other MLP models.\n\"\"\"); // NOTE: this description was AI generated\n\nwhile (true)\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(\"Please input your text: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nvar text = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\n\nfloat[] embeddings = embedder.GetEmbeddings(text).Result;\nConsole.WriteLine($\"Embeddings contain {embeddings.Length:N0} floating point values:\");\nConsole.ForegroundColor = ConsoleColor.DarkGray;\nConsole.WriteLine(string.Join(\", \", embeddings.Take(20)) + \", ...\");\nConsole.WriteLine();\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/GrammarJsonResponse/","title":"Grammar - json response","text":"<pre><code>using LLama.Common;\nusing LLama.Grammars;\n\nnamespace LLama.Examples.Examples\n{\n// This example shows how to get response in json format using grammar.\npublic class GrammarJsonResponse\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar gbnf = File.ReadAllText(\"Assets/json.gbnf\").Trim();\nvar grammar = Grammar.Parse(gbnf, \"root\");\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nvar ex = new StatelessExecutor(model, parameters);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the LLM will follow your instructions and always respond in a JSON format. For example, you can input \\\"Tell me the attributes of a good dish\\\"\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nusing var grammarInstance = grammar.CreateInstance();\nvar inferenceParams = new InferenceParams()\n{\nTemperature = 0.6f,\nAntiPrompts = new List&lt;string&gt; { \"Question:\", \"#\", \"Question: \", \".\\n\" },\nMaxTokens = 50,\nGrammar = grammarInstance\n};\n\nwhile (true)\n{\nConsole.Write(\"\\nQuestion: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nvar prompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(\"Answer: \");\nprompt = $\"Question: {prompt?.Trim()} Answer: \";\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/InstructModeExecute/","title":"Instruct executor - basic","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\n// This example shows how to use InstructExecutor to generate the response.\npublic class InstructModeExecute\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar prompt = File.ReadAllText(\"Assets/dan.txt\").Trim();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InstructExecutor(context);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the LLM will follow your instructions. For example, you can input \\\"Write a story about a fox who want to \" +\n\"make friend with human, no less than 200 words.\\\"\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nvar inferenceParams = new InferenceParams() { Temperature = 0.8f, MaxTokens = 600 };\n\nwhile (true)\n{\nawait foreach (var text in executor.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/InteractiveModeExecute/","title":"Interactive executor - basic","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\n// This is an example which shows how to chat with LLM with InteractiveExecutor.\npublic class InteractiveModeExecute\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar prompt = (await File.ReadAllTextAsync(\"Assets/chat-with-bob.txt\")).Trim();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar ex = new InteractiveExecutor(context);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the prompt is printed, the maximum tokens is set to 128 and the context size is 256. (an example for small scale usage)\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nConsole.Write(prompt);\n\nvar inferenceParams = new InferenceParams() { Temperature = 0.6f, AntiPrompts = new List&lt;string&gt; { \"User:\" }, MaxTokens = 128 };\n\nwhile (true)\n{\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/KernelMemory/","title":"Kernel memory integration - basic","text":"<pre><code>using LLamaSharp.KernelMemory;\nusing Microsoft.KernelMemory;\nusing Microsoft.KernelMemory.Configuration;\nusing System.Diagnostics;\n\nnamespace LLama.Examples.Examples\n{\n// This example is from Microsoft's official kernel memory \"custom prompts\" example:\n// https://github.com/microsoft/kernel-memory/blob/6d516d70a23d50c6cb982e822e6a3a9b2e899cfa/examples/101-dotnet-custom-Prompts/Program.cs#L1-L86\n\n// Microsoft.KernelMemory has more features than Microsoft.SemanticKernel.\n// See https://microsoft.github.io/kernel-memory/ for details.\n\npublic class KernelMemory\n{\npublic static async Task Run()\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\n\"\"\"\n\nThis program uses the Microsoft.KernelMemory package to ingest documents\nand answer questions about them in an interactive chat prompt.\n\n\"\"\");\n\n// Setup the kernel memory with the LLM model\nstring modelPath = UserSettings.GetModelPath();\nIKernelMemory memory = CreateMemory(modelPath);\n\n// Ingest documents (format is automatically detected from the filename)\nstring[] filesToIngest = [\nPath.GetFullPath(@\"./Assets/sample-SK-Readme.pdf\"),\nPath.GetFullPath(@\"./Assets/sample-KM-Readme.pdf\"),\n];\n\nfor (int i = 0; i &lt; filesToIngest.Length; i++)\n{\nstring path = filesToIngest[i];\nStopwatch sw = Stopwatch.StartNew();\nConsole.ForegroundColor = ConsoleColor.Blue;\nConsole.WriteLine($\"Importing {i + 1} of {filesToIngest.Length}: {path}\");\nawait memory.ImportDocumentAsync(path, steps: Constants.PipelineWithoutSummary);\nConsole.WriteLine($\"Completed in {sw.Elapsed}\\n\");\n}\n\n// Ask a predefined question\nConsole.ForegroundColor = ConsoleColor.Green;\nstring question1 = \"What formats does KM support\";\nConsole.WriteLine($\"Question: {question1}\");\nawait AnswerQuestion(memory, question1);\n\n// Let the user ask additional questions\nwhile (true)\n{\nConsole.ForegroundColor = ConsoleColor.Green;\nConsole.Write(\"Question: \");\nstring question = Console.ReadLine()!;\nif (string.IsNullOrEmpty(question))\nreturn;\n\nawait AnswerQuestion(memory, question);\n}\n}\n\nprivate static IKernelMemory CreateMemory(string modelPath)\n{\nCommon.InferenceParams infParams = new() { AntiPrompts = [\"\\n\\n\"] };\n\nLLamaSharpConfig lsConfig = new(modelPath) { DefaultInferenceParams = infParams };\n\nSearchClientConfig searchClientConfig = new()\n{\nMaxMatchesCount = 1,\nAnswerTokens = 100,\n};\n\nTextPartitioningOptions parseOptions = new()\n{\nMaxTokensPerParagraph = 300,\nMaxTokensPerLine = 100,\nOverlappingTokens = 30\n};\n\nreturn new KernelMemoryBuilder()\n.WithLLamaSharpDefaults(lsConfig)\n.WithSearchClientConfig(searchClientConfig)\n.With(parseOptions)\n.Build();\n}\n\nprivate static async Task AnswerQuestion(IKernelMemory memory, string question)\n{\nStopwatch sw = Stopwatch.StartNew();\nConsole.ForegroundColor = ConsoleColor.DarkGray;\nConsole.WriteLine($\"Generating answer...\");\n\nMemoryAnswer answer = await memory.AskAsync(question);\nConsole.WriteLine($\"Answer generated in {sw.Elapsed}\");\n\nConsole.ForegroundColor = ConsoleColor.Gray;\nConsole.WriteLine($\"Answer: {answer.Result}\");\nforeach (var source in answer.RelevantSources)\n{\nConsole.WriteLine($\"Source: {source.SourceName}\");\n}\nConsole.WriteLine();\n}\n}\n}\n</code></pre>"},{"location":"Examples/KernelMemorySaveAndLoad/","title":"Kernel-memory - save &amp; load","text":"<pre><code>using LLamaSharp.KernelMemory;\nusing Microsoft.KernelMemory;\nusing Microsoft.KernelMemory.Configuration;\nusing Microsoft.KernelMemory.ContentStorage.DevTools;\nusing Microsoft.KernelMemory.FileSystem.DevTools;\nusing Microsoft.KernelMemory.MemoryStorage.DevTools;\nusing System.Diagnostics;\n\nnamespace LLama.Examples.Examples;\n\n// This example shows how to use kernel-memory integration with pre-saved embeddings.\npublic class KernelMemorySaveAndLoad\n{\nstatic string StorageFolder =&gt; Path.GetFullPath($\"./storage-{nameof(KernelMemorySaveAndLoad)}\");\nstatic bool StorageExists =&gt; Directory.Exists(StorageFolder) &amp;&amp; Directory.GetDirectories(StorageFolder).Length &gt; 0;\n\npublic static async Task Run()\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\n\"\"\"\n\nThis program uses the Microsoft.KernelMemory package to ingest documents\nand store the embeddings as local files so they can be quickly recalled\nwhen this application is launched again. \"\"\");\n\nstring modelPath = UserSettings.GetModelPath();\nIKernelMemory memory = CreateMemoryWithLocalStorage(modelPath);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nif (StorageExists)\n{\nConsole.WriteLine(\n\"\"\"\n\nKernel memory files have been located!\nInformation about previously analyzed documents has been loaded.\n\n\"\"\");\n}\nelse\n{\nConsole.WriteLine(\n$\"\"\"\n\nExisting kernel memory was not found.\nDocuments will be analyzed (slow) and information saved to disk.\nAnalysis will not be required the next time this program is run.\nPress ENTER to proceed...\n\n\"\"\");\nConsole.ReadLine();\nawait IngestDocuments(memory);\n}\n\nawait AskSingleQuestion(memory, \"What formats does KM support?\");\nawait StartUserChatSession(memory);\n}\n\nprivate static IKernelMemory CreateMemoryWithLocalStorage(string modelPath)\n{\nCommon.InferenceParams infParams = new() { AntiPrompts = [\"\\n\\n\"] };\n\nLLamaSharpConfig lsConfig = new(modelPath) { DefaultInferenceParams = infParams };\n\nSearchClientConfig searchClientConfig = new()\n{\nMaxMatchesCount = 1,\nAnswerTokens = 100,\n};\n\nTextPartitioningOptions parseOptions = new()\n{\nMaxTokensPerParagraph = 300,\nMaxTokensPerLine = 100,\nOverlappingTokens = 30\n};\n\nSimpleFileStorageConfig storageConfig = new()\n{\nDirectory = StorageFolder,\nStorageType = FileSystemTypes.Disk,\n};\n\nSimpleVectorDbConfig vectorDbConfig = new()\n{\nDirectory = StorageFolder,\nStorageType = FileSystemTypes.Disk,\n};\n\nConsole.ForegroundColor = ConsoleColor.Blue;\nConsole.WriteLine($\"Kernel memory folder: {StorageFolder}\");\n\nConsole.ForegroundColor = ConsoleColor.DarkGray;\nreturn new KernelMemoryBuilder()\n.WithSimpleFileStorage(storageConfig)\n.WithSimpleVectorDb(vectorDbConfig)\n.WithLLamaSharpDefaults(lsConfig)\n.WithSearchClientConfig(searchClientConfig)\n.With(parseOptions)\n.Build();\n}\n\nprivate static async Task AskSingleQuestion(IKernelMemory memory, string question)\n{\nConsole.ForegroundColor = ConsoleColor.Green;\nConsole.WriteLine($\"Question: {question}\");\nawait ShowAnswer(memory, question);\n}\n\nprivate static async Task StartUserChatSession(IKernelMemory memory)\n{\nwhile (true)\n{\nConsole.ForegroundColor = ConsoleColor.Green;\nConsole.Write(\"Question: \");\nstring question = Console.ReadLine()!;\nif (string.IsNullOrEmpty(question))\nreturn;\n\nawait ShowAnswer(memory, question);\n}\n}\n\nprivate static async Task IngestDocuments(IKernelMemory memory)\n{\nstring[] filesToIngest = [\nPath.GetFullPath(@\"./Assets/sample-SK-Readme.pdf\"),\nPath.GetFullPath(@\"./Assets/sample-KM-Readme.pdf\"),\n];\n\nfor (int i = 0; i &lt; filesToIngest.Length; i++)\n{\nstring path = filesToIngest[i];\nStopwatch sw = Stopwatch.StartNew();\nConsole.ForegroundColor = ConsoleColor.Blue;\nConsole.WriteLine($\"Importing {i + 1} of {filesToIngest.Length}: {path}\");\nawait memory.ImportDocumentAsync(path, steps: Constants.PipelineWithoutSummary);\nConsole.WriteLine($\"Completed in {sw.Elapsed}\\n\");\n}\n}\n\nprivate static async Task ShowAnswer(IKernelMemory memory, string question)\n{\nStopwatch sw = Stopwatch.StartNew();\nConsole.ForegroundColor = ConsoleColor.DarkGray;\nConsole.WriteLine($\"Generating answer...\");\n\nMemoryAnswer answer = await memory.AskAsync(question);\nConsole.WriteLine($\"Answer generated in {sw.Elapsed}\");\n\nConsole.ForegroundColor = ConsoleColor.Gray;\nConsole.WriteLine($\"Answer: {answer.Result}\");\nforeach (var source in answer.RelevantSources)\n{\nConsole.WriteLine($\"Source: {source.SourceName}\");\n}\nConsole.WriteLine();\n}\n}\n</code></pre>"},{"location":"Examples/LLavaInteractiveModeExecute/","title":"LLaVA - basic","text":"<pre><code>using System.Text.RegularExpressions;\nusing LLama.Common;\nusing Spectre.Console;\nusing LLama.Native;\n\nnamespace LLama.Examples.Examples\n{\n// This example shows how to chat with LLaVA model with both image and text as input.\n// It uses the interactive executor to inference.\npublic class LlavaInteractiveModeExecute\n{\npublic static async Task Run()\n{\nstring multiModalProj = UserSettings.GetMMProjPath();\nstring modelPath = UserSettings.GetModelPath();\nstring modelImage = UserSettings.GetImagePath();\nconst int maxTokens = 1024;\n\nvar prompt = $\"{{{modelImage}}}\\nUSER:\\nProvide a full description of the image.\\nASSISTANT:\\n\";\n\nvar parameters = new ModelParams(modelPath);\n\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\n\n// Llava Init\nusing var clipModel = LLavaWeights.LoadFromFile(multiModalProj);\n\nvar ex = new InteractiveExecutor(context, clipModel );\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the prompt is printed, the maximum tokens is set to {0} and the context size is {1}.\", maxTokens, parameters.ContextSize );\nConsole.WriteLine(\"To send an image, enter its filename in curly braces, like this {c:/image.jpg}.\");  var inferenceParams = new InferenceParams() { Temperature = 0.1f, AntiPrompts = new List&lt;string&gt; { \"\\nUSER:\" }, MaxTokens = maxTokens };\n\ndo\n{\n\n// Evaluate if we have images\n//\nvar imageMatches = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Value);\nvar imageCount = imageMatches.Count();\nvar hasImages = imageCount &gt; 0;\n\nif (hasImages)\n{\nvar imagePathsWithCurlyBraces = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Value);\nvar imagePaths = Regex.Matches(prompt, \"{([^}]*)}\").Select(m =&gt; m.Groups[1].Value).ToList();\n\nList&lt;byte[]&gt; imageBytes;\ntry\n{\nimageBytes = imagePaths.Select(File.ReadAllBytes).ToList();\n}\ncatch (IOException exception)\n{\nConsole.ForegroundColor = ConsoleColor.Red;\nConsole.Write(\n$\"Could not load your {(imageCount == 1 ? \"image\" : \"images\")}:\");\nConsole.Write($\"{exception.Message}\");\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Please try again.\");\nbreak;\n}\n\n// Each prompt with images we clear cache\n// When the prompt contains images we clear KV_CACHE to restart conversation\n// See:\n// https://github.com/ggerganov/llama.cpp/discussions/3620\nex.Context.NativeHandle.KvCacheRemove( LLamaSeqId.Zero, -1, -1 );\n\nint index = 0;\nforeach (var path in imagePathsWithCurlyBraces)\n{\n// First image replace to tag &lt;image, the rest of the images delete the tag\nprompt = prompt.Replace(path, index++ == 0 ? \"&lt;image&gt;\" : \"\");\n}\n\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine($\"Here are the images, that are sent to the chat model in addition to your message.\");\nConsole.WriteLine();\n\nforeach (var consoleImage in imageBytes?.Select(bytes =&gt; new CanvasImage(bytes)))\n{\nconsoleImage.MaxWidth = 50;\nAnsiConsole.Write(consoleImage);\n}\n\nConsole.WriteLine();\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine($\"The images were scaled down for the console only, the model gets full versions.\");\nConsole.WriteLine($\"Write /exit or press Ctrl+c to return to main menu.\");\nConsole.WriteLine();\n\n\n// Initialize Images in executor\n//\nforeach (var image in imagePaths)\n{\nex.Images.Add(await File.ReadAllBytesAsync(image));\n}\n}\n\nConsole.ForegroundColor = Color.White;\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\nConsole.Write(\" \");\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.WriteLine();\n\n// let the user finish with exit\n//\nif (prompt != null &amp;&amp; prompt.Equals(\"/exit\", StringComparison.OrdinalIgnoreCase))\nbreak;\n\n}\nwhile(true);\n}\n}\n}\n</code></pre>"},{"location":"Examples/LoadAndSaveSession/","title":"ChatSession - load &amp; save","text":"<p>\u26a0\ufe0fWarning: this example has been outdated for the latest version of LLamaSharp, please refer to this example to see how to save and load state for <code>ChatSession</code>. If you are using some old versions of LLamaSharp, this example may help you.</p> <pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\npublic class SaveAndLoadSession\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar prompt = (await File.ReadAllTextAsync(\"Assets/chat-with-bob.txt\")).Trim();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar ex = new InteractiveExecutor(context);\n\nvar session = new ChatSession(ex);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The chat session has started. In this example, the prompt is printed for better visual result. Input \\\"save\\\" to save and reload the session.\");\nConsole.ForegroundColor = ConsoleColor.White;\n\n// show the prompt\nConsole.Write(prompt);\nwhile (true)\n{\nawait foreach (\nvar text\nin session.ChatAsync(\nnew ChatHistory.Message(AuthorRole.User, prompt),\nnew InferenceParams()\n{\nTemperature = 0.6f,\nAntiPrompts = new List&lt;string&gt; { \"User:\" }\n}))\n{\nConsole.Write(text);\n}\n\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\nif (prompt == \"save\")\n{\nConsole.Write(\"Preparing to save the state, please input the path you want to save it: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nvar statePath = Console.ReadLine();\nsession.SaveSession(statePath);\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Saved session!\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nex.Context.Dispose();\nex = new(new LLamaContext(model, parameters));\nsession = new ChatSession(ex);\nsession.LoadSession(statePath);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Loaded session!\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nConsole.Write(\"Now you can continue your session: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/LoadAndSaveState/","title":"Executor - save/load state","text":"<pre><code>using LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\n// This example shows how to save/load state of the executor.\npublic class LoadAndSaveState\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar prompt = (await File.ReadAllTextAsync(\"Assets/chat-with-bob.txt\")).Trim();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar ex = new InteractiveExecutor(context);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the prompt is printed, \" +\n\"the maximum tokens is set to 64 and the context size is 256. (an example for small scale usage)\");\n\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(prompt);\n\nvar inferenceParams = new InferenceParams() { Temperature = 0.6f, AntiPrompts = new List&lt;string&gt; { \"User:\" } };\n\nwhile (true)\n{\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams))\n{\nConsole.Write(text);\n}\n\nprompt = Console.ReadLine();\nif (prompt == \"save\")\n{\nConsole.Write(\"Your path to save model state: \");\nvar modelStatePath = Console.ReadLine();\nex.Context.SaveState(modelStatePath);\n\nConsole.Write(\"Your path to save executor state: \");\nvar executorStatePath = Console.ReadLine();\nawait ex.SaveState(executorStatePath);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"All states saved!\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nvar ctx = ex.Context;\nctx.LoadState(modelStatePath);\nex = new InteractiveExecutor(ctx);\nawait ex.LoadState(executorStatePath);\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"Loaded state!\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nConsole.Write(\"Now you can continue your session: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nprompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\n}\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/QuantizeModel/","title":"Quantization","text":"<pre><code>namespace LLama.Examples.Examples\n{\npublic class QuantizeModel\n{\npublic static void Run()\n{\nstring inputPath = UserSettings.GetModelPath();\n\nConsole.Write(\"Please input your output model path: \");\nvar outputPath = Console.ReadLine();\n\nConsole.Write(\"Please input the quantize type (one of q4_0, q4_1, q5_0, q5_1, q8_0): \");\nvar quantizeType = Console.ReadLine();\n\nif (LLamaQuantizer.Quantize(inputPath, outputPath, quantizeType))\n{\nConsole.WriteLine(\"Quantization succeeded!\");\n}\nelse\n{\nConsole.WriteLine(\"Quantization failed!\");\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/SemanticKernelChat/","title":"Semantic-kernel - chat","text":"<pre><code>using LLama.Common;\nusing LLamaSharp.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel.ChatCompletion;\n\nnamespace LLama.Examples.Examples\n{\npublic class SemanticKernelChat\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"This example is from: \\n\" +\n\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/KernelSyntaxExamples/Example17_ChatGPT.cs\");\n\n// Load weights into memory\nvar parameters = new ModelParams(modelPath);\nusing var model = LLamaWeights.LoadFromFile(parameters);\nvar ex = new StatelessExecutor(model, parameters);\n\nvar chatGPT = new LLamaSharpChatCompletion(ex);\n\nvar chatHistory = chatGPT.CreateNewChat(\"This is a conversation between the \" +\n\"assistant and the user. \\n\\n You are a librarian, expert about books. \");\n\nConsole.WriteLine(\"Chat content:\");\nConsole.WriteLine(\"------------------------\");\n\nchatHistory.AddUserMessage(\"Hi, I'm looking for book suggestions\");\nawait MessageOutputAsync(chatHistory);\n\n// First bot assistant message\nvar reply = await chatGPT.GetChatMessageContentAsync(chatHistory);\nchatHistory.AddAssistantMessage(reply.Content);\nawait MessageOutputAsync(chatHistory);\n\n// Second user message\nchatHistory.AddUserMessage(\"I love history and philosophy, I'd like to learn \" +\n\"something new about Greece, any suggestion\");\nawait MessageOutputAsync(chatHistory);\n\n// Second bot assistant message\nreply = await chatGPT.GetChatMessageContentAsync(chatHistory);\nchatHistory.AddAssistantMessage(reply.Content);\nawait MessageOutputAsync(chatHistory);\n}\n\n/// &lt;summary&gt;\n/// Outputs the last message of the chat history\n/// &lt;/summary&gt;\nprivate static Task MessageOutputAsync(Microsoft.SemanticKernel.ChatCompletion.ChatHistory chatHistory)\n{\nvar message = chatHistory.Last();\n\nConsole.WriteLine($\"{message.Role}: {message.Content}\");\nConsole.WriteLine(\"------------------------\");\n\nreturn Task.CompletedTask;\n}\n}\n}\n</code></pre>"},{"location":"Examples/SemanticKernelMemory/","title":"Semantic-kernel - with kernel-memory","text":"<p>Semantic Memory allows to store your data like traditional DBs, adding the ability to query it using natural language.</p> <pre><code>using LLama.Common;\nusing Microsoft.SemanticKernel.Memory;\nusing LLamaSharp.SemanticKernel.TextEmbedding;\n\nnamespace LLama.Examples.Examples\n{\npublic class SemanticKernelMemory\n{\nprivate const string MemoryCollectionName = \"SKGitHub\";\n\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nConsole.WriteLine(\"This example is from: \\n\" +\n\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/samples/KernelSyntaxExamples/Example14_SemanticMemory.cs\");\n\nvar seed = 1337u;\n// Load weights into memory\nvar parameters = new ModelParams(modelPath)\n{\nSeed = seed,\nEmbeddingMode = true\n};\n\nusing var model = LLamaWeights.LoadFromFile(parameters);\nvar embedding = new LLamaEmbedder(model, parameters);\n\nConsole.WriteLine(\"====================================================\");\nConsole.WriteLine(\"======== Semantic Memory (volatile, in RAM) ========\");\nConsole.WriteLine(\"====================================================\");\n\n/* You can build your own semantic memory combining an Embedding Generator\n             * with a Memory storage that supports search by similarity (ie semantic search).\n             *\n             * In this example we use a volatile memory, a local simulation of a vector DB.\n             *\n             * You can replace VolatileMemoryStore with Qdrant (see QdrantMemoryStore connector)\n             * or implement your connectors for Pinecone, Vespa, Postgres + pgvector, SQLite VSS, etc.\n             */\n\nvar memory = new MemoryBuilder()\n.WithTextEmbeddingGeneration(new LLamaSharpEmbeddingGeneration(embedding))\n.WithMemoryStore(new VolatileMemoryStore())\n.Build();\n\nawait RunExampleAsync(memory);\n}\n\nprivate static async Task RunExampleAsync(ISemanticTextMemory memory)\n{\nawait StoreMemoryAsync(memory);\n\nawait SearchMemoryAsync(memory, \"How do I get started?\");\n\n/*\n            Output:\n\n            Query: How do I get started?\n\n            Result 1:\n              URL:     : https://github.com/microsoft/semantic-kernel/blob/main/README.md\n              Title    : README: Installation, getting started, and how to contribute\n\n            Result 2:\n              URL:     : https://github.com/microsoft/semantic-kernel/blob/main/samples/dotnet-jupyter-notebooks/00-getting-started.ipynb\n              Title    : Jupyter notebook describing how to get started with the Semantic Kernel\n\n            */\n\nawait SearchMemoryAsync(memory, \"Can I build a chat with SK?\");\n\n/*\n            Output:\n\n            Query: Can I build a chat with SK?\n\n            Result 1:\n              URL:     : https://github.com/microsoft/semantic-kernel/tree/main/samples/skills/ChatSkill/ChatGPT\n              Title    : Sample demonstrating how to create a chat skill interfacing with ChatGPT\n\n            Result 2:\n              URL:     : https://github.com/microsoft/semantic-kernel/blob/main/samples/apps/chat-summary-webapp-react/README.md\n              Title    : README: README associated with a sample chat summary react-based webapp\n\n            */\n\nawait SearchMemoryAsync(memory, \"Jupyter notebook\");\n\nawait SearchMemoryAsync(memory, \"README: README associated with a sample chat summary react-based webapp\");\n\nawait SearchMemoryAsync(memory, \"Jupyter notebook describing how to pass prompts from a file to a semantic skill or function\");\n}\n\nprivate static async Task SearchMemoryAsync(ISemanticTextMemory memory, string query)\n{\nConsole.WriteLine(\"\\nQuery: \" + query + \"\\n\");\n\nvar memories = memory.SearchAsync(MemoryCollectionName, query, limit: 10, minRelevanceScore: 0.5);\n\nint i = 0;\nawait foreach (MemoryQueryResult result in memories)\n{\nConsole.WriteLine($\"Result {++i}:\");\nConsole.WriteLine(\"  URL:     : \" + result.Metadata.Id);\nConsole.WriteLine(\"  Title    : \" + result.Metadata.Description);\nConsole.WriteLine(\"  Relevance: \" + result.Relevance);\nConsole.WriteLine();\n}\n\nConsole.WriteLine(\"----------------------\");\n}\n\nprivate static async Task StoreMemoryAsync(ISemanticTextMemory memory)\n{\n/* Store some data in the semantic memory.\n             *\n             * When using Azure Cognitive Search the data is automatically indexed on write.\n             *\n             * When using the combination of VolatileStore and Embedding generation, SK takes\n             * care of creating and storing the index\n             */\n\nConsole.WriteLine(\"\\nAdding some GitHub file URLs and their descriptions to the semantic memory.\");\nvar githubFiles = SampleData();\nvar i = 0;\nforeach (var entry in githubFiles)\n{\nvar result = await memory.SaveReferenceAsync(\ncollection: MemoryCollectionName,\nexternalSourceName: \"GitHub\",\nexternalId: entry.Key,\ndescription: entry.Value,\ntext: entry.Value);\n\nConsole.WriteLine($\"#{++i} saved.\");\nConsole.WriteLine(result);\n}\n\nConsole.WriteLine(\"\\n----------------------\");\n}\n\nprivate static Dictionary&lt;string, string&gt; SampleData()\n{\nreturn new Dictionary&lt;string, string&gt;\n{\n[\"https://github.com/microsoft/semantic-kernel/blob/main/README.md\"]\n= \"README: Installation, getting started, and how to contribute\",\n[\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/notebooks/02-running-prompts-from-file.ipynb\"]\n= \"Jupyter notebook describing how to pass prompts from a file to a semantic skill or function\",\n[\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/notebooks//00-getting-started.ipynb\"]\n= \"Jupyter notebook describing how to get started with the Semantic Kernel\",\n[\"https://github.com/microsoft/semantic-kernel/tree/main/samples/skills/ChatSkill/ChatGPT\"]\n= \"Sample demonstrating how to create a chat skill interfacing with ChatGPT\",\n[\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/src/SemanticKernel/Memory/VolatileMemoryStore.cs\"]\n= \"C# class that defines a volatile embedding store\",\n[\"https://github.com/microsoft/semantic-kernel/blob/main/samples/dotnet/KernelHttpServer/README.md\"]\n= \"README: How to set up a Semantic Kernel Service API using Azure Function Runtime v4\",\n[\"https://github.com/microsoft/semantic-kernel/blob/main/samples/apps/chat-summary-webapp-react/README.md\"]\n= \"README: README associated with a sample chat summary react-based webapp\",\n};\n}\n}\n}\n</code></pre>"},{"location":"Examples/SemanticKernelPrompt/","title":"Semantic-kernel - basic","text":"<pre><code>using LLama.Common;\nusing LLamaSharp.SemanticKernel.ChatCompletion;\nusing Microsoft.SemanticKernel;\nusing LLamaSharp.SemanticKernel.TextCompletion;\nusing Microsoft.SemanticKernel.TextGeneration;\nusing Microsoft.Extensions.DependencyInjection;\n\nnamespace LLama.Examples.Examples\n{\n// The basic example for using the semantic-kernel integration\npublic class SemanticKernelPrompt\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"This example is from: \" +\n\"https://github.com/microsoft/semantic-kernel/blob/main/dotnet/README.md\");\n\n// Load weights into memory\nvar parameters = new ModelParams(modelPath);\nusing var model = LLamaWeights.LoadFromFile(parameters);\nvar ex = new StatelessExecutor(model, parameters);\n\nvar builder = Kernel.CreateBuilder();\nbuilder.Services.AddKeyedSingleton&lt;ITextGenerationService&gt;(\"local-llama\", new LLamaSharpTextCompletion(ex));\n\nvar kernel = builder.Build();\n\nvar prompt = @\"{{$input}}\n\nOne line TLDR with the fewest words.\";\n\nChatRequestSettings settings = new() { MaxTokens = 100 };\nvar summarize = kernel.CreateFunctionFromPrompt(prompt, settings);\n\nstring text1 = @\"\n1st Law of Thermodynamics - Energy cannot be created or destroyed.\n2nd Law of Thermodynamics - For a spontaneous process, the entropy of the universe increases.\n3rd Law of Thermodynamics - A perfect crystal at zero Kelvin has zero entropy.\";\n\nstring text2 = @\"\n1. An object at rest remains at rest, and an object in motion remains in motion at constant speed and in a straight line unless acted on by an unbalanced force.\n2. The acceleration of an object depends on the mass of the object and the amount of force applied.\n3. Whenever one object exerts a force on another object, the second object exerts an equal and opposite on the first.\";\n\nConsole.WriteLine((await kernel.InvokeAsync(summarize, new() { [\"input\"] = text1 })).GetValue&lt;string&gt;());\n\nConsole.WriteLine((await kernel.InvokeAsync(summarize, new() { [\"input\"] = text2 })).GetValue&lt;string&gt;());\n}\n}\n}\n</code></pre>"},{"location":"Examples/StatelessModeExecute/","title":"Stateless executor","text":"<pre><code>using LLama.Common;\nusing LLama.Examples.Extensions;\n\nnamespace LLama.Examples.Examples\n{\n// Basic usage of the stateless executor.\npublic class StatelessModeExecute\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath)\n{\nContextSize = 1024,\nSeed = 1337,\nGpuLayerCount = 5\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nvar ex = new StatelessExecutor(model, parameters);\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(\"The executor has been enabled. In this example, the inference is an one-time job. That says, the previous input and response has \" +\n\"no impact on the current response. Now you can ask it questions. Note that in this example, no prompt was set for LLM and the maximum response tokens is 50. \" +\n\"It may not perform well because of lack of prompt. This is also an example that could indicate the importance of prompt in LLM. To improve it, you can add \" +\n\"a prompt for it yourself!\");\nConsole.ForegroundColor = ConsoleColor.White;\n\nvar inferenceParams = new InferenceParams() { Temperature = 0.6f, AntiPrompts = new List&lt;string&gt; { \"Question:\", \"#\", \"Question: \", \".\\n\" }, MaxTokens = 50 };\n\nwhile (true)\n{\nConsole.Write(\"\\nQuestion: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nvar prompt = Console.ReadLine();\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.Write(\"Answer: \");\nprompt = $\"Question: {prompt?.Trim()} Answer: \";\nawait foreach (var text in ex.InferAsync(prompt, inferenceParams).Spinner())\n{\nConsole.Write(text);\n}\n}\n}\n}\n}\n</code></pre>"},{"location":"Examples/TalkToYourself/","title":"Talk to yourself","text":"<pre><code>using System.Text;\nusing LLama.Abstractions;\nusing LLama.Common;\n\nnamespace LLama.Examples.Examples\n{\n// Let two bots chat with each other.\npublic class TalkToYourself\n{\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\n// Load weights into memory\nvar @params = new ModelParams(modelPath);\nusing var weights = LLamaWeights.LoadFromFile(@params);\n\n// Create 2 contexts sharing the same weights\nusing var aliceCtx = weights.CreateContext(@params);\nvar alice = new InteractiveExecutor(aliceCtx);\nusing var bobCtx = weights.CreateContext(@params);\nvar bob = new InteractiveExecutor(bobCtx);\n\n// Initial alice prompt\nvar alicePrompt = \"Transcript of a dialog, where the Alice interacts a person named Bob. Alice is friendly, kind, honest and good at writing.\\nAlice: Hello\";\nvar aliceResponse = await Prompt(alice, ConsoleColor.Green, alicePrompt, false, false);\n\n// Initial bob prompt\nvar bobPrompt = $\"Transcript of a dialog, where the Bob interacts a person named Alice. Bob is smart, intellectual and good at writing.\\nAlice: Hello{aliceResponse}\";\nvar bobResponse = await Prompt(bob, ConsoleColor.Red, bobPrompt, true, true);\n\n// swap back and forth from Alice to Bob\nwhile (true)\n{\naliceResponse = await Prompt(alice, ConsoleColor.Green, bobResponse, false, true);\nbobResponse = await Prompt(bob, ConsoleColor.Red, aliceResponse, false, true);\n\nif (Console.KeyAvailable)\nbreak;\n}\n}\n\nprivate static async Task&lt;string&gt; Prompt(ILLamaExecutor executor, ConsoleColor color, string prompt, bool showPrompt, bool showResponse)\n{\nvar inferenceParams = new InferenceParams\n{\nTemperature = 0.9f,\nAntiPrompts = new List&lt;string&gt; { \"Alice:\", \"Bob:\", \"User:\" },\nMaxTokens = 128,\nMirostat = MirostatType.Mirostat2,\nMirostatTau = 10,\n};\n\nConsole.ForegroundColor = ConsoleColor.White;\nif (showPrompt)\nConsole.Write(prompt);\n\nConsole.ForegroundColor = color;\nvar builder = new StringBuilder();\nawait foreach (var text in executor.InferAsync(prompt, inferenceParams))\n{\nbuilder.Append(text);\nif (showResponse)\nConsole.Write(text);\n}\n\nreturn builder.ToString();\n}\n}\n}\n</code></pre>"},{"location":"Integrations/bot-sharp/","title":"BotSharp integration","text":"<p>The document is under work, please have a wait. Thank you for your support! :)</p>"},{"location":"Integrations/kernel-memory/","title":"LLamaSharp.kernel-memory","text":"<p>The document is under work, please have a wait. Thank you for your support! :)</p>"},{"location":"Integrations/langchain/","title":"Langchain integration","text":"<p>The document is under work, please have a wait. Thank you for your support! :)</p>"},{"location":"Integrations/semantic-kernel/","title":"LLamaSharp.SemanticKernel","text":"<p>LLamaSharp.SemanticKernel are connections for SemanticKernel: an SDK for integrating various LLM interfaces into a single implementation. With this, you can add local LLaMa queries as another connection point with your existing connections.</p> <p>For reference on how to implement it, view the following examples: </p> <ul> <li>SemanticKernelChat</li> <li>SemanticKernelPrompt</li> <li>SemanticKernelMemory</li> </ul>"},{"location":"Integrations/semantic-kernel/#itextcompletion","title":"ITextCompletion","text":"<pre><code>using var model = LLamaWeights.LoadFromFile(parameters);\n// LLamaSharpTextCompletion can accept ILLamaExecutor. \nvar ex = new StatelessExecutor(model, parameters);\nvar builder = new KernelBuilder();\nbuilder.WithAIService&lt;ITextCompletion&gt;(\"local-llama\", new LLamaSharpTextCompletion(ex), true);\n</code></pre>"},{"location":"Integrations/semantic-kernel/#ichatcompletion","title":"IChatCompletion","text":"<pre><code>using var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\n// LLamaSharpChatCompletion requires InteractiveExecutor, as it's the best fit for the given command.\nvar ex = new InteractiveExecutor(context);\nvar chatGPT = new LLamaSharpChatCompletion(ex);\n</code></pre>"},{"location":"Integrations/semantic-kernel/#itextembeddinggeneration","title":"ITextEmbeddingGeneration","text":"<pre><code>using var model = LLamaWeights.LoadFromFile(parameters);\nvar embedding = new LLamaEmbedder(model, parameters);\nvar kernelWithCustomDb = Kernel.Builder\n.WithLoggerFactory(ConsoleLogger.LoggerFactory)\n.WithAIService&lt;ITextEmbeddingGeneration&gt;(\"local-llama-embed\", new LLamaSharpEmbeddingGeneration(embedding), true)\n.WithMemoryStorage(new VolatileMemoryStore())\n.Build();\n</code></pre>"},{"location":"More/log/","title":"The Logger in LLamaSharp","text":"<p>LLamaSharp supports customized logger because it could be used in many kinds of applications, like Winform/WPF, WebAPI and Blazor, so that the preference of logger varies.</p>"},{"location":"More/log/#define-customized-logger","title":"Define customized logger","text":"<p>What you need to do is to implement the <code>ILogger</code> interface. </p> <pre><code>public interface ILLamaLogger\n{\npublic enum LogLevel\n{\nInfo,\nDebug,\nWarning,\nError\n}\nvoid Log(string source, string message, LogLevel level);\n}\n</code></pre> <p>The <code>source</code> specifies where the log message is from, which could be a function, a class, etc..</p> <p>The <code>message</code> is the log message itself.</p> <p>The <code>level</code> is the level of the information in the log. As shown above, there're four levels, which are <code>info</code>, <code>debug</code>, <code>warning</code> and <code>error</code> respectively.</p> <p>The following is a simple example of the logger implementation:</p> <pre><code>public sealed class LLamaDefaultLogger : ILLamaLogger\n{\nprivate static readonly Lazy&lt;LLamaDefaultLogger&gt; _instance = new Lazy&lt;LLamaDefaultLogger&gt;(() =&gt; new LLamaDefaultLogger());\n\nprivate bool _toConsole = true;\nprivate bool _toFile = false;\n\nprivate FileStream? _fileStream = null;\nprivate StreamWriter _fileWriter = null;\n\npublic static LLamaDefaultLogger Default =&gt; _instance.Value;\n\nprivate LLamaDefaultLogger()\n{\n\n}\n\npublic LLamaDefaultLogger EnableConsole()\n{\n_toConsole = true;\nreturn this;\n}\n\npublic LLamaDefaultLogger DisableConsole()\n{\n_toConsole = false;\nreturn this;\n}\n\npublic LLamaDefaultLogger EnableFile(string filename, FileMode mode = FileMode.Append)\n{\n_fileStream = new FileStream(filename, mode, FileAccess.Write);\n_fileWriter = new StreamWriter(_fileStream);\n_toFile = true;\nreturn this;\n}\n\npublic LLamaDefaultLogger DisableFile(string filename)\n{\nif (_fileWriter is not null)\n{\n_fileWriter.Close();\n_fileWriter = null;\n}\nif (_fileStream is not null)\n{\n_fileStream.Close();\n_fileStream = null;\n}\n_toFile = false;\nreturn this;\n}\n\npublic void Log(string source, string message, LogLevel level)\n{\nif (level == LogLevel.Info)\n{\nInfo(message);\n}\nelse if (level == LogLevel.Debug)\n{\n\n}\nelse if (level == LogLevel.Warning)\n{\nWarn(message);\n}\nelse if (level == LogLevel.Error)\n{\nError(message);\n}\n}\n\npublic void Info(string message)\n{\nmessage = MessageFormat(\"info\", message);\nif (_toConsole)\n{\nConsole.ForegroundColor = ConsoleColor.White;\nConsole.WriteLine(message);\nConsole.ResetColor();\n}\nif (_toFile)\n{\nDebug.Assert(_fileStream is not null);\nDebug.Assert(_fileWriter is not null);\n_fileWriter.WriteLine(message);\n}\n}\n\npublic void Warn(string message)\n{\nmessage = MessageFormat(\"warn\", message);\nif (_toConsole)\n{\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.WriteLine(message);\nConsole.ResetColor();\n}\nif (_toFile)\n{\nDebug.Assert(_fileStream is not null);\nDebug.Assert(_fileWriter is not null);\n_fileWriter.WriteLine(message);\n}\n}\n\npublic void Error(string message)\n{\nmessage = MessageFormat(\"error\", message);\nif (_toConsole)\n{\nConsole.ForegroundColor = ConsoleColor.Red;\nConsole.WriteLine(message);\nConsole.ResetColor();\n}\nif (_toFile)\n{\nDebug.Assert(_fileStream is not null);\nDebug.Assert(_fileWriter is not null);\n_fileWriter.WriteLine(message);\n}\n}\n\nprivate string MessageFormat(string level, string message)\n{\nDateTime now = DateTime.Now;\nstring formattedDate = now.ToString(\"yyyy.MM.dd HH:mm:ss\");\nreturn $\"[{formattedDate}][{level}]: {message}\";\n}\n}\n</code></pre>"},{"location":"Tutorials/ChatSession/","title":"LLamaSharp chat session","text":""},{"location":"Tutorials/ChatSession/#basic-usages-of-chatsession","title":"Basic usages of ChatSession","text":"<p><code>ChatSession</code> is a higher-level abstraction than the executors. In the context of a chat application like ChatGPT, a \"chat session\" refers to an interactive conversation or exchange of messages between the user and the chatbot. It represents a continuous flow of communication where the user enters input or asks questions, and the chatbot responds accordingly. A chat session typically starts when the user initiates a conversation with the chatbot and continues until the interaction comes to a natural end or is explicitly terminated by either the user or the system. During a chat session, the chatbot maintains the context of the conversation, remembers previous messages, and generates appropriate responses based on the user's inputs and the ongoing dialogue.</p>"},{"location":"Tutorials/ChatSession/#initialize-a-session","title":"Initialize a session","text":"<p>Currently, the only parameter that is accepted is an <code>ILLamaExecutor</code>, because this is the only parameter that we're sure to exist in all the future versions. Since it's the high-level abstraction, we're conservative to the API designs. In the future, there may be more kinds of constructors added.</p> <pre><code>InteractiveExecutor ex = new(new LLamaModel(new ModelParams(modelPath)));\nChatSession session = new ChatSession(ex);\n</code></pre>"},{"location":"Tutorials/ChatSession/#chat-with-the-bot","title":"Chat with the bot","text":"<p>There'll be two kinds of input accepted by the <code>Chat</code> API, which are <code>ChatHistory</code> and <code>String</code>. The API with string is quite similar to that of the executors. Meanwhile, the API with <code>ChatHistory</code> is aimed to provide more flexible usages. For example, you have had a chat with the bot in session A before you open the session B. Now session B has no memory for what you said before. Therefore, you can feed the history of A to B.</p> <pre><code>string prompt = \"What is C#?\";\n\nawait foreach (var text in session.ChatAsync(prompt, new InferenceParams() { Temperature = 0.6f, AntiPrompts = new List&lt;string&gt; { \"User:\" } })) // the inference params should be changed depending on your statement\n{\nConsole.Write(text);\n}\n</code></pre>"},{"location":"Tutorials/ChatSession/#get-the-history","title":"Get the history","text":"<p>Currently <code>History</code> is a property of <code>ChatSession</code>.</p> <pre><code>foreach(var rec in session.History.Messages)\n{\nConsole.WriteLine($\"{rec.AuthorRole}: {rec.Content}\");\n}\n</code></pre>"},{"location":"Tutorials/ChatSession/#saveload-chat-session","title":"Save/Load Chat Session","text":"<p>Generally, the chat session could be switched, which requires the ability of loading and saving session.</p> <p>The API is also quite simple, the files will be saved into a directory you specified. If the path does not exist, a new directory will be created.</p> <pre><code>string savePath = \"&lt;save dir&gt;\";\nsession.SaveSession(savePath);\n\nsession.LoadSession(savePath, loadTransforms:true);\nsession.LoadSession(savePath, loadTransforms:false);\n</code></pre> <p>You could also keep the state in memory and load them with the following APIs.</p> <pre><code>var sessionState = session.GetSessionState();\nsession.LoadSession(sessionState, loadTransforms:true);\nsession.LoadSession(sessionState, loadTransforms:false);\n\n## Transforms in Chat Session\n\nThere's three important elements in `ChatSession`, which are input, output and history. Besides, there're some conversions between them. Since the process of them under different conditions varies, LLamaSharp hands over this part of the power to the users.\n\nCurrently, there're three kinds of process that could be customized, as introduced below.\n\n### Input transform\n\nIn general, the input of the chat API is a text (without stream), therefore `ChatSession` processes it in a pipeline. If you want to use your customized transform, you need to define a transform that implements `ITextTransform` and add it to the pipeline of `ChatSession`.\n\n```cs\npublic interface ITextTransform\n{\nstring Transform(string text);\n}\n</code></pre> <pre><code>public class MyInputTransform1 : ITextTransform\n{\npublic string Transform(string text)\n{\nreturn $\"Question: {text}\\n\";\n}\n}\n\npublic class MyInputTransform2 : ITextTransform\n{\npublic string Transform(string text)\n{\nreturn text + \"Answer: \";\n}\n}\n\nsession.AddInputTransform(new MyInputTransform1()).AddInputTransform(new MyInputTransform2());\n</code></pre>"},{"location":"Tutorials/ChatSession/#output-transform","title":"Output transform","text":"<p>Different from the input, the output of chat API is a text stream. Therefore you need to process it word by word, instead of getting the full text at once.</p> <p>The interface of it has an <code>IEnumerable&lt;string&gt;</code> as input, which is actually a yield sequence.</p> <pre><code>public interface ITextStreamTransform\n{\nIEnumerable&lt;string&gt; Transform(IEnumerable&lt;string&gt; tokens);\nIAsyncEnumerable&lt;string&gt; TransformAsync(IAsyncEnumerable&lt;string&gt; tokens);\n}\n</code></pre> <p>When implementing it, you could throw a not-implemented exception in one of them if you only need to use the chat API in synchronously or asynchronously.</p> <p>Different from the input transform pipeline, the output transform only supports one transform.</p> <pre><code>session.WithOutputTransform(new MyOutputTransform());\n</code></pre> <p>Here's an example of how to implement the interface. In this example, the transform detects whether there's some keywords in the response and removes them.</p> <pre><code>/// &lt;summary&gt;\n/// A text output transform that removes the keywords from the response.\n/// &lt;/summary&gt;\npublic class KeywordTextOutputStreamTransform : ITextStreamTransform\n{\nHashSet&lt;string&gt; _keywords;\nint _maxKeywordLength;\nbool _removeAllMatchedTokens;\n\n/// &lt;summary&gt;\n/// \n/// &lt;/summary&gt;\n/// &lt;param name=\"keywords\"&gt;Keywords that you want to remove from the response.&lt;/param&gt;\n/// &lt;param name=\"redundancyLength\"&gt;The extra length when searching for the keyword. For example, if your only keyword is \"highlight\", \n/// maybe the token you get is \"\\r\\nhighligt\". In this condition, if redundancyLength=0, the token cannot be successfully matched because the length of \"\\r\\nhighligt\" (10)\n/// has already exceeded the maximum length of the keywords (8). On the contrary, setting redundancyLengyh &gt;= 2 leads to successful match.\n/// The larger the redundancyLength is, the lower the processing speed. But as an experience, it won't introduce too much performance impact when redundancyLength &lt;= 5 &lt;/param&gt;\n/// &lt;param name=\"removeAllMatchedTokens\"&gt;If set to true, when getting a matched keyword, all the related tokens will be removed. Otherwise only the part of keyword will be removed.&lt;/param&gt;\npublic KeywordTextOutputStreamTransform(IEnumerable&lt;string&gt; keywords, int redundancyLength = 3, bool removeAllMatchedTokens = false)\n{\n_keywords = new(keywords);\n_maxKeywordLength = keywords.Select(x =&gt; x.Length).Max() + redundancyLength;\n_removeAllMatchedTokens = removeAllMatchedTokens;\n}\n/// &lt;inheritdoc /&gt;\npublic IEnumerable&lt;string&gt; Transform(IEnumerable&lt;string&gt; tokens)\n{\nvar window = new Queue&lt;string&gt;();\n\nforeach (var s in tokens)\n{\nwindow.Enqueue(s);\nvar current = string.Join(\"\", window);\nif (_keywords.Any(x =&gt; current.Contains(x)))\n{\nvar matchedKeyword = _keywords.First(x =&gt; current.Contains(x));\nint total = window.Count;\nfor (int i = 0; i &lt; total; i++)\n{\nwindow.Dequeue();\n}\nif (!_removeAllMatchedTokens)\n{\nyield return current.Replace(matchedKeyword, \"\");\n}\n}\nif (current.Length &gt;= _maxKeywordLength)\n{\nif (_keywords.Any(x =&gt; current.Contains(x)))\n{\nvar matchedKeyword = _keywords.First(x =&gt; current.Contains(x));\nint total = window.Count;\nfor (int i = 0; i &lt; total; i++)\n{\nwindow.Dequeue();\n}\nif (!_removeAllMatchedTokens)\n{\nyield return current.Replace(matchedKeyword, \"\");\n}\n}\nelse\n{\nint total = window.Count;\nfor (int i = 0; i &lt; total; i++)\n{\nyield return window.Dequeue();\n}\n}\n}\n}\nint totalCount = window.Count;\nfor (int i = 0; i &lt; totalCount; i++)\n{\nyield return window.Dequeue();\n}\n}\n/// &lt;inheritdoc /&gt;\npublic async IAsyncEnumerable&lt;string&gt; TransformAsync(IAsyncEnumerable&lt;string&gt; tokens)\n{\nthrow new NotImplementedException(); // This is implemented in `LLamaTransforms` but we ignore it here.\n}\n}\n</code></pre>"},{"location":"Tutorials/ChatSession/#history-transform","title":"History transform","text":"<p>The chat history could be converted to or from a text, which is exactly what the interface of it.</p> <pre><code>public interface IHistoryTransform\n{\nstring HistoryToText(ChatHistory history);\nChatHistory TextToHistory(AuthorRole role, string text);\n}\n</code></pre> <p>Similar to the output transform, the history transform is added in the following way:</p> <pre><code>session.WithHistoryTransform(new MyHistoryTransform());\n</code></pre> <p>The implementation is quite flexible, depending on what you want the history message to be like. Here's an example, which is the default history transform in LLamaSharp.</p> <pre><code>/// &lt;summary&gt;\n/// The default history transform.\n/// Uses plain text with the following format:\n/// [Author]: [Message]\n/// &lt;/summary&gt;\npublic class DefaultHistoryTransform : IHistoryTransform\n{\nprivate readonly string defaultUserName = \"User\";\nprivate readonly string defaultAssistantName = \"Assistant\";\nprivate readonly string defaultSystemName = \"System\";\nprivate readonly string defaultUnknownName = \"??\";\n\nstring _userName;\nstring _assistantName;\nstring _systemName;\nstring _unknownName;\nbool _isInstructMode;\npublic DefaultHistoryTransform(string? userName = null, string? assistantName = null, string? systemName = null, string? unknownName = null, bool isInstructMode = false)\n{\n_userName = userName ?? defaultUserName;\n_assistantName = assistantName ?? defaultAssistantName;\n_systemName = systemName ?? defaultSystemName;\n_unknownName = unknownName ?? defaultUnknownName;\n_isInstructMode = isInstructMode;\n}\n\npublic virtual string HistoryToText(ChatHistory history)\n{\nStringBuilder sb = new();\nforeach (var message in history.Messages)\n{\nif (message.AuthorRole == AuthorRole.User)\n{\nsb.AppendLine($\"{_userName}: {message.Content}\");\n}\nelse if (message.AuthorRole == AuthorRole.System)\n{\nsb.AppendLine($\"{_systemName}: {message.Content}\");\n}\nelse if (message.AuthorRole == AuthorRole.Unknown)\n{\nsb.AppendLine($\"{_unknownName}: {message.Content}\");\n}\nelse if (message.AuthorRole == AuthorRole.Assistant)\n{\nsb.AppendLine($\"{_assistantName}: {message.Content}\");\n}\n}\nreturn sb.ToString();\n}\n\npublic virtual ChatHistory TextToHistory(AuthorRole role, string text)\n{\nChatHistory history = new ChatHistory();\nhistory.AddMessage(role, TrimNamesFromText(text, role));\nreturn history;\n}\n\npublic virtual string TrimNamesFromText(string text, AuthorRole role)\n{\nif (role == AuthorRole.User &amp;&amp; text.StartsWith($\"{_userName}:\"))\n{\ntext = text.Substring($\"{_userName}:\".Length).TrimStart();\n}\nelse if (role == AuthorRole.Assistant &amp;&amp; text.EndsWith($\"{_assistantName}:\"))\n{\ntext = text.Substring(0, text.Length - $\"{_assistantName}:\".Length).TrimEnd();\n}\nif (_isInstructMode &amp;&amp; role == AuthorRole.Assistant &amp;&amp; text.EndsWith(\"\\n&gt; \"))\n{\ntext = text.Substring(0, text.Length - \"\\n&gt; \".Length).TrimEnd();\n}\nreturn text;\n}\n}\n</code></pre>"},{"location":"Tutorials/Executors/","title":"LLamaSharp executors","text":"<p>LLamaSharp executor defines the behavior of the model when it is called. Currently, there are four kinds of executors, which are <code>InteractiveExecutor</code>, <code>InstructExecutor</code>, <code>StatelessExecutor</code> and <code>BatchedExecutor</code>.</p> <p>In a word, <code>InteractiveExecutor</code> is suitable for getting answer of your questions from LLM continuously. <code>InstructExecutor</code> let LLM execute your instructions, such as \"continue writing\". <code>StatelessExecutor</code> is best for one-time job because the previous inference has no impact on the current inference. <code>BatchedExecutor</code> could accept multiple inputs and generate multiple outputs of different sessions at the same time, significantly improving the throughput of the program.</p>"},{"location":"Tutorials/Executors/#text-to-text-apis-of-the-executors","title":"Text-to-Text APIs of the executors","text":"<p>All the executors implements the interface <code>ILLamaExecutor</code>, which provides two APIs to execute text-to-text tasks.</p> <pre><code>public interface ILLamaExecutor\n{\n/// &lt;summary&gt;\n/// The loaded context for this executor.\n/// &lt;/summary&gt;\npublic LLamaContext Context { get; }\n\n// LLava Section\n//\n/// &lt;summary&gt;\n/// Identify if it's a multi-modal model and there is a image to process.\n/// &lt;/summary&gt;\npublic bool IsMultiModal { get; }\n/// &lt;summary&gt;\n/// Multi-Modal Projections / Clip Model weights\n/// &lt;/summary&gt;\npublic LLavaWeights? ClipModel { get;  }        /// &lt;summary&gt;\n/// List of images: List of images in byte array format.\n/// &lt;/summary&gt;\npublic List&lt;byte[]&gt; Images { get; }\n\n\n/// &lt;summary&gt;\n/// Asynchronously infers a response from the model.\n/// &lt;/summary&gt;\n/// &lt;param name=\"text\"&gt;Your prompt&lt;/param&gt;\n/// &lt;param name=\"inferenceParams\"&gt;Any additional parameters&lt;/param&gt;\n/// &lt;param name=\"token\"&gt;A cancellation token.&lt;/param&gt;\n/// &lt;returns&gt;&lt;/returns&gt;\nIAsyncEnumerable&lt;string&gt; InferAsync(string text, IInferenceParams? inferenceParams = null, CancellationToken token = default);\n}\n</code></pre> <p>The output of both two APIs are yield enumerable. Therefore, when receiving the output, you can directly use <code>foreach</code> to take actions on each word you get by order, instead of waiting for the whole process completed.</p>"},{"location":"Tutorials/Executors/#interactiveexecutor--instructexecutor","title":"InteractiveExecutor &amp; InstructExecutor","text":"<p>Both of them are taking \"completing the prompt\" as the goal to generate the response. For example, if you input <code>Long long ago, there was a fox who wanted to make friend with humen. One day</code>, then the LLM will continue to write the story.</p> <p>Under interactive mode, you serve a role of user and the LLM serves the role of assistant. Then it will help you with your question or request. </p> <p>Under instruct mode, you give LLM some instructions and it follows.</p> <p>Though the behaviors of them sounds similar, it could introduce many differences depending on your prompt. For example, \"chat-with-bob\" has good performance under interactive mode and <code>alpaca</code> does well with instruct mode.</p> <pre><code>// chat-with-bob\n\nTranscript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\n\nUser: Hello, Bob.\nBob: Hello. How may I help you today?\nUser: Please tell me the largest city in Europe.\nBob: Sure. The largest city in Europe is Moscow, the capital of Russia.\nUser:\n</code></pre> <pre><code>// alpaca\n\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n</code></pre> <p>Therefore, please modify the prompt correspondingly when switching from one mode to the other.</p>"},{"location":"Tutorials/Executors/#statelessexecutor","title":"StatelessExecutor.","text":"<p>Despite the differences between interactive mode and instruct mode, both of them are stateful mode. That is, your previous question/instruction will impact on the current response from LLM. On the contrary, the stateless executor does not have such a \"memory\". No matter how many times you talk to it, it will only concentrate on what you say in this time. It is very useful when you want a clean context, without being affected by previous inputs.</p> <p>Since the stateless executor has no memory of conversations before, you need to input your question with the whole prompt into it to get the better answer.</p> <p>For example, if you feed <code>Q: Who is Trump? A:</code> to the stateless executor, it may give the following answer with the antiprompt <code>Q:</code>.</p> <pre><code>Donald J. Trump, born June 14, 1946, is an American businessman, television personality, politician and the 45th President of the United States (2017-2021). # Anexo:Torneo de Hamburgo 2022 (individual masculino)\n\n## Presentaci\u00f3n previa\n\n* Defensor del t\u00edtulo:  Daniil Medv\u00e9dev\n</code></pre> <p>It seems that things went well at first. However, after answering the question itself, LLM began to talk about some other things until the answer reached the token count limit. The reason of this strange behavior is the anti-prompt cannot be match. With the input, LLM cannot decide whether to append a string \"A: \" at the end of the response.</p> <p>As an improvement, let's take the following text as the input:</p> <pre><code>Q: What is the capital of the USA? A: Washingtong. Q: What is the sum of 1 and 2? A: 3. Q: Who is Trump? A: \n</code></pre> <p>Then, I got the following answer with the anti-prompt <code>Q:</code>.</p> <pre><code>45th president of the United States.\n</code></pre> <p>At this time, by repeating the same mode of <code>Q: xxx? A: xxx.</code>, LLM outputs the anti-prompt we want to help to decide where to stop the generation.</p>"},{"location":"Tutorials/Executors/#batchedexecutor","title":"BatchedExecutor","text":"<p>Different from other executors, <code>BatchedExecutor</code> could accept multiple inputs from different sessions and generate outputs for them at the same time. Here is an example to use it.</p> <pre><code>using LLama.Batched;\nusing LLama.Common;\nusing LLama.Native;\nusing LLama.Sampling;\nusing Spectre.Console;\n\nnamespace LLama.Examples.Examples;\n\n/// &lt;summary&gt;\n/// This demonstrates using a batch to generate two sequences and then using one\n/// sequence as the negative guidance (\"classifier free guidance\") for the other.\n/// &lt;/summary&gt;\npublic class BatchedExecutorGuidance\n{\nprivate const int n_len = 32;\n\npublic static async Task Run()\n{\nstring modelPath = UserSettings.GetModelPath();\n\nvar parameters = new ModelParams(modelPath);\nusing var model = LLamaWeights.LoadFromFile(parameters);\n\nvar positivePrompt = AnsiConsole.Ask(\"Positive Prompt (or ENTER for default):\", \"My favourite colour is\").Trim();\nvar negativePrompt = AnsiConsole.Ask(\"Negative Prompt (or ENTER for default):\", \"I hate the colour red. My favourite colour is\").Trim();\nvar weight = AnsiConsole.Ask(\"Guidance Weight (or ENTER for default):\", 2.0f);\n\n// Create an executor that can evaluate a batch of conversations together\nusing var executor = new BatchedExecutor(model, parameters);\n\n// Print some info\nvar name = executor.Model.Metadata.GetValueOrDefault(\"general.name\", \"unknown model name\");\nConsole.WriteLine($\"Created executor with model: {name}\");\n\n// Load the two prompts into two conversations\nusing var guided = executor.Create();\nguided.Prompt(positivePrompt);\nusing var guidance = executor.Create();\nguidance.Prompt(negativePrompt);\n\n// Run inference to evaluate prompts\nawait AnsiConsole\n.Status()\n.Spinner(Spinner.Known.Line)\n.StartAsync(\"Evaluating Prompts...\", _ =&gt; executor.Infer());\n\n// Fork the \"guided\" conversation. We'll run this one without guidance for comparison\nusing var unguided = guided.Fork();\n\n// Run inference loop\nvar unguidedSampler = new GuidedSampler(null, weight);\nvar unguidedDecoder = new StreamingTokenDecoder(executor.Context);\nvar guidedSampler = new GuidedSampler(guidance, weight);\nvar guidedDecoder = new StreamingTokenDecoder(executor.Context);\nawait AnsiConsole\n.Progress()\n.StartAsync(async progress =&gt;\n{\nvar reporter = progress.AddTask(\"Running Inference\", maxValue: n_len);\n\nfor (var i = 0; i &lt; n_len; i++)\n{\nif (i != 0)\nawait executor.Infer();\n\n// Sample from the \"unguided\" conversation. This is just a conversation using the same prompt, without any\n// guidance. This serves as a comparison to show the effect of guidance.\nvar u = unguidedSampler.Sample(executor.Context.NativeHandle, unguided.Sample(), Array.Empty&lt;LLamaToken&gt;());\nunguidedDecoder.Add(u);\nunguided.Prompt(u);\n\n// Sample from the \"guided\" conversation. This sampler will internally use the \"guidance\" conversation\n// to steer the conversation. See how this is done in GuidedSampler.ProcessLogits (bottom of this file).\nvar g = guidedSampler.Sample(executor.Context.NativeHandle, guided.Sample(), Array.Empty&lt;LLamaToken&gt;());\nguidedDecoder.Add(g);\n\n// Use this token to advance both guided _and_ guidance. Keeping them in sync (except for the initial prompt).\nguided.Prompt(g);\nguidance.Prompt(g);\n\n// Early exit if we reach the natural end of the guided sentence\nif (g == model.EndOfSentenceToken)\nbreak;\n\n// Update progress bar\nreporter.Increment(1);\n}\n});\n\nAnsiConsole.MarkupLine($\"[green]Unguided:[/][white]{unguidedDecoder.Read().ReplaceLineEndings(\" \")}[/]\");\nAnsiConsole.MarkupLine($\"[green]Guided:[/][white]{guidedDecoder.Read().ReplaceLineEndings(\" \")}[/]\");\n}\n\nprivate class GuidedSampler(Conversation? guidance, float weight)\n: BaseSamplingPipeline\n{\npublic override void Accept(SafeLLamaContextHandle ctx, LLamaToken token)\n{\n}\n\npublic override ISamplingPipeline Clone()\n{\nthrow new NotSupportedException();\n}\n\nprotected override void ProcessLogits(SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n{\nif (guidance == null)\nreturn;\n\n// Get the logits generated by the guidance sequences\nvar guidanceLogits = guidance.Sample();\n\n// Use those logits to guide this sequence\nNativeApi.llama_sample_apply_guidance(ctx, logits, guidanceLogits, weight);\n}\n\nprotected override LLamaToken ProcessTokenDataArray(SafeLLamaContextHandle ctx, LLamaTokenDataArray candidates, ReadOnlySpan&lt;LLamaToken&gt; lastTokens)\n{\ncandidates.Temperature(ctx, 0.8f);\ncandidates.TopK(ctx, 25);\n\nreturn candidates.SampleToken(ctx);\n}\n}\n}\n</code></pre>"},{"location":"Tutorials/Executors/#inference-parameters","title":"Inference parameters","text":"<p>Different from context parameters, which is indicated in understand-llama-context, executors accept parameters when you call its API to execute the inference. That means you could change the parameters every time you ask the model to generate the outputs.</p> <p>Here is the parameters for LLamaSharp executors.</p> <pre><code>/// &lt;summary&gt;\n/// The parameters used for inference.\n/// &lt;/summary&gt;\npublic record InferenceParams\n: IInferenceParams\n{\n/// &lt;summary&gt;\n/// number of tokens to keep from initial prompt\n/// &lt;/summary&gt;\npublic int TokensKeep { get; set; } = 0;\n\n/// &lt;summary&gt;\n/// how many new tokens to predict (n_predict), set to -1 to inifinitely generate response\n/// until it complete.\n/// &lt;/summary&gt;\npublic int MaxTokens { get; set; } = -1;\n\n/// &lt;summary&gt;\n/// logit bias for specific tokens\n/// &lt;/summary&gt;\npublic Dictionary&lt;LLamaToken, float&gt;? LogitBias { get; set; } = null;\n\n/// &lt;summary&gt;\n/// Sequences where the model will stop generating further tokens.\n/// &lt;/summary&gt;\npublic IReadOnlyList&lt;string&gt; AntiPrompts { get; set; } = Array.Empty&lt;string&gt;();\n\n/// &lt;inheritdoc /&gt;\npublic int TopK { get; set; } = 40;\n\n/// &lt;inheritdoc /&gt;\npublic float TopP { get; set; } = 0.95f;\n\n/// &lt;inheritdoc /&gt;\npublic float MinP { get; set; } = 0.05f;\n\n/// &lt;inheritdoc /&gt;\npublic float TfsZ { get; set; } = 1.0f;\n\n/// &lt;inheritdoc /&gt;\npublic float TypicalP { get; set; } = 1.0f;\n\n/// &lt;inheritdoc /&gt;\npublic float Temperature { get; set; } = 0.8f;\n\n/// &lt;inheritdoc /&gt;\npublic float RepeatPenalty { get; set; } = 1.1f;\n\n/// &lt;inheritdoc /&gt;\npublic int RepeatLastTokensCount { get; set; } = 64;\n\n/// &lt;inheritdoc /&gt;\npublic float FrequencyPenalty { get; set; } = .0f;\n\n/// &lt;inheritdoc /&gt;\npublic float PresencePenalty { get; set; } = .0f;\n\n/// &lt;inheritdoc /&gt;\npublic MirostatType Mirostat { get; set; } = MirostatType.Disable;\n\n/// &lt;inheritdoc /&gt;\npublic float MirostatTau { get; set; } = 5.0f;\n\n/// &lt;inheritdoc /&gt;\npublic float MirostatEta { get; set; } = 0.1f;\n\n/// &lt;inheritdoc /&gt;\npublic bool PenalizeNL { get; set; } = true;\n\n/// &lt;inheritdoc /&gt;\npublic SafeLLamaGrammarHandle? Grammar { get; set; }\n\n/// &lt;inheritdoc /&gt;\npublic ISamplingPipeline? SamplingPipeline { get; set; }\n}\n</code></pre>"},{"location":"Tutorials/Executors/#save-and-load-executor-state","title":"Save and load executor state","text":"<p>An executor also has its state, which can be saved and loaded. That means a lot when you want to support restore a previous session for the user in your application.</p> <p>The following code shows how to use save and load executor state.</p> <pre><code>InteractiveExecutor executor = new InteractiveExecutor(model);\n// do some things...\nexecutor.SaveState(\"executor.st\");\nvar stateData = executor.GetStateData();\n\nInteractiveExecutor executor2 = new InteractiveExecutor(model);\nexecutor2.LoadState(stateData);\n// do some things...\n\nInteractiveExecutor executor3 = new InteractiveExecutor(model);\nexecutor3.LoadState(\"executor.st\");\n// do some things...\n</code></pre>"},{"location":"Tutorials/GetEmbeddings/","title":"Get embeddings","text":"<p>Getting the embeddings of a text in LLM is sometimes useful, for example, to train other MLP models.</p> <p>To get the embeddings, please initialize a <code>LLamaEmbedder</code> and then call <code>GetEmbeddings</code>.</p> <pre><code>var embedder = new LLamaEmbedder(new ModelParams(\"&lt;modelPath&gt;\"));\nstring text = \"hello, LLM.\";\nfloat[] embeddings = embedder.GetEmbeddings(text);\n</code></pre> <p>The output is a float array. Note that the length of the array is related with the model you load. If you just want to get a smaller size embedding, please consider changing a model.</p>"},{"location":"Tutorials/NativeLibraryConfig/","title":"Configure the native library loading","text":"<p>As indicated in Architecture, LLamaSharp uses the native library to run the LLM models. Sometimes you may want to compile the native library yourself, or just dynamically load the library due to the environment of your user of your application. Luckily, since version 0.7.0, dynamic loading of native library has been supported! That allows you to customize the native library loading process.</p>"},{"location":"Tutorials/NativeLibraryConfig/#when-you-should-compile-the-native-library-yourself","title":"When you should compile the native library yourself","text":"<p>Before introducing the way to customize native library loading, please follow the tips below to see if you need to compile the native library yourself, rather than use the published backend packages, which contain native library files for multiple targets.</p> <ol> <li>Your device/environment has not been supported by any published backend packages. For example, vulkan has not been supported yet. In this case, it will mean a lot to open an issue to tell us you are using it. Since our support for new backend will have a delay, you could compile yourself before that.</li> <li>You want to gain the best performance of LLamaSharp. Because LLamaSharp offloads the model to both GPU and CPU, the performance is significantly related with CPU if your GPU memory size is small. AVX (Advanced Vector Extensions) and BLAS (Basic Linear Algebra Subprograms) are the most important ways to accelerate the CPU computation. By default, LLamaSharp disables the support for BLAS and use AVX2 for CUDA backend yet. If you would like to enable BLAS or use AVX 512 along with CUDA, please compile the native library yourself, following the instructions here.</li> <li>You want to debug the c++ code.</li> </ol>"},{"location":"Tutorials/NativeLibraryConfig/#use-nativelibraryconfig","title":"Use NativeLibraryConfig","text":"<p>We provide <code>LLama.Native.NativeLibraryConfig</code> class with singleton mode to allow users to customize the loading process of the native library. Any method of it should be called before the model loading, because a native library file must be decided before any model is loaded.</p>"},{"location":"Tutorials/NativeLibraryConfig/#load-specified-native-library-file","title":"Load specified native library file","text":"<p>All you need to do is adding the following code to the very beginning of your code.</p> <pre><code>NativeLibraryConfig.All.WithLibrary(\"&lt;Your native library path&gt;\");\n</code></pre> <p>If you want to configure the loading for LLama library or llava library respectively, please call the following APIs.</p> <pre><code>NativeLibraryConfig.LLama.WithLibrary(\"&lt;Your llama native library path&gt;\");\nNativeLibraryConfig.LLava.WithLibrary(\"&lt;Your llava native library path&gt;\");\n</code></pre>"},{"location":"Tutorials/NativeLibraryConfig/#automatically-select-one-from-multiple-native-library-files","title":"Automatically select one from multiple native library files","text":"<p>Let's consider this case: you don't know your user's device when distributing your application, so you put all the possible native libraries in a folder and want to select the best one depending on the user's device. LLamaSharp allows you to define the strategy to do it.</p> <ul> <li><code>NativeLibraryConfig.All.WithCuda</code>: decide if you want to use cuda if possible.</li> <li><code>NativeLibraryConfig.All.WithAvx</code>: decide the highest AVX level you want to use if possible.</li> <li><code>NativeLibraryConfig.All.WithSearchDirectory</code>: specify the directory to search the native library files.</li> <li><code>NativeLibraryConfig.All.WithAutoFallback</code>: whether to allow fall back to other options if no native library that matches your specified settings could be found.</li> </ul>"},{"location":"Tutorials/NativeLibraryConfig/#skip-the-check-in-case-of-false-trigger-of-validation","title":"Skip the check in case of false trigger of validation","text":"<p><code>NativeLibraryConfig.All.SkipCheck</code> allows you to skip the checks of the device/environment compatibility. This API will be very useful if your users have an unusual environment. For example, if the user installed the cuda toolkit in a customized path and didn't set the environment variables correctly, it might be regarded as invalid even though the loading could actually succeed.</p>"},{"location":"Tutorials/NativeLibraryConfig/#dry-run-your-configuration","title":"Dry run your configuration","text":"<p>In some conditions, you or your users are not sure if the native library could be successfully loaded with the configuration. To address this issue, <code>NativeLibraryConfig.All.DryRun</code> is provided to try to run the native library loading with the current configuration. It will return whether the loading is successfully, and the loaded library. However, the loaded library is not assigned to the handle of <code>NativeApi</code>. So you could try different configurations until one of them could be loaded successfully.</p> <p>NOTE: Similar to other APIs, <code>NativeLibraryConfig.All.DryRun</code> is only available before calling an arbitrary API in <code>NativeApi</code>, too.</p>"},{"location":"Tutorials/NativeLibraryConfig/#set-the-log-level-of-native-library-loading","title":"Set the log level of native library loading","text":"<pre><code>NativeLibraryConfig.All.WithLogs();\n</code></pre> <p>There are four log levels, which are error, warning, info and debug. If you are not sure if the correct library is selected, please set log level to <code>info</code> to see the full logs.</p>"},{"location":"Tutorials/Quantization/","title":"Quantization","text":"<p>Quantization is significant to accelerate the model inference. Since there's little accuracy (performance) reduction when quantizing the model, get it easy to quantize it!</p> <p>To quantize the model, please call <code>Quantize</code> from <code>LLamaQuantizer</code>, which is a static method.</p> <pre><code>string srcPath = \"&lt;model.bin&gt;\";\nstring dstPath = \"&lt;model_q4_0.bin&gt;\";\nLLamaQuantizer.Quantize(srcPath, dstPath, \"q4_0\");\n// The following overload is also okay.\n// LLamaQuantizer.Quantize(srcPath, dstPath, LLamaFtype.LLAMA_FTYPE_MOSTLY_Q4_0);\n</code></pre> <p>After calling it, a quantized model file will be saved.</p> <p>There're currently the following types of quantization supported:</p> <pre><code>{ \"Q4_0\",   LLAMA_FTYPE_MOSTLY_Q4_0,   \" 3.56G, +0.2166 ppl @ LLaMA-v1-7B\", },\n{ \"Q4_1\",   LLAMA_FTYPE_MOSTLY_Q4_1,   \" 3.90G, +0.1585 ppl @ LLaMA-v1-7B\", },\n{ \"Q5_0\",   LLAMA_FTYPE_MOSTLY_Q5_0,   \" 4.33G, +0.0683 ppl @ LLaMA-v1-7B\", },\n{ \"Q5_1\",   LLAMA_FTYPE_MOSTLY_Q5_1,   \" 4.70G, +0.0349 ppl @ LLaMA-v1-7B\", },\n{ \"IQ2_XXS\",LLAMA_FTYPE_MOSTLY_IQ2_XXS,\" 2.06 bpw quantization\",            },\n{ \"IQ2_XS\", LLAMA_FTYPE_MOSTLY_IQ2_XS, \" 2.31 bpw quantization\",            },\n{ \"IQ2_S\",  LLAMA_FTYPE_MOSTLY_IQ2_S,  \" 2.5  bpw quantization\",            },\n{ \"IQ2_M\",  LLAMA_FTYPE_MOSTLY_IQ2_M,  \" 2.7  bpw quantization\",            },\n{ \"IQ1_S\",  LLAMA_FTYPE_MOSTLY_IQ1_S,  \" 1.56 bpw quantization\",            },\n{ \"IQ1_M\",  LLAMA_FTYPE_MOSTLY_IQ1_M,  \" 1.75 bpw quantization\",            },\n{ \"Q2_K\",   LLAMA_FTYPE_MOSTLY_Q2_K,   \" 2.63G, +0.6717 ppl @ LLaMA-v1-7B\", },\n{ \"Q2_K_S\", LLAMA_FTYPE_MOSTLY_Q2_K_S, \" 2.16G, +9.0634 ppl @ LLaMA-v1-7B\", },\n{ \"IQ3_XXS\",LLAMA_FTYPE_MOSTLY_IQ3_XXS,\" 3.06 bpw quantization\",            },\n{ \"IQ3_S\",  LLAMA_FTYPE_MOSTLY_IQ3_S,  \" 3.44 bpw quantization\",            },\n{ \"IQ3_M\",  LLAMA_FTYPE_MOSTLY_IQ3_M,  \" 3.66 bpw quantization mix\",        },\n{ \"Q3_K\",   LLAMA_FTYPE_MOSTLY_Q3_K_M, \"alias for Q3_K_M\" },\n{ \"IQ3_XS\", LLAMA_FTYPE_MOSTLY_IQ3_XS, \" 3.3 bpw quantization\"   ,          },\n{ \"Q3_K_S\", LLAMA_FTYPE_MOSTLY_Q3_K_S, \" 2.75G, +0.5551 ppl @ LLaMA-v1-7B\", },\n{ \"Q3_K_M\", LLAMA_FTYPE_MOSTLY_Q3_K_M, \" 3.07G, +0.2496 ppl @ LLaMA-v1-7B\", },\n{ \"Q3_K_L\", LLAMA_FTYPE_MOSTLY_Q3_K_L, \" 3.35G, +0.1764 ppl @ LLaMA-v1-7B\", },\n{ \"IQ4_NL\", LLAMA_FTYPE_MOSTLY_IQ4_NL, \" 4.50 bpw non-linear quantization\", },\n{ \"IQ4_XS\", LLAMA_FTYPE_MOSTLY_IQ4_XS, \" 4.25 bpw non-linear quantization\", },\n{ \"Q4_K\",   LLAMA_FTYPE_MOSTLY_Q4_K_M, \"alias for Q4_K_M\", },\n{ \"Q4_K_S\", LLAMA_FTYPE_MOSTLY_Q4_K_S, \" 3.59G, +0.0992 ppl @ LLaMA-v1-7B\", },\n{ \"Q4_K_M\", LLAMA_FTYPE_MOSTLY_Q4_K_M, \" 3.80G, +0.0532 ppl @ LLaMA-v1-7B\", },\n{ \"Q5_K\",   LLAMA_FTYPE_MOSTLY_Q5_K_M, \"alias for Q5_K_M\", },\n{ \"Q5_K_S\", LLAMA_FTYPE_MOSTLY_Q5_K_S, \" 4.33G, +0.0400 ppl @ LLaMA-v1-7B\", },\n{ \"Q5_K_M\", LLAMA_FTYPE_MOSTLY_Q5_K_M, \" 4.45G, +0.0122 ppl @ LLaMA-v1-7B\", },\n{ \"Q6_K\",   LLAMA_FTYPE_MOSTLY_Q6_K,   \" 5.15G, +0.0008 ppl @ LLaMA-v1-7B\", },\n{ \"Q8_0\",   LLAMA_FTYPE_MOSTLY_Q8_0,   \" 6.70G, +0.0004 ppl @ LLaMA-v1-7B\", },\n{ \"F16\",    LLAMA_FTYPE_MOSTLY_F16,    \"13.00G              @ 7B\", },\n{ \"F32\",    LLAMA_FTYPE_ALL_F32,       \"26.00G              @ 7B\", },\n// Note: Ensure COPY comes after F32 to avoid ftype 0 from matching.\n{ \"COPY\",   LLAMA_FTYPE_ALL_F32,       \"only copy tensors, no quantizing\", },\n</code></pre>"},{"location":"Tutorials/UnderstandLLamaContext/","title":"Understand LLamaSharp context","text":"<p><code>LLamaContext</code> is the most important component as a link between native APIs and higher-level APIs. It contains the basic settings for model inference and holds the kv-cache, which could significantly accelerate the model inference. Since <code>LLamaContext</code> is not coupled with <code>LLamaWeights</code>, it's possible to create multiple context based on one piece of model weight. Each <code>ILLamaExecutor</code> will hold a <code>LLamaContext</code> instance, but it's possible to switch to different context in an executor.</p> <p>If your application has multiple sessions, please take care of managing <code>LLamaContext</code>.</p> <p><code>LLamaContext</code> takes the following parameters as its settings. Note that the parameters could not be changed once the context has been created.</p> <pre><code>public interface IContextParams\n{\n/// &lt;summary&gt;\n/// Model context size (n_ctx)\n/// &lt;/summary&gt;\nuint? ContextSize { get; }\n\n/// &lt;summary&gt;\n/// batch size for prompt processing (must be &gt;=32 to use BLAS) (n_batch)\n/// &lt;/summary&gt;\nuint BatchSize { get; }\n\n/// &lt;summary&gt;\n/// Seed for the random number generator (seed)\n/// &lt;/summary&gt;\nuint Seed { get; }\n\n/// &lt;summary&gt;\n/// Whether to use embedding mode. (embedding) Note that if this is set to true, \n/// The LLamaModel won't produce text response anymore.\n/// &lt;/summary&gt;\nbool EmbeddingMode { get; }\n\n/// &lt;summary&gt;\n/// RoPE base frequency (null to fetch from the model)\n/// &lt;/summary&gt;\nfloat? RopeFrequencyBase { get; }\n\n/// &lt;summary&gt;\n/// RoPE frequency scaling factor (null to fetch from the model)\n/// &lt;/summary&gt;\nfloat? RopeFrequencyScale { get; }\n\n/// &lt;summary&gt;\n/// The encoding to use for models\n/// &lt;/summary&gt;\nEncoding Encoding { get; }\n\n/// &lt;summary&gt;\n/// Number of threads (null = autodetect) (n_threads)\n/// &lt;/summary&gt;\nuint? Threads { get; }\n\n/// &lt;summary&gt;\n/// Number of threads to use for batch processing (null = autodetect) (n_threads)\n/// &lt;/summary&gt;\nuint? BatchThreads { get; }\n\n/// &lt;summary&gt;\n/// YaRN extrapolation mix factor (null = from model)\n/// &lt;/summary&gt;\nfloat? YarnExtrapolationFactor { get; }\n\n/// &lt;summary&gt;\n/// YaRN magnitude scaling factor (null = from model)\n/// &lt;/summary&gt;\nfloat? YarnAttentionFactor { get; }\n\n/// &lt;summary&gt;\n/// YaRN low correction dim (null = from model)\n/// &lt;/summary&gt;\nfloat? YarnBetaFast { get; }\n\n/// &lt;summary&gt;\n/// YaRN high correction dim (null = from model)\n/// &lt;/summary&gt;\nfloat? YarnBetaSlow { get; }\n\n/// &lt;summary&gt;\n/// YaRN original context length (null = from model)\n/// &lt;/summary&gt;\nuint? YarnOriginalContext { get; }\n\n/// &lt;summary&gt;\n/// YaRN scaling method to use.\n/// &lt;/summary&gt;\nRopeScalingType? YarnScalingType { get; }\n\n/// &lt;summary&gt;\n/// Override the type of the K cache\n/// &lt;/summary&gt;\nGGMLType? TypeK { get; }\n\n/// &lt;summary&gt;\n/// Override the type of the V cache\n/// &lt;/summary&gt;\nGGMLType? TypeV { get; }\n\n/// &lt;summary&gt;\n/// Whether to disable offloading the KQV cache to the GPU\n/// &lt;/summary&gt;\nbool NoKqvOffload { get; }\n\n/// &lt;summary&gt;\n/// defragment the KV cache if holes/size &amp;gt; defrag_threshold, Set to &amp;lt; 0 to disable (default)\n/// &lt;/summary&gt;\nfloat DefragThreshold { get; }\n\n/// &lt;summary&gt;\n/// Whether to pool (sum) embedding results by sequence id (ignored if no pooling layer)\n/// &lt;/summary&gt;\nbool DoPooling { get; }\n}\n</code></pre> <p><code>LLamaContext</code> has its state, which could be saved and loaded.</p> <pre><code>LLamaContext.SaveState(string filename)\nLLamaContext.GetState()\n</code></pre>"},{"location":"xmldocs/","title":"LLamaSharp","text":""},{"location":"xmldocs/#llama","title":"LLama","text":"<p>AntipromptProcessor</p> <p>ChatSession</p> <p>InstructExecutor</p> <p>InteractiveExecutor</p> <p>LLamaContext</p> <p>LLamaEmbedder</p> <p>LLamaQuantizer</p> <p>LLamaReranker</p> <p>LLamaTemplate</p> <p>LLamaTransforms</p> <p>LLamaWeights</p> <p>LLavaWeights</p> <p>SessionState</p> <p>StatefulExecutorBase</p> <p>StatelessExecutor</p> <p>StreamingTokenDecoder</p>"},{"location":"xmldocs/#llamaabstractions","title":"LLama.Abstractions","text":"<p>IContextParams</p> <p>IHistoryTransform</p> <p>IInferenceParams</p> <p>ILLamaExecutor</p> <p>ILLamaParams</p> <p>IModelParams</p> <p>INativeLibrary</p> <p>INativeLibrarySelectingPolicy</p> <p>ITextStreamTransform</p> <p>ITextTransform</p> <p>LLamaExecutorExtensions</p> <p>MetadataOverride</p> <p>MetadataOverrideConverter</p> <p>TensorBufferOverride</p> <p>TensorSplitsCollection</p> <p>TensorSplitsCollectionConverter</p>"},{"location":"xmldocs/#llamabatched","title":"LLama.Batched","text":"<p>AlreadyPromptedConversationException</p> <p>BatchedExecutor</p> <p>CannotModifyWhileRequiresInferenceException</p> <p>CannotSampleRequiresInferenceException</p> <p>CannotSampleRequiresPromptException</p> <p>CannotSaveWhileRequiresInferenceException</p> <p>Conversation</p> <p>ConversationExtensions</p> <p>ExperimentalBatchedExecutorException</p>"},{"location":"xmldocs/#llamacommon","title":"LLama.Common","text":"<p>AuthorRole</p> <p>ChatHistory</p> <p>FixedSizeQueue&lt;T&gt;</p> <p>InferenceParams</p> <p>MirostatType</p> <p>ModelParams</p>"},{"location":"xmldocs/#llamaexceptions","title":"LLama.Exceptions","text":"<p>GetLogitsInvalidIndexException</p> <p>LLamaDecodeError</p> <p>LoadWeightsFailedException</p> <p>MissingTemplateException</p> <p>RuntimeError</p> <p>TemplateNotFoundException</p>"},{"location":"xmldocs/#llamaextensions","title":"LLama.Extensions","text":"<p>IContextParamsExtensions</p> <p>IModelParamsExtensions</p> <p>SpanNormalizationExtensions</p>"},{"location":"xmldocs/#llamanative","title":"LLama.Native","text":"<p>AvxLevel</p> <p>DecodeResult</p> <p>DefaultNativeLibrarySelectingPolicy</p> <p>EncodeResult</p> <p>GGMLType</p> <p>GPUSplitMode</p> <p>ICustomSampler</p> <p>LLamaAttentionType</p> <p>LLamaBatch</p> <p>LLamaBatchEmbeddings</p> <p>LLamaChatMessage</p> <p>LLamaContextParams</p> <p>LLamaFtype</p> <p>LLamaKvCacheViewSafeHandle</p> <p>LLamaLogitBias</p> <p>LLamaLogLevel</p> <p>LLamaModelKvOverrideType</p> <p>LLamaModelMetadataOverride</p> <p>LLamaModelParams</p> <p>LLamaModelQuantizeParams</p> <p>LLamaModelTensorBufferOverride</p> <p>LLamaNativeBatch</p> <p>LLamaPerfContextTimings</p> <p>LLamaPoolingType</p> <p>LLamaPos</p> <p>LLamaRopeType</p> <p>LLamaSamplerChainParams</p> <p>LLamaSamplingTimings</p> <p>LLamaSeqId</p> <p>LLamaToken</p> <p>LLamaTokenAttr</p> <p>LLamaTokenData</p> <p>LLamaTokenDataArray</p> <p>LLamaTokenDataArrayNative</p> <p>LLamaVocabType</p> <p>LLavaImageEmbed</p> <p>LoraAdapter</p> <p>NativeApi</p> <p>NativeLibraryConfig</p> <p>NativeLibraryConfigContainer</p> <p>NativeLibraryFromPath</p> <p>NativeLibraryMetadata</p> <p>NativeLibraryName</p> <p>NativeLibraryWithAvx</p> <p>NativeLibraryWithCuda</p> <p>NativeLibraryWithMacOrFallback</p> <p>NativeLibraryWithVulkan</p> <p>NativeLogConfig</p> <p>RopeScalingType</p> <p>SafeLLamaContextHandle</p> <p>SafeLLamaHandleBase</p> <p>SafeLlamaModelHandle</p> <p>SafeLLamaSamplerChainHandle</p> <p>SafeLlavaImageEmbedHandle</p> <p>SafeLlavaModelHandle</p> <p>SystemInfo</p> <p>UnknownNativeLibrary</p>"},{"location":"xmldocs/#llamasampling","title":"LLama.Sampling","text":"<p>BaseSamplingPipeline</p> <p>DefaultSamplingPipeline</p> <p>Grammar</p> <p>GreedySamplingPipeline</p> <p>ISamplingPipeline</p> <p>ISamplingPipelineExtensions</p>"},{"location":"xmldocs/#llamatransformers","title":"LLama.Transformers","text":"<p>PromptTemplateTransformer</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/","title":"llama.abstractions.icontextparams","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#icontextparams","title":"IContextParams","text":"<p>Namespace: LLama.Abstractions</p> <p>The parameters for initializing a LLama context from a model.</p> <pre><code>public interface IContextParams\n</code></pre> <p>Attributes NullableContextAttribute</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.icontextparams/#contextsize","title":"ContextSize","text":"<p>Model context size (n_ctx)</p> <pre><code>public abstract Nullable&lt;uint&gt; ContextSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#batchsize","title":"BatchSize","text":"<p>maximum batch size that can be submitted at once (must be &gt;=32 to use BLAS) (n_batch)</p> <pre><code>public abstract uint BatchSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_1","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#ubatchsize","title":"UBatchSize","text":"<p>Physical batch size</p> <pre><code>public abstract uint UBatchSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_2","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#seqmax","title":"SeqMax","text":"<p>max number of sequences (i.e. distinct states for recurrent models)</p> <pre><code>public abstract uint SeqMax { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_3","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#embeddings","title":"Embeddings","text":"<p>If true, extract embeddings (together with logits).</p> <pre><code>public abstract bool Embeddings { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_4","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#ropefrequencybase","title":"RopeFrequencyBase","text":"<p>RoPE base frequency (null to fetch from the model)</p> <pre><code>public abstract Nullable&lt;float&gt; RopeFrequencyBase { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_5","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#ropefrequencyscale","title":"RopeFrequencyScale","text":"<p>RoPE frequency scaling factor (null to fetch from the model)</p> <pre><code>public abstract Nullable&lt;float&gt; RopeFrequencyScale { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_6","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#encoding","title":"Encoding","text":"<p>The encoding to use for models</p> <pre><code>public abstract Encoding Encoding { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_7","title":"Property Value","text":"<p>Encoding</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#threads","title":"Threads","text":"<p>Number of threads (null = autodetect) (n_threads)</p> <pre><code>public abstract Nullable&lt;int&gt; Threads { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_8","title":"Property Value","text":"<p>Nullable&lt;Int32&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#batchthreads","title":"BatchThreads","text":"<p>Number of threads to use for batch processing (null = autodetect) (n_threads)</p> <pre><code>public abstract Nullable&lt;int&gt; BatchThreads { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_9","title":"Property Value","text":"<p>Nullable&lt;Int32&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnextrapolationfactor","title":"YarnExtrapolationFactor","text":"<p>YaRN extrapolation mix factor (null = from model)</p> <pre><code>public abstract Nullable&lt;float&gt; YarnExtrapolationFactor { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_10","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnattentionfactor","title":"YarnAttentionFactor","text":"<p>YaRN magnitude scaling factor (null = from model)</p> <pre><code>public abstract Nullable&lt;float&gt; YarnAttentionFactor { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_11","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnbetafast","title":"YarnBetaFast","text":"<p>YaRN low correction dim (null = from model)</p> <pre><code>public abstract Nullable&lt;float&gt; YarnBetaFast { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_12","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnbetaslow","title":"YarnBetaSlow","text":"<p>YaRN high correction dim (null = from model)</p> <pre><code>public abstract Nullable&lt;float&gt; YarnBetaSlow { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_13","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnoriginalcontext","title":"YarnOriginalContext","text":"<p>YaRN original context length (null = from model)</p> <pre><code>public abstract Nullable&lt;uint&gt; YarnOriginalContext { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_14","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#yarnscalingtype","title":"YarnScalingType","text":"<p>YaRN scaling method to use.</p> <pre><code>public abstract Nullable&lt;RopeScalingType&gt; YarnScalingType { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_15","title":"Property Value","text":"<p>Nullable&lt;RopeScalingType&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#typek","title":"TypeK","text":"<p>Override the type of the K cache</p> <pre><code>public abstract Nullable&lt;GGMLType&gt; TypeK { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_16","title":"Property Value","text":"<p>Nullable&lt;GGMLType&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#typev","title":"TypeV","text":"<p>Override the type of the V cache</p> <pre><code>public abstract Nullable&lt;GGMLType&gt; TypeV { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_17","title":"Property Value","text":"<p>Nullable&lt;GGMLType&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#nokqvoffload","title":"NoKqvOffload","text":"<p>Whether to disable offloading the KQV cache to the GPU</p> <pre><code>public abstract bool NoKqvOffload { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_18","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#flashattention","title":"FlashAttention","text":"<p>Whether to use flash attention</p> <pre><code>public abstract bool FlashAttention { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_19","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#defragthreshold","title":"DefragThreshold","text":"<p>defragment the KV cache if holes/size &gt; defrag_threshold, Set to &lt; 0 to disable (default)  defragment the KV cache if holes/size &gt; defrag_threshold, Set to  or &lt; 0 to disable (default)</p> <pre><code>public abstract Nullable&lt;float&gt; DefragThreshold { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_20","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#poolingtype","title":"PoolingType","text":"<p>How to pool (sum) embedding results by sequence id (ignored if no pooling layer)</p> <pre><code>public abstract LLamaPoolingType PoolingType { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_21","title":"Property Value","text":"<p>LLamaPoolingType</p>"},{"location":"xmldocs/llama.abstractions.icontextparams/#attentiontype","title":"AttentionType","text":"<p>Attention type to use for embeddings</p> <pre><code>public abstract LLamaAttentionType AttentionType { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.icontextparams/#property-value_22","title":"Property Value","text":"<p>LLamaAttentionType</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/","title":"llama.abstractions.ihistorytransform","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#ihistorytransform","title":"IHistoryTransform","text":"<p>Namespace: LLama.Abstractions</p> <p>Transform history to plain text and vice versa.</p> <pre><code>public interface IHistoryTransform\n</code></pre> <p>Attributes NullableContextAttribute, JsonConverterAttribute</p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.ihistorytransform/#historytotextchathistory","title":"HistoryToText(ChatHistory)","text":"<p>Convert a ChatHistory instance to plain text.</p> <pre><code>string HistoryToText(ChatHistory history)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#parameters","title":"Parameters","text":"<p><code>history</code> ChatHistory The ChatHistory instance</p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#texttohistoryauthorrole-string","title":"TextToHistory(AuthorRole, String)","text":"<p>Converts plain text to a ChatHistory instance.</p> <pre><code>ChatHistory TextToHistory(AuthorRole role, string text)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#parameters_1","title":"Parameters","text":"<p><code>role</code> AuthorRole The role for the author.</p> <p><code>text</code> String The chat history as plain text.</p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#returns_1","title":"Returns","text":"<p>ChatHistory The updated history.</p>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#clone","title":"Clone()","text":"<p>Copy the transform.</p> <pre><code>IHistoryTransform Clone()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.ihistorytransform/#returns_2","title":"Returns","text":"<p>IHistoryTransform</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/","title":"llama.abstractions.iinferenceparams","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#iinferenceparams","title":"IInferenceParams","text":"<p>Namespace: LLama.Abstractions</p> <p>The parameters used for inference.</p> <pre><code>public interface IInferenceParams\n</code></pre> <p>Attributes NullableContextAttribute</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.iinferenceparams/#tokenskeep","title":"TokensKeep","text":"<p>number of tokens to keep from initial prompt</p> <pre><code>public abstract int TokensKeep { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#maxtokens","title":"MaxTokens","text":"<p>how many new tokens to predict (n_predict), set to -1 to inifinitely generate response  until it complete.</p> <pre><code>public abstract int MaxTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#antiprompts","title":"AntiPrompts","text":"<p>Sequences where the model will stop generating further tokens.</p> <pre><code>public abstract IReadOnlyList&lt;string&gt; AntiPrompts { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_2","title":"Property Value","text":"<p>IReadOnlyList&lt;String&gt;</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#samplingpipeline","title":"SamplingPipeline","text":"<p>Set a custom sampling pipeline to use.</p> <pre><code>public abstract ISamplingPipeline SamplingPipeline { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_3","title":"Property Value","text":"<p>ISamplingPipeline</p>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#decodespecialtokens","title":"DecodeSpecialTokens","text":"<p>If true, special characters will be converted to text. If false they will be invisible.</p> <pre><code>public abstract bool DecodeSpecialTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.iinferenceparams/#property-value_4","title":"Property Value","text":"<p>Boolean</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/","title":"llama.abstractions.illamaexecutor","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#illamaexecutor","title":"ILLamaExecutor","text":"<p>Namespace: LLama.Abstractions</p> <p>A high level interface for LLama models.</p> <pre><code>public interface ILLamaExecutor\n</code></pre> <p>Attributes NullableContextAttribute</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.illamaexecutor/#context","title":"Context","text":"<p>The loaded context for this executor.</p> <pre><code>public abstract LLamaContext Context { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#property-value","title":"Property Value","text":"<p>LLamaContext</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#ismultimodal","title":"IsMultiModal","text":"<p>Identify if it's a multi-modal model and there is a image to process.</p> <pre><code>public abstract bool IsMultiModal { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#clipmodel","title":"ClipModel","text":"<p>Multi-Modal Projections / Clip Model weights</p> <pre><code>public abstract LLavaWeights ClipModel { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#property-value_2","title":"Property Value","text":"<p>LLavaWeights</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#images","title":"Images","text":"<p>List of images: List of images in byte array format.</p> <pre><code>public abstract List&lt;Byte[]&gt; Images { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#property-value_3","title":"Property Value","text":"<p>List&lt;Byte[]&gt;</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.illamaexecutor/#inferasyncstring-iinferenceparams-cancellationtoken","title":"InferAsync(String, IInferenceParams, CancellationToken)","text":"<p>Asynchronously infers a response from the model.</p> <pre><code>IAsyncEnumerable&lt;string&gt; InferAsync(string text, IInferenceParams inferenceParams, CancellationToken token)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#parameters","title":"Parameters","text":"<p><code>text</code> String Your prompt</p> <p><code>inferenceParams</code> IInferenceParams Any additional parameters</p> <p><code>token</code> CancellationToken A cancellation token.</p>"},{"location":"xmldocs/llama.abstractions.illamaexecutor/#returns","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.illamaparams/","title":"llama.abstractions.illamaparams","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.illamaparams/#illamaparams","title":"ILLamaParams","text":"<p>Namespace: LLama.Abstractions</p> <p>Convenience interface for implementing both type of parameters.</p> <pre><code>public interface ILLamaParams : IModelParams, IContextParams\n</code></pre> <p>Implements IModelParams, IContextParams</p> <p>Remarks:</p> <p>Mostly exists for backwards compatibility reasons, when these two were not split.</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.imodelparams/","title":"llama.abstractions.imodelparams","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#imodelparams","title":"IModelParams","text":"<p>Namespace: LLama.Abstractions</p> <p>The parameters for initializing a LLama model.</p> <pre><code>public interface IModelParams\n</code></pre> <p>Attributes NullableContextAttribute</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.imodelparams/#maingpu","title":"MainGpu","text":"<p>main_gpu interpretation depends on split_mode:</p> <ul> <li>None - The GPU that is used for the entire mode.</li> <li>Row - The GPU that is used for small tensors and intermediate results.</li> <li>Layer - Ignored.</li> </ul> <pre><code>public abstract int MainGpu { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#splitmode","title":"SplitMode","text":"<p>How to split the model across multiple GPUs</p> <pre><code>public abstract Nullable&lt;GPUSplitMode&gt; SplitMode { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_1","title":"Property Value","text":"<p>Nullable&lt;GPUSplitMode&gt;</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#tensorbufferoverrides","title":"TensorBufferOverrides","text":"<p>Buffer type overrides for specific tensor patterns, allowing you to specify hardware devices to use for individual tensors or sets of tensors.  Equivalent to --override-tensor or -ot on the llama.cpp command line or tensor_buft_overrides internally.</p> <pre><code>public abstract List&lt;TensorBufferOverride&gt; TensorBufferOverrides { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_2","title":"Property Value","text":"<p>List&lt;TensorBufferOverride&gt;</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#gpulayercount","title":"GpuLayerCount","text":"<p>Number of layers to run in VRAM / GPU memory (n_gpu_layers)</p> <pre><code>public abstract int GpuLayerCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_3","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#usememorymap","title":"UseMemorymap","text":"<p>Use mmap for faster loads (use_mmap)</p> <pre><code>public abstract bool UseMemorymap { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_4","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#usememorylock","title":"UseMemoryLock","text":"<p>Use mlock to keep model in memory (use_mlock)</p> <pre><code>public abstract bool UseMemoryLock { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_5","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#modelpath","title":"ModelPath","text":"<p>Model path (model)</p> <pre><code>public abstract string ModelPath { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_6","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#tensorsplits","title":"TensorSplits","text":"<p>how split tensors should be distributed across GPUs</p> <pre><code>public abstract TensorSplitsCollection TensorSplits { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_7","title":"Property Value","text":"<p>TensorSplitsCollection</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#vocabonly","title":"VocabOnly","text":"<p>Load vocab only (no weights)</p> <pre><code>public abstract bool VocabOnly { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_8","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#checktensors","title":"CheckTensors","text":"<p>Validate model tensor data before loading</p> <pre><code>public abstract bool CheckTensors { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_9","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.imodelparams/#metadataoverrides","title":"MetadataOverrides","text":"<p>Override specific metadata items in the model</p> <pre><code>public abstract List&lt;MetadataOverride&gt; MetadataOverrides { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.imodelparams/#property-value_10","title":"Property Value","text":"<p>List&lt;MetadataOverride&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.inativelibrary/","title":"Llama.abstractions.inativelibrary","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.inativelibrary/#inativelibrary","title":"INativeLibrary","text":"<p>Namespace: LLama.Abstractions</p> <p>Descriptor of a native library.</p> <pre><code>public interface INativeLibrary\n</code></pre> <p>Attributes NullableContextAttribute</p>"},{"location":"xmldocs/llama.abstractions.inativelibrary/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.inativelibrary/#metadata","title":"Metadata","text":"<p>Metadata of this library.</p> <pre><code>public abstract NativeLibraryMetadata Metadata { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.inativelibrary/#property-value","title":"Property Value","text":"<p>NativeLibraryMetadata</p>"},{"location":"xmldocs/llama.abstractions.inativelibrary/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.inativelibrary/#preparesysteminfo-llamalogcallback","title":"Prepare(SystemInfo, LLamaLogCallback)","text":"<p>Prepare the native library file and returns the local path of it.  If it's a relative path, LLamaSharp will search the path in the search directies you set.</p> <pre><code>IEnumerable&lt;string&gt; Prepare(SystemInfo systemInfo, LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.inativelibrary/#parameters","title":"Parameters","text":"<p><code>systemInfo</code> SystemInfo The system information of the current machine.</p> <p><code>logCallback</code> LLamaLogCallback The log callback.</p>"},{"location":"xmldocs/llama.abstractions.inativelibrary/#returns","title":"Returns","text":"<p>IEnumerable&lt;String&gt; The relative paths of the library. You could return multiple paths to try them one by one. If no file is available, please return an empty array.</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.inativelibraryselectingpolicy/","title":"Llama.abstractions.inativelibraryselectingpolicy","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.inativelibraryselectingpolicy/#inativelibraryselectingpolicy","title":"INativeLibrarySelectingPolicy","text":"<p>Namespace: LLama.Abstractions</p> <p>Decides the selected native library that should be loaded according to the configurations.</p> <pre><code>public interface INativeLibrarySelectingPolicy\n</code></pre> <p>Attributes NullableContextAttribute</p>"},{"location":"xmldocs/llama.abstractions.inativelibraryselectingpolicy/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.inativelibraryselectingpolicy/#applydescription-systeminfo-llamalogcallback","title":"Apply(Description, SystemInfo, LLamaLogCallback)","text":"<p>Select the native library.</p> <pre><code>IEnumerable&lt;INativeLibrary&gt; Apply(Description description, SystemInfo systemInfo, LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.inativelibraryselectingpolicy/#parameters","title":"Parameters","text":"<p><code>description</code> Description</p> <p><code>systemInfo</code> SystemInfo The system information of the current machine.</p> <p><code>logCallback</code> LLamaLogCallback The log callback.</p>"},{"location":"xmldocs/llama.abstractions.inativelibraryselectingpolicy/#returns","title":"Returns","text":"<p>IEnumerable&lt;INativeLibrary&gt; The information of the selected native library files, in order by priority from the beginning to the end.</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/","title":"llama.abstractions.itextstreamtransform","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#itextstreamtransform","title":"ITextStreamTransform","text":"<p>Namespace: LLama.Abstractions</p> <p>Takes a stream of tokens and transforms them.</p> <pre><code>public interface ITextStreamTransform\n</code></pre> <p>Attributes NullableContextAttribute, JsonConverterAttribute</p>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#transformasynciasyncenumerablestring","title":"TransformAsync(IAsyncEnumerable&lt;String&gt;)","text":"<p>Takes a stream of tokens and transforms them, returning a new stream of tokens asynchronously.</p> <pre><code>IAsyncEnumerable&lt;string&gt; TransformAsync(IAsyncEnumerable&lt;string&gt; tokens)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#parameters","title":"Parameters","text":"<p><code>tokens</code> IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#returns","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#clone","title":"Clone()","text":"<p>Copy the transform.</p> <pre><code>ITextStreamTransform Clone()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.itextstreamtransform/#returns_1","title":"Returns","text":"<p>ITextStreamTransform</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.itexttransform/","title":"llama.abstractions.itexttransform","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.itexttransform/#itexttransform","title":"ITextTransform","text":"<p>Namespace: LLama.Abstractions</p> <p>An interface for text transformations.  These can be used to compose a pipeline of text transformations, such as:  - Tokenization  - Lowercasing  - Punctuation removal  - Trimming  - etc.</p> <pre><code>public interface ITextTransform\n</code></pre> <p>Attributes NullableContextAttribute, JsonConverterAttribute</p>"},{"location":"xmldocs/llama.abstractions.itexttransform/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.itexttransform/#transformstring","title":"Transform(String)","text":"<p>Takes a string and transforms it.</p> <pre><code>string Transform(string text)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.itexttransform/#parameters","title":"Parameters","text":"<p><code>text</code> String</p>"},{"location":"xmldocs/llama.abstractions.itexttransform/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.itexttransform/#clone","title":"Clone()","text":"<p>Copy the transform.</p> <pre><code>ITextTransform Clone()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.itexttransform/#returns_1","title":"Returns","text":"<p>ITextTransform</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.llamaexecutorextensions/","title":"Llama.abstractions.llamaexecutorextensions","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.llamaexecutorextensions/#llamaexecutorextensions","title":"LLamaExecutorExtensions","text":"<p>Namespace: LLama.Abstractions</p> <p>Extension methods to the LLamaExecutorExtensions interface.</p> <pre><code>public static class LLamaExecutorExtensions\n</code></pre> <p>Inheritance Object \u2192 LLamaExecutorExtensions Attributes NullableContextAttribute, NullableAttribute, ExtensionAttribute</p>"},{"location":"xmldocs/llama.abstractions.llamaexecutorextensions/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.llamaexecutorextensions/#aschatclientillamaexecutor-ihistorytransform-itextstreamtransform","title":"AsChatClient(ILLamaExecutor, IHistoryTransform, ITextStreamTransform)","text":"<p>Gets an  instance for the specified ILLamaExecutor.</p> <pre><code>public static IChatClient AsChatClient(ILLamaExecutor executor, IHistoryTransform historyTransform, ITextStreamTransform outputTransform)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.llamaexecutorextensions/#parameters","title":"Parameters","text":"<p><code>executor</code> ILLamaExecutor The executor.</p> <p><code>historyTransform</code> IHistoryTransform The IHistoryTransform to use to transform an input list messages into a prompt.</p> <p><code>outputTransform</code> ITextStreamTransform The ITextStreamTransform to use to transform the output into text.</p>"},{"location":"xmldocs/llama.abstractions.llamaexecutorextensions/#returns","title":"Returns","text":"<p>IChatClient An  instance for the provided ILLamaExecutor.</p>"},{"location":"xmldocs/llama.abstractions.llamaexecutorextensions/#exceptions","title":"Exceptions","text":"<p>ArgumentNullException <code>executor</code> is null.</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/","title":"llama.abstractions.metadataoverride","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#metadataoverride","title":"MetadataOverride","text":"<p>Namespace: LLama.Abstractions</p> <p>An override for a single key/value pair in model metadata</p> <pre><code>public sealed class MetadataOverride : System.IEquatable`1[[LLama.Abstractions.MetadataOverride, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 MetadataOverride Implements IEquatable&lt;MetadataOverride&gt; Attributes NullableContextAttribute, NullableAttribute, JsonConverterAttribute</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.metadataoverride/#key","title":"Key","text":"<p>Get the key being overridden by this override</p> <pre><code>public string Key { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.abstractions.metadataoverride/#metadataoverridestring-int32","title":"MetadataOverride(String, Int32)","text":"<p>Create a new override for an int key</p> <pre><code>public MetadataOverride(string key, int value)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters","title":"Parameters","text":"<p><code>key</code> String</p> <p><code>value</code> Int32</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#metadataoverridestring-single","title":"MetadataOverride(String, Single)","text":"<p>Create a new override for a float key</p> <pre><code>public MetadataOverride(string key, float value)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters_1","title":"Parameters","text":"<p><code>key</code> String</p> <p><code>value</code> Single</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#metadataoverridestring-boolean","title":"MetadataOverride(String, Boolean)","text":"<p>Create a new override for a boolean key</p> <pre><code>public MetadataOverride(string key, bool value)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters_2","title":"Parameters","text":"<p><code>key</code> String</p> <p><code>value</code> Boolean</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#metadataoverridestring-string","title":"MetadataOverride(String, String)","text":"<p>Create a new override for a string key</p> <pre><code>public MetadataOverride(string key, string value)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters_3","title":"Parameters","text":"<p><code>key</code> String</p> <p><code>value</code> String</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.metadataoverride/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#returns_1","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters_4","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#equalsmetadataoverride","title":"Equals(MetadataOverride)","text":"<pre><code>public bool Equals(MetadataOverride other)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#parameters_5","title":"Parameters","text":"<p><code>other</code> MetadataOverride</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public MetadataOverride &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverride/#returns_4","title":"Returns","text":"<p>MetadataOverride</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/","title":"llama.abstractions.metadataoverrideconverter","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#metadataoverrideconverter","title":"MetadataOverrideConverter","text":"<p>Namespace: LLama.Abstractions</p> <p>A JSON converter for MetadataOverride</p> <pre><code>public class MetadataOverrideConverter : System.Text.Json.Serialization.JsonConverter`1[[LLama.Abstractions.MetadataOverride, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 JsonConverter \u2192 JsonConverter&lt;MetadataOverride&gt; \u2192 MetadataOverrideConverter Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#handlenull","title":"HandleNull","text":"<pre><code>public bool HandleNull { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#type","title":"Type","text":"<pre><code>public Type Type { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#property-value_1","title":"Property Value","text":"<p>Type</p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#metadataoverrideconverter_1","title":"MetadataOverrideConverter()","text":"<pre><code>public MetadataOverrideConverter()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#readutf8jsonreader-type-jsonserializeroptions","title":"Read(Utf8JsonReader&amp;, Type, JsonSerializerOptions)","text":"<pre><code>public MetadataOverride Read(Utf8JsonReader&amp; reader, Type typeToConvert, JsonSerializerOptions options)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#parameters","title":"Parameters","text":"<p><code>reader</code> Utf8JsonReader&amp;</p> <p><code>typeToConvert</code> Type</p> <p><code>options</code> JsonSerializerOptions</p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#returns","title":"Returns","text":"<p>MetadataOverride</p>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#writeutf8jsonwriter-metadataoverride-jsonserializeroptions","title":"Write(Utf8JsonWriter, MetadataOverride, JsonSerializerOptions)","text":"<pre><code>public void Write(Utf8JsonWriter writer, MetadataOverride value, JsonSerializerOptions options)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.metadataoverrideconverter/#parameters_1","title":"Parameters","text":"<p><code>writer</code> Utf8JsonWriter</p> <p><code>value</code> MetadataOverride</p> <p><code>options</code> JsonSerializerOptions</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.tensorbufferoverride/","title":"Llama.abstractions.tensorbufferoverride","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.tensorbufferoverride/#tensorbufferoverride","title":"TensorBufferOverride","text":"<p>Namespace: LLama.Abstractions</p> <p>Represents a mapping between a tensor name pattern and a specific buffer type</p> <pre><code>public class TensorBufferOverride\n</code></pre> <p>Inheritance Object \u2192 TensorBufferOverride Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.abstractions.tensorbufferoverride/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.tensorbufferoverride/#pattern","title":"Pattern","text":"<p>Pattern to match tensor names. This is a regular expression. You can check the tensor names via the model.Metadata.</p> <pre><code>public string Pattern { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorbufferoverride/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.tensorbufferoverride/#buffertype","title":"BufferType","text":"<p>Buffer type to use for matching tensors. Examples: CPU, GPU0, GPU1</p> <pre><code>public string BufferType { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorbufferoverride/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.abstractions.tensorbufferoverride/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.abstractions.tensorbufferoverride/#tensorbufferoverridestring-string","title":"TensorBufferOverride(String, String)","text":"<p>Creates a new tensor buffer override</p> <pre><code>public TensorBufferOverride(string pattern, string bufferType)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorbufferoverride/#parameters","title":"Parameters","text":"<p><code>pattern</code> String Pattern to match tensor names</p> <p><code>bufferType</code> String Buffer type to use for matching tensors</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/","title":"llama.abstractions.tensorsplitscollection","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#tensorsplitscollection","title":"TensorSplitsCollection","text":"<p>Namespace: LLama.Abstractions</p> <p>A fixed size array to set the tensor splits across multiple GPUs</p> <pre><code>public sealed class TensorSplitsCollection : System.Collections.Generic.IEnumerable`1[[System.Single, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], System.Collections.IEnumerable\n</code></pre> <p>Inheritance Object \u2192 TensorSplitsCollection Implements IEnumerable&lt;Single&gt;, IEnumerable Attributes NullableContextAttribute, NullableAttribute, DefaultMemberAttribute, JsonConverterAttribute</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#length","title":"Length","text":"<p>The size of this array</p> <pre><code>public int Length { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#item","title":"Item","text":"<pre><code>public float Item { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#property-value_1","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#tensorsplitscollectionsingle","title":"TensorSplitsCollection(Single[])","text":"<p>Create a new tensor splits collection, copying the given values</p> <pre><code>public TensorSplitsCollection(Single[] splits)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#parameters","title":"Parameters","text":"<p><code>splits</code> Single[]</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#exceptions","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#tensorsplitscollection_1","title":"TensorSplitsCollection()","text":"<p>Create a new tensor splits collection with all values initialised to the default</p> <pre><code>public TensorSplitsCollection()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#clear","title":"Clear()","text":"<p>Set all values to zero</p> <pre><code>public void Clear()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#getenumerator","title":"GetEnumerator()","text":"<pre><code>public IEnumerator&lt;float&gt; GetEnumerator()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollection/#returns","title":"Returns","text":"<p>IEnumerator&lt;Single&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/","title":"llama.abstractions.tensorsplitscollectionconverter","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#tensorsplitscollectionconverter","title":"TensorSplitsCollectionConverter","text":"<p>Namespace: LLama.Abstractions</p> <p>A JSON converter for TensorSplitsCollection</p> <pre><code>public class TensorSplitsCollectionConverter : System.Text.Json.Serialization.JsonConverter`1[[LLama.Abstractions.TensorSplitsCollection, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 JsonConverter \u2192 JsonConverter&lt;TensorSplitsCollection&gt; \u2192 TensorSplitsCollectionConverter Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#handlenull","title":"HandleNull","text":"<pre><code>public bool HandleNull { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#type","title":"Type","text":"<pre><code>public Type Type { get; }\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#property-value_1","title":"Property Value","text":"<p>Type</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#tensorsplitscollectionconverter_1","title":"TensorSplitsCollectionConverter()","text":"<pre><code>public TensorSplitsCollectionConverter()\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#readutf8jsonreader-type-jsonserializeroptions","title":"Read(Utf8JsonReader&amp;, Type, JsonSerializerOptions)","text":"<pre><code>public TensorSplitsCollection Read(Utf8JsonReader&amp; reader, Type typeToConvert, JsonSerializerOptions options)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#parameters","title":"Parameters","text":"<p><code>reader</code> Utf8JsonReader&amp;</p> <p><code>typeToConvert</code> Type</p> <p><code>options</code> JsonSerializerOptions</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#returns","title":"Returns","text":"<p>TensorSplitsCollection</p>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#writeutf8jsonwriter-tensorsplitscollection-jsonserializeroptions","title":"Write(Utf8JsonWriter, TensorSplitsCollection, JsonSerializerOptions)","text":"<pre><code>public void Write(Utf8JsonWriter writer, TensorSplitsCollection value, JsonSerializerOptions options)\n</code></pre>"},{"location":"xmldocs/llama.abstractions.tensorsplitscollectionconverter/#parameters_1","title":"Parameters","text":"<p><code>writer</code> Utf8JsonWriter</p> <p><code>value</code> TensorSplitsCollection</p> <p><code>options</code> JsonSerializerOptions</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.antipromptprocessor/","title":"llama.antipromptprocessor","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.antipromptprocessor/#antipromptprocessor","title":"AntipromptProcessor","text":"<p>Namespace: LLama</p> <p>AntipromptProcessor keeps track of past tokens looking for any set Anti-Prompts</p> <pre><code>public sealed class AntipromptProcessor\n</code></pre> <p>Inheritance Object \u2192 AntipromptProcessor Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.antipromptprocessor/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.antipromptprocessor/#antipromptprocessorienumerablestring","title":"AntipromptProcessor(IEnumerable&lt;String&gt;)","text":"<p>Initializes a new instance of the AntipromptProcessor class.</p> <pre><code>public AntipromptProcessor(IEnumerable&lt;string&gt; antiprompts)\n</code></pre>"},{"location":"xmldocs/llama.antipromptprocessor/#parameters","title":"Parameters","text":"<p><code>antiprompts</code> IEnumerable&lt;String&gt; The antiprompts.</p>"},{"location":"xmldocs/llama.antipromptprocessor/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.antipromptprocessor/#addantipromptstring","title":"AddAntiprompt(String)","text":"<p>Add an antiprompt to the collection</p> <pre><code>public void AddAntiprompt(string antiprompt)\n</code></pre>"},{"location":"xmldocs/llama.antipromptprocessor/#parameters_1","title":"Parameters","text":"<p><code>antiprompt</code> String</p>"},{"location":"xmldocs/llama.antipromptprocessor/#setantipromptsienumerablestring","title":"SetAntiprompts(IEnumerable&lt;String&gt;)","text":"<p>Overwrite all current antiprompts with a new set</p> <pre><code>public void SetAntiprompts(IEnumerable&lt;string&gt; antiprompts)\n</code></pre>"},{"location":"xmldocs/llama.antipromptprocessor/#parameters_2","title":"Parameters","text":"<p><code>antiprompts</code> IEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.antipromptprocessor/#addstring","title":"Add(String)","text":"<p>Add some text and check if the buffer now ends with any antiprompt</p> <pre><code>public bool Add(string text)\n</code></pre>"},{"location":"xmldocs/llama.antipromptprocessor/#parameters_3","title":"Parameters","text":"<p><code>text</code> String</p>"},{"location":"xmldocs/llama.antipromptprocessor/#returns","title":"Returns","text":"<p>Boolean true if the text buffer ends with any antiprompt</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/","title":"llama.batched.alreadypromptedconversationexception","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#alreadypromptedconversationexception","title":"AlreadyPromptedConversationException","text":"<p>Namespace: LLama.Batched</p> <p>This exception is thrown when \"Prompt()\" is called on a Conversation which has  already been prompted and before \"Infer()\" has been called on the associated  BatchedExecutor.</p> <pre><code>public class AlreadyPromptedConversationException : ExperimentalBatchedExecutorException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException \u2192 AlreadyPromptedConversationException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#events","title":"Events","text":""},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.batched.alreadypromptedconversationexception/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.batchedexecutor/","title":"llama.batched.batchedexecutor","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#batchedexecutor","title":"BatchedExecutor","text":"<p>Namespace: LLama.Batched</p> <p>A batched executor that can infer multiple separate \"conversations\" simultaneously.</p> <pre><code>public sealed class BatchedExecutor : System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 BatchedExecutor Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.batchedexecutor/#context","title":"Context","text":"<p>The LLamaContext this executor is using</p> <pre><code>public LLamaContext Context { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#property-value","title":"Property Value","text":"<p>LLamaContext</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#model","title":"Model","text":"<p>The LLamaWeights this executor is using</p> <pre><code>public LLamaWeights Model { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#property-value_1","title":"Property Value","text":"<p>LLamaWeights</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#batchedtokencount","title":"BatchedTokenCount","text":"<p>Get the number of tokens in the batch, waiting for BatchedExecutor.Infer(CancellationToken) to be called</p> <pre><code>public int BatchedTokenCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#batchqueuecount","title":"BatchQueueCount","text":"<p>Number of batches in the queue, waiting for BatchedExecutor.Infer(CancellationToken) to be called</p> <pre><code>public int BatchQueueCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#property-value_3","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#isdisposed","title":"IsDisposed","text":"<p>Check if this executor has been disposed.</p> <pre><code>public bool IsDisposed { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#property-value_4","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.batched.batchedexecutor/#batchedexecutorllamaweights-icontextparams","title":"BatchedExecutor(LLamaWeights, IContextParams)","text":"<p>Create a new batched executor</p> <pre><code>public BatchedExecutor(LLamaWeights model, IContextParams contextParams)\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#parameters","title":"Parameters","text":"<p><code>model</code> LLamaWeights The model to use</p> <p><code>contextParams</code> IContextParams Parameters to create a new context</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.batched.batchedexecutor/#create","title":"Create()","text":"<p>Start a new Conversation</p> <pre><code>public Conversation Create()\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#returns","title":"Returns","text":"<p>Conversation</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#loadstring","title":"Load(String)","text":"<p>Load a conversation that was previously saved to a file. Once loaded the conversation will  need to be prompted.</p> <pre><code>public Conversation Load(string filepath)\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#parameters_1","title":"Parameters","text":"<p><code>filepath</code> String</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#returns_1","title":"Returns","text":"<p>Conversation</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#exceptions","title":"Exceptions","text":"<p>ObjectDisposedException</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#loadstate","title":"Load(State)","text":"<p>Load a conversation that was previously saved into memory. Once loaded the conversation will need to be prompted.</p> <pre><code>public Conversation Load(State state)\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#parameters_2","title":"Parameters","text":"<p><code>state</code> State</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#returns_2","title":"Returns","text":"<p>Conversation</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#exceptions_1","title":"Exceptions","text":"<p>ObjectDisposedException</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#infercancellationtoken","title":"Infer(CancellationToken)","text":"<p>Run inference for all conversations in the batch which have pending tokens.</p> <p>If the result is <code>NoKvSlot</code> then there is not enough memory for inference, try disposing some conversation  threads and running inference again.</p> <pre><code>public Task&lt;DecodeResult&gt; Infer(CancellationToken cancellation)\n</code></pre>"},{"location":"xmldocs/llama.batched.batchedexecutor/#parameters_3","title":"Parameters","text":"<p><code>cancellation</code> CancellationToken</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#returns_3","title":"Returns","text":"<p>Task&lt;DecodeResult&gt;</p>"},{"location":"xmldocs/llama.batched.batchedexecutor/#dispose","title":"Dispose()","text":"<pre><code>public void Dispose()\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/","title":"llama.batched.cannotmodifywhilerequiresinferenceexception","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#cannotmodifywhilerequiresinferenceexception","title":"CannotModifyWhileRequiresInferenceException","text":"<p>Namespace: LLama.Batched</p> <p>This exception is thrown when Conversation.Modify(ModifyKvCache) is called when Conversation.RequiresInference = true</p> <pre><code>public class CannotModifyWhileRequiresInferenceException : ExperimentalBatchedExecutorException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException \u2192 CannotModifyWhileRequiresInferenceException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#events","title":"Events","text":""},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/","title":"llama.batched.cannotsamplerequiresinferenceexception","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#cannotsamplerequiresinferenceexception","title":"CannotSampleRequiresInferenceException","text":"<p>Namespace: LLama.Batched</p> <p>This exception is thrown when \"Sample()\" is called on a Conversation which has  already been prompted and before \"Infer()\" has been called on the associated  BatchedExecutor.</p> <pre><code>public class CannotSampleRequiresInferenceException : ExperimentalBatchedExecutorException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException \u2192 CannotSampleRequiresInferenceException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#events","title":"Events","text":""},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.batched.cannotsamplerequiresinferenceexception/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/","title":"llama.batched.cannotsamplerequirespromptexception","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#cannotsamplerequirespromptexception","title":"CannotSampleRequiresPromptException","text":"<p>Namespace: LLama.Batched</p> <p>This exception is thrown when \"Sample()\" is called on a Conversation which was not  first prompted.  BatchedExecutor.</p> <pre><code>public class CannotSampleRequiresPromptException : ExperimentalBatchedExecutorException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException \u2192 CannotSampleRequiresPromptException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#events","title":"Events","text":""},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.batched.cannotsamplerequirespromptexception/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/","title":"Llama.batched.cannotsavewhilerequiresinferenceexception","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#cannotsavewhilerequiresinferenceexception","title":"CannotSaveWhileRequiresInferenceException","text":"<p>Namespace: LLama.Batched</p> <p>This exception is thrown when \"Save()\" is called on a Conversation which has  already been prompted and before \"Infer()\" has been called.  BatchedExecutor.</p> <pre><code>public class CannotSaveWhileRequiresInferenceException : ExperimentalBatchedExecutorException, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException \u2192 CannotSaveWhileRequiresInferenceException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#events","title":"Events","text":""},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.batched.cannotsavewhilerequiresinferenceexception/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.conversation/","title":"llama.batched.conversation","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.conversation/#conversation","title":"Conversation","text":"<p>Namespace: LLama.Batched</p> <p>A single conversation thread that can be prompted (adding tokens from the user) or inferred (extracting a token from the LLM)</p> <pre><code>public sealed class Conversation : System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 Conversation Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.batched.conversation/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.conversation/#executor","title":"Executor","text":"<p>The executor which this conversation belongs to</p> <pre><code>public BatchedExecutor Executor { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value","title":"Property Value","text":"<p>BatchedExecutor</p>"},{"location":"xmldocs/llama.batched.conversation/#conversationid","title":"ConversationId","text":"<p>Unique ID for this conversation</p> <pre><code>public LLamaSeqId ConversationId { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value_1","title":"Property Value","text":"<p>LLamaSeqId</p>"},{"location":"xmldocs/llama.batched.conversation/#tokencount","title":"TokenCount","text":"<p>Total number of tokens in this conversation, cannot exceed the context length.</p> <pre><code>public int TokenCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.conversation/#isdisposed","title":"IsDisposed","text":"<p>Indicates if this conversation has been disposed, nothing can be done with a disposed conversation</p> <pre><code>public bool IsDisposed { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.batched.conversation/#requiresinference","title":"RequiresInference","text":"<p>Indicates if this conversation is waiting for inference to be run on the executor. \"Prompt\" and \"Sample\" cannot be called when this is true.</p> <pre><code>public bool RequiresInference { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value_4","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.batched.conversation/#requiressampling","title":"RequiresSampling","text":"<p>Indicates that this conversation should be sampled.</p> <pre><code>public bool RequiresSampling { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#property-value_5","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.batched.conversation/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.batched.conversation/#finalize","title":"Finalize()","text":"<p>Finalizer for Conversation</p> <pre><code>protected void Finalize()\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#dispose","title":"Dispose()","text":"<p>End this conversation, freeing all resources used by it</p> <pre><code>public void Dispose()\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#exceptions","title":"Exceptions","text":"<p>ObjectDisposedException</p>"},{"location":"xmldocs/llama.batched.conversation/#fork","title":"Fork()","text":"<p>Create a copy of the current conversation</p> <pre><code>public Conversation Fork()\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#returns","title":"Returns","text":"<p>Conversation</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_1","title":"Exceptions","text":"<p>ObjectDisposedException</p> <p>Remarks:</p> <p>The copy shares internal state, so consumes very little extra memory.</p>"},{"location":"xmldocs/llama.batched.conversation/#getsampleindexint32","title":"GetSampleIndex(Int32)","text":"<p>Get the index in the context which each token can be sampled from, the return value of this function get be used to retrieve logits  (SafeLLamaContextHandle.GetLogitsIth(Int32)) or to sample a token (SafeLLamaSamplerChainHandle.Sample(SafeLLamaContextHandle, Int32).</p> <pre><code>public int GetSampleIndex(int offset)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters","title":"Parameters","text":"<p><code>offset</code> Int32 How far from the end of the previous prompt should logits be sampled. Any value other than 0 requires  allLogits to have been set during prompting.  For example if 5 tokens were supplied in the last prompt call:</p>"},{"location":"xmldocs/llama.batched.conversation/#-","title":"-","text":""},{"location":"xmldocs/llama.batched.conversation/#-_1","title":"-","text":"<p>-</p>"},{"location":"xmldocs/llama.batched.conversation/#returns_1","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_2","title":"Exceptions","text":"<p>ObjectDisposedException</p> <p>CannotSampleRequiresPromptException Thrown if this conversation was not prompted before the previous call to infer</p> <p>CannotSampleRequiresInferenceException Thrown if Infer() must be called on the executor</p>"},{"location":"xmldocs/llama.batched.conversation/#sampleint32","title":"Sample(Int32)","text":"<p>Get the logits from this conversation, ready for sampling</p> <pre><code>public Span&lt;float&gt; Sample(int offset)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_1","title":"Parameters","text":"<p><code>offset</code> Int32 How far from the end of the previous prompt should logits be sampled. Any value other than 0 requires allLogits to have been set during prompting</p>"},{"location":"xmldocs/llama.batched.conversation/#returns_2","title":"Returns","text":"<p>Span&lt;Single&gt;</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_3","title":"Exceptions","text":"<p>ObjectDisposedException</p> <p>CannotSampleRequiresPromptException Thrown if this conversation was not prompted before the previous call to infer</p> <p>CannotSampleRequiresInferenceException Thrown if Infer() must be called on the executor</p>"},{"location":"xmldocs/llama.batched.conversation/#promptlistllamatoken-boolean","title":"Prompt(List&lt;LLamaToken&gt;, Boolean)","text":"<p>Add tokens to this conversation</p> <pre><code>public void Prompt(List&lt;LLamaToken&gt; tokens, bool allLogits)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_2","title":"Parameters","text":"<p><code>tokens</code> List&lt;LLamaToken&gt;</p> <p><code>allLogits</code> Boolean If true, generate logits for all tokens. If false, only generate logits for the last token.</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_4","title":"Exceptions","text":"<p>ObjectDisposedException</p> <p>AlreadyPromptedConversationException</p>"},{"location":"xmldocs/llama.batched.conversation/#promptreadonlyspanllamatoken-boolean","title":"Prompt(ReadOnlySpan&lt;LLamaToken&gt;, Boolean)","text":"<p>Add tokens to this conversation</p> <pre><code>public void Prompt(ReadOnlySpan&lt;LLamaToken&gt; tokens, bool allLogits)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_3","title":"Parameters","text":"<p><code>tokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p> <p><code>allLogits</code> Boolean If true, generate logits for all tokens. If false, only generate logits for the last token.</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_5","title":"Exceptions","text":"<p>ObjectDisposedException</p> <p>AlreadyPromptedConversationException</p>"},{"location":"xmldocs/llama.batched.conversation/#promptllamatoken","title":"Prompt(LLamaToken)","text":"<p>Add a single token to this conversation</p> <pre><code>public void Prompt(LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_4","title":"Parameters","text":"<p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_6","title":"Exceptions","text":"<p>ObjectDisposedException</p> <p>AlreadyPromptedConversationException</p>"},{"location":"xmldocs/llama.batched.conversation/#promptsafellavaimageembedhandle","title":"Prompt(SafeLlavaImageEmbedHandle)","text":"<p>Prompt this conversation with an image embedding</p> <pre><code>public void Prompt(SafeLlavaImageEmbedHandle embedding)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_5","title":"Parameters","text":"<p><code>embedding</code> SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.batched.conversation/#promptreadonlyspansingle","title":"Prompt(ReadOnlySpan&lt;Single&gt;)","text":"<p>Prompt this conversation with embeddings</p> <pre><code>public void Prompt(ReadOnlySpan&lt;float&gt; embeddings)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_6","title":"Parameters","text":"<p><code>embeddings</code> ReadOnlySpan&lt;Single&gt; The raw values of the embeddings. This span must divide equally by the embedding size of this model.</p>"},{"location":"xmldocs/llama.batched.conversation/#modifymodifykvcache","title":"Modify(ModifyKvCache)","text":"<p>Directly modify the KV cache of this conversation</p> <pre><code>public void Modify(ModifyKvCache modifier)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_7","title":"Parameters","text":"<p><code>modifier</code> ModifyKvCache</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_7","title":"Exceptions","text":"<p>CannotModifyWhileRequiresInferenceException Thrown if this method is called while Conversation.RequiresInference == true</p>"},{"location":"xmldocs/llama.batched.conversation/#savestring","title":"Save(String)","text":"<p>Save the complete state of this conversation to a file. if the file already exists it will be overwritten.</p> <pre><code>public void Save(string filepath)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#parameters_8","title":"Parameters","text":"<p><code>filepath</code> String</p>"},{"location":"xmldocs/llama.batched.conversation/#exceptions_8","title":"Exceptions","text":"<p>CannotSaveWhileRequiresInferenceException</p>"},{"location":"xmldocs/llama.batched.conversation/#save","title":"Save()","text":"<p>Save the complete state of this conversation in system memory.</p> <pre><code>public State Save()\n</code></pre>"},{"location":"xmldocs/llama.batched.conversation/#returns_3","title":"Returns","text":"<p>State</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.conversationextensions/","title":"llama.batched.conversationextensions","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.conversationextensions/#conversationextensions","title":"ConversationExtensions","text":"<p>Namespace: LLama.Batched</p> <p>Extension method for Conversation</p> <pre><code>public static class ConversationExtensions\n</code></pre> <p>Inheritance Object \u2192 ConversationExtensions Attributes NullableContextAttribute, NullableAttribute, ExtensionAttribute</p>"},{"location":"xmldocs/llama.batched.conversationextensions/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.batched.conversationextensions/#sampleconversation-safellamasamplerchainhandle-int32","title":"Sample(Conversation, SafeLLamaSamplerChainHandle, Int32)","text":"<p>Sample a token from this conversation using the given sampler chain</p> <pre><code>public static LLamaToken Sample(Conversation conversation, SafeLLamaSamplerChainHandle sampler, int offset)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversationextensions/#parameters","title":"Parameters","text":"<p><code>conversation</code> Conversation Conversation to sample from</p> <p><code>sampler</code> SafeLLamaSamplerChainHandle</p> <p><code>offset</code> Int32 Offset from the end of the conversation to the logits to sample, see Conversation.GetSampleIndex(Int32) for more details</p>"},{"location":"xmldocs/llama.batched.conversationextensions/#returns","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.batched.conversationextensions/#sampleconversation-isamplingpipeline-int32","title":"Sample(Conversation, ISamplingPipeline, Int32)","text":"<p>Sample a token from this conversation using the given sampling pipeline</p> <pre><code>public static LLamaToken Sample(Conversation conversation, ISamplingPipeline sampler, int offset)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversationextensions/#parameters_1","title":"Parameters","text":"<p><code>conversation</code> Conversation Conversation to sample from</p> <p><code>sampler</code> ISamplingPipeline</p> <p><code>offset</code> Int32 Offset from the end of the conversation to the logits to sample, see Conversation.GetSampleIndex(Int32) for more details</p>"},{"location":"xmldocs/llama.batched.conversationextensions/#returns_1","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.batched.conversationextensions/#rewindconversation-int32","title":"Rewind(Conversation, Int32)","text":"<p>Rewind a Conversation back to an earlier state by removing tokens from the end</p> <pre><code>public static void Rewind(Conversation conversation, int tokens)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversationextensions/#parameters_2","title":"Parameters","text":"<p><code>conversation</code> Conversation The conversation to rewind</p> <p><code>tokens</code> Int32 The number of tokens to rewind</p>"},{"location":"xmldocs/llama.batched.conversationextensions/#exceptions","title":"Exceptions","text":"<p>ArgumentOutOfRangeException Thrown if <code>tokens</code> parameter is larger than TokenCount</p>"},{"location":"xmldocs/llama.batched.conversationextensions/#shiftleftconversation-int32-int32","title":"ShiftLeft(Conversation, Int32, Int32)","text":"<p>Shift all tokens over to the left, removing \"count\" tokens from the start and shifting everything over.  Leaves \"keep\" tokens at the start completely untouched. This can be used to free up space when the context  gets full, keeping the prompt at the start intact.</p> <pre><code>public static void ShiftLeft(Conversation conversation, int count, int keep)\n</code></pre>"},{"location":"xmldocs/llama.batched.conversationextensions/#parameters_3","title":"Parameters","text":"<p><code>conversation</code> Conversation The conversation to rewind</p> <p><code>count</code> Int32 How much to shift tokens over by</p> <p><code>keep</code> Int32 The number of tokens at the start which should not be shifted</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/","title":"llama.batched.experimentalbatchedexecutorexception","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#experimentalbatchedexecutorexception","title":"ExperimentalBatchedExecutorException","text":"<p>Namespace: LLama.Batched</p> <p>Base class for exceptions thrown from BatchedExecutor</p> <pre><code>public class ExperimentalBatchedExecutorException : System.Exception, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 ExperimentalBatchedExecutorException Implements ISerializable</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#events","title":"Events","text":""},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.batched.experimentalbatchedexecutorexception/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.chatsession/","title":"llama.chatsession","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.chatsession/#chatsession","title":"ChatSession","text":"<p>Namespace: LLama</p> <p>The main chat session class.</p> <pre><code>public class ChatSession\n</code></pre> <p>Inheritance Object \u2192 ChatSession Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.chatsession/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.chatsession/#outputtransform","title":"OutputTransform","text":"<p>The output transform used in this session.</p> <pre><code>public ITextStreamTransform OutputTransform;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#model_state_filename","title":"MODEL_STATE_FILENAME","text":"<p>The filename for the serialized model state (KV cache, etc).</p> <pre><code>public static string MODEL_STATE_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#executor_state_filename","title":"EXECUTOR_STATE_FILENAME","text":"<p>The filename for the serialized executor state.</p> <pre><code>public static string EXECUTOR_STATE_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#history_state_filename","title":"HISTORY_STATE_FILENAME","text":"<p>The filename for the serialized chat history.</p> <pre><code>public static string HISTORY_STATE_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#input_transform_filename","title":"INPUT_TRANSFORM_FILENAME","text":"<p>The filename for the serialized input transform pipeline.</p> <pre><code>public static string INPUT_TRANSFORM_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#output_transform_filename","title":"OUTPUT_TRANSFORM_FILENAME","text":"<p>The filename for the serialized output transform.</p> <pre><code>public static string OUTPUT_TRANSFORM_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#history_transform_filename","title":"HISTORY_TRANSFORM_FILENAME","text":"<p>The filename for the serialized history transform.</p> <pre><code>public static string HISTORY_TRANSFORM_FILENAME;\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.chatsession/#executor","title":"Executor","text":"<p>The executor for this session.</p> <pre><code>public ILLamaExecutor Executor { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#property-value","title":"Property Value","text":"<p>ILLamaExecutor</p>"},{"location":"xmldocs/llama.chatsession/#history","title":"History","text":"<p>The chat history for this session.</p> <pre><code>public ChatHistory History { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#property-value_1","title":"Property Value","text":"<p>ChatHistory</p>"},{"location":"xmldocs/llama.chatsession/#historytransform","title":"HistoryTransform","text":"<p>The history transform used in this session.</p> <pre><code>public IHistoryTransform HistoryTransform { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#property-value_2","title":"Property Value","text":"<p>IHistoryTransform</p>"},{"location":"xmldocs/llama.chatsession/#inputtransformpipeline","title":"InputTransformPipeline","text":"<p>The input transform pipeline used in this session.</p> <pre><code>public List&lt;ITextTransform&gt; InputTransformPipeline { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#property-value_3","title":"Property Value","text":"<p>List&lt;ITextTransform&gt;</p>"},{"location":"xmldocs/llama.chatsession/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.chatsession/#chatsessionillamaexecutor","title":"ChatSession(ILLamaExecutor)","text":"<p>Create a new chat session.</p> <pre><code>public ChatSession(ILLamaExecutor executor)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters","title":"Parameters","text":"<p><code>executor</code> ILLamaExecutor The executor for this session</p>"},{"location":"xmldocs/llama.chatsession/#chatsessionillamaexecutor-chathistory","title":"ChatSession(ILLamaExecutor, ChatHistory)","text":"<p>Create a new chat session with a custom history.</p> <pre><code>public ChatSession(ILLamaExecutor executor, ChatHistory history)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_1","title":"Parameters","text":"<p><code>executor</code> ILLamaExecutor</p> <p><code>history</code> ChatHistory</p>"},{"location":"xmldocs/llama.chatsession/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.chatsession/#initializesessionfromhistoryasyncillamaexecutor-chathistory-ihistorytransform","title":"InitializeSessionFromHistoryAsync(ILLamaExecutor, ChatHistory, IHistoryTransform)","text":"<p>Create a new chat session and preprocess history.</p> <pre><code>public static Task&lt;ChatSession&gt; InitializeSessionFromHistoryAsync(ILLamaExecutor executor, ChatHistory history, IHistoryTransform transform)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_2","title":"Parameters","text":"<p><code>executor</code> ILLamaExecutor The executor for this session</p> <p><code>history</code> ChatHistory History for this session</p> <p><code>transform</code> IHistoryTransform History Transform for this session</p>"},{"location":"xmldocs/llama.chatsession/#returns","title":"Returns","text":"<p>Task&lt;ChatSession&gt; A new chat session.</p>"},{"location":"xmldocs/llama.chatsession/#withhistorytransformihistorytransform","title":"WithHistoryTransform(IHistoryTransform)","text":"<p>Use a custom history transform.</p> <pre><code>public ChatSession WithHistoryTransform(IHistoryTransform transform)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_3","title":"Parameters","text":"<p><code>transform</code> IHistoryTransform</p>"},{"location":"xmldocs/llama.chatsession/#returns_1","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#addinputtransformitexttransform","title":"AddInputTransform(ITextTransform)","text":"<p>Add a text transform to the input transform pipeline.</p> <pre><code>public ChatSession AddInputTransform(ITextTransform transform)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_4","title":"Parameters","text":"<p><code>transform</code> ITextTransform</p>"},{"location":"xmldocs/llama.chatsession/#returns_2","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#withoutputtransformitextstreamtransform","title":"WithOutputTransform(ITextStreamTransform)","text":"<p>Use a custom output transform.</p> <pre><code>public ChatSession WithOutputTransform(ITextStreamTransform transform)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_5","title":"Parameters","text":"<p><code>transform</code> ITextStreamTransform</p>"},{"location":"xmldocs/llama.chatsession/#returns_3","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#savesessionstring","title":"SaveSession(String)","text":"<p>Save a session from a directory.</p> <pre><code>public void SaveSession(string path)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_6","title":"Parameters","text":"<p><code>path</code> String</p>"},{"location":"xmldocs/llama.chatsession/#exceptions","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.chatsession/#getsessionstate","title":"GetSessionState()","text":"<p>Get the session state.</p> <pre><code>public SessionState GetSessionState()\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#returns_4","title":"Returns","text":"<p>SessionState SessionState object representing session state in-memory</p>"},{"location":"xmldocs/llama.chatsession/#loadsessionsessionstate-boolean","title":"LoadSession(SessionState, Boolean)","text":"<p>Load a session from a session state.</p> <pre><code>public void LoadSession(SessionState state, bool loadTransforms)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_7","title":"Parameters","text":"<p><code>state</code> SessionState</p> <p><code>loadTransforms</code> Boolean If true loads transforms saved in the session state.</p>"},{"location":"xmldocs/llama.chatsession/#exceptions_1","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.chatsession/#loadsessionstring-boolean","title":"LoadSession(String, Boolean)","text":"<p>Load a session from a directory.</p> <pre><code>public void LoadSession(string path, bool loadTransforms)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_8","title":"Parameters","text":"<p><code>path</code> String</p> <p><code>loadTransforms</code> Boolean If true loads transforms saved in the session state.</p>"},{"location":"xmldocs/llama.chatsession/#exceptions_2","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.chatsession/#addmessagemessage","title":"AddMessage(Message)","text":"<p>Add a message to the chat history.</p> <pre><code>public ChatSession AddMessage(Message message)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_9","title":"Parameters","text":"<p><code>message</code> Message</p>"},{"location":"xmldocs/llama.chatsession/#returns_5","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#addsystemmessagestring","title":"AddSystemMessage(String)","text":"<p>Add a system message to the chat history.</p> <pre><code>public ChatSession AddSystemMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_10","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_6","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#addassistantmessagestring","title":"AddAssistantMessage(String)","text":"<p>Add an assistant message to the chat history.</p> <pre><code>public ChatSession AddAssistantMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_11","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_7","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#addusermessagestring","title":"AddUserMessage(String)","text":"<p>Add a user message to the chat history.</p> <pre><code>public ChatSession AddUserMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_12","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_8","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#removelastmessage","title":"RemoveLastMessage()","text":"<p>Remove the last message from the chat history.</p> <pre><code>public ChatSession RemoveLastMessage()\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#returns_9","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#addandprocessmessagemessage","title":"AddAndProcessMessage(Message)","text":"<p>Compute KV cache for the message and add it to the chat history.</p> <pre><code>public Task&lt;ChatSession&gt; AddAndProcessMessage(Message message)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_13","title":"Parameters","text":"<p><code>message</code> Message</p>"},{"location":"xmldocs/llama.chatsession/#returns_10","title":"Returns","text":"<p>Task&lt;ChatSession&gt;</p>"},{"location":"xmldocs/llama.chatsession/#addandprocesssystemmessagestring","title":"AddAndProcessSystemMessage(String)","text":"<p>Compute KV cache for the system message and add it to the chat history.</p> <pre><code>public Task&lt;ChatSession&gt; AddAndProcessSystemMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_14","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_11","title":"Returns","text":"<p>Task&lt;ChatSession&gt;</p>"},{"location":"xmldocs/llama.chatsession/#addandprocessusermessagestring","title":"AddAndProcessUserMessage(String)","text":"<p>Compute KV cache for the user message and add it to the chat history.</p> <pre><code>public Task&lt;ChatSession&gt; AddAndProcessUserMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_15","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_12","title":"Returns","text":"<p>Task&lt;ChatSession&gt;</p>"},{"location":"xmldocs/llama.chatsession/#addandprocessassistantmessagestring","title":"AddAndProcessAssistantMessage(String)","text":"<p>Compute KV cache for the assistant message and add it to the chat history.</p> <pre><code>public Task&lt;ChatSession&gt; AddAndProcessAssistantMessage(string content)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_16","title":"Parameters","text":"<p><code>content</code> String</p>"},{"location":"xmldocs/llama.chatsession/#returns_13","title":"Returns","text":"<p>Task&lt;ChatSession&gt;</p>"},{"location":"xmldocs/llama.chatsession/#replaceusermessagemessage-message","title":"ReplaceUserMessage(Message, Message)","text":"<p>Replace a user message with a new message and remove all messages after the new message.  This is useful when the user wants to edit a message. And regenerate the response.</p> <pre><code>public ChatSession ReplaceUserMessage(Message oldMessage, Message newMessage)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_17","title":"Parameters","text":"<p><code>oldMessage</code> Message</p> <p><code>newMessage</code> Message</p>"},{"location":"xmldocs/llama.chatsession/#returns_14","title":"Returns","text":"<p>ChatSession</p>"},{"location":"xmldocs/llama.chatsession/#chatasyncmessage-boolean-iinferenceparams-cancellationtoken","title":"ChatAsync(Message, Boolean, IInferenceParams, CancellationToken)","text":"<p>Chat with the model.</p> <pre><code>public IAsyncEnumerable&lt;string&gt; ChatAsync(Message message, bool applyInputTransformPipeline, IInferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_18","title":"Parameters","text":"<p><code>message</code> Message</p> <p><code>applyInputTransformPipeline</code> Boolean</p> <p><code>inferenceParams</code> IInferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.chatsession/#returns_15","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.chatsession/#exceptions_3","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.chatsession/#chatasyncmessage-iinferenceparams-cancellationtoken","title":"ChatAsync(Message, IInferenceParams, CancellationToken)","text":"<p>Chat with the model.</p> <pre><code>public IAsyncEnumerable&lt;string&gt; ChatAsync(Message message, IInferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_19","title":"Parameters","text":"<p><code>message</code> Message</p> <p><code>inferenceParams</code> IInferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.chatsession/#returns_16","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.chatsession/#chatasyncchathistory-boolean-iinferenceparams-cancellationtoken","title":"ChatAsync(ChatHistory, Boolean, IInferenceParams, CancellationToken)","text":"<p>Chat with the model.</p> <pre><code>public IAsyncEnumerable&lt;string&gt; ChatAsync(ChatHistory history, bool applyInputTransformPipeline, IInferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_20","title":"Parameters","text":"<p><code>history</code> ChatHistory</p> <p><code>applyInputTransformPipeline</code> Boolean</p> <p><code>inferenceParams</code> IInferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.chatsession/#returns_17","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.chatsession/#exceptions_4","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.chatsession/#chatasyncchathistory-iinferenceparams-cancellationtoken","title":"ChatAsync(ChatHistory, IInferenceParams, CancellationToken)","text":"<p>Chat with the model.</p> <pre><code>public IAsyncEnumerable&lt;string&gt; ChatAsync(ChatHistory history, IInferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_21","title":"Parameters","text":"<p><code>history</code> ChatHistory</p> <p><code>inferenceParams</code> IInferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.chatsession/#returns_18","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.chatsession/#regenerateassistantmessageasyncinferenceparams-cancellationtoken","title":"RegenerateAssistantMessageAsync(InferenceParams, CancellationToken)","text":"<p>Regenerate the last assistant message.</p> <pre><code>public IAsyncEnumerable&lt;string&gt; RegenerateAssistantMessageAsync(InferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.chatsession/#parameters_22","title":"Parameters","text":"<p><code>inferenceParams</code> InferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.chatsession/#returns_19","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.chatsession/#exceptions_5","title":"Exceptions","text":"<p>InvalidOperationException</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.authorrole/","title":"llama.common.authorrole","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.authorrole/#authorrole","title":"AuthorRole","text":"<p>Namespace: LLama.Common</p> <p>Role of the message author, e.g. user/assistant/system</p> <pre><code>public enum AuthorRole\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 AuthorRole Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.common.authorrole/#fields","title":"Fields","text":"Name Value Description Unknown -1 Role is unknown System 0 Message comes from a \"system\" prompt, not written by a user or language model User 1 Message comes from the user Assistant 2 Messages was generated by the language model <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.chathistory/","title":"llama.common.chathistory","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.chathistory/#chathistory","title":"ChatHistory","text":"<p>Namespace: LLama.Common</p> <p>The chat history class</p> <pre><code>public class ChatHistory\n</code></pre> <p>Inheritance Object \u2192 ChatHistory Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.common.chathistory/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.common.chathistory/#messages","title":"Messages","text":"<p>List of messages in the chat</p> <pre><code>public List&lt;Message&gt; Messages { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#property-value","title":"Property Value","text":"<p>List&lt;Message&gt;</p>"},{"location":"xmldocs/llama.common.chathistory/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.common.chathistory/#chathistory_1","title":"ChatHistory()","text":"<p>Create a new instance of the chat content class</p> <pre><code>public ChatHistory()\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#chathistorymessage","title":"ChatHistory(Message[])","text":"<p>Create a new instance of the chat history from array of messages</p> <pre><code>public ChatHistory(Message[] messageHistory)\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#parameters","title":"Parameters","text":"<p><code>messageHistory</code> Message[]</p>"},{"location":"xmldocs/llama.common.chathistory/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.common.chathistory/#addmessageauthorrole-string","title":"AddMessage(AuthorRole, String)","text":"<p>Add a message to the chat history</p> <pre><code>public void AddMessage(AuthorRole authorRole, string content)\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#parameters_1","title":"Parameters","text":"<p><code>authorRole</code> AuthorRole Role of the message author</p> <p><code>content</code> String Message content</p>"},{"location":"xmldocs/llama.common.chathistory/#tojson","title":"ToJson()","text":"<p>Serialize the chat history to JSON</p> <pre><code>public string ToJson()\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.common.chathistory/#fromjsonstring","title":"FromJson(String)","text":"<p>Deserialize a chat history from JSON</p> <pre><code>public static ChatHistory FromJson(string json)\n</code></pre>"},{"location":"xmldocs/llama.common.chathistory/#parameters_2","title":"Parameters","text":"<p><code>json</code> String</p>"},{"location":"xmldocs/llama.common.chathistory/#returns_1","title":"Returns","text":"<p>ChatHistory</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/","title":"llama.common.fixedsizequeue-1","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#fixedsizequeuet","title":"FixedSizeQueue&lt;T&gt;","text":"<p>Namespace: LLama.Common</p> <p>A queue with fixed storage size.  Currently it's only a naive implementation and needs to be further optimized in the future.</p> <pre><code>public class FixedSizeQueue&lt;T&gt; : , , , System.Collections.IEnumerable\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#type-parameters","title":"Type Parameters","text":"<p><code>T</code></p> <p>Inheritance Object \u2192 FixedSizeQueue&lt;T&gt; Implements IReadOnlyList&lt;T&gt;, IReadOnlyCollection&lt;T&gt;, IEnumerable&lt;T&gt;, IEnumerable Attributes NullableContextAttribute, NullableAttribute, DefaultMemberAttribute</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.common.fixedsizequeue-1/#item","title":"Item","text":"<pre><code>public T Item { get; }\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#property-value","title":"Property Value","text":"<p>T</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#count","title":"Count","text":"<p>Number of items in this queue</p> <pre><code>public int Count { get; }\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#capacity","title":"Capacity","text":"<p>Maximum number of items allowed in this queue</p> <pre><code>public int Capacity { get; }\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.common.fixedsizequeue-1/#fixedsizequeueint32","title":"FixedSizeQueue(Int32)","text":"<p>Create a new queue</p> <pre><code>public FixedSizeQueue(int size)\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#parameters","title":"Parameters","text":"<p><code>size</code> Int32 the maximum number of items to store in this queue</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#fixedsizequeueint32-ienumerablet","title":"FixedSizeQueue(Int32, IEnumerable&lt;T&gt;)","text":"<p>Fill the quene with the data. Please ensure that data.Count &lt;= size</p> <pre><code>public FixedSizeQueue(int size, IEnumerable&lt;T&gt; data)\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#parameters_1","title":"Parameters","text":"<p><code>size</code> Int32</p> <p><code>data</code> IEnumerable&lt;T&gt;</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.common.fixedsizequeue-1/#enqueuet","title":"Enqueue(T)","text":"<p>Enquene an element.</p> <pre><code>public void Enqueue(T item)\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#parameters_2","title":"Parameters","text":"<p><code>item</code> T</p>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#getenumerator","title":"GetEnumerator()","text":"<pre><code>public IEnumerator&lt;T&gt; GetEnumerator()\n</code></pre>"},{"location":"xmldocs/llama.common.fixedsizequeue-1/#returns","title":"Returns","text":"<p>IEnumerator&lt;T&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.inferenceparams/","title":"llama.common.inferenceparams","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.inferenceparams/#inferenceparams","title":"InferenceParams","text":"<p>Namespace: LLama.Common</p> <p>The parameters used for inference.</p> <pre><code>public class InferenceParams : LLama.Abstractions.IInferenceParams, System.IEquatable`1[[LLama.Common.InferenceParams, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 InferenceParams Implements IInferenceParams, IEquatable&lt;InferenceParams&gt; Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.common.inferenceparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.common.inferenceparams/#equalitycontract","title":"EqualityContract","text":"<pre><code>protected Type EqualityContract { get; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value","title":"Property Value","text":"<p>Type</p>"},{"location":"xmldocs/llama.common.inferenceparams/#tokenskeep","title":"TokensKeep","text":"<p>number of tokens to keep from initial prompt when applying context shifting</p> <pre><code>public int TokensKeep { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.inferenceparams/#maxtokens","title":"MaxTokens","text":"<p>how many new tokens to predict (n_predict), set to -1 to inifinitely generate response  until it complete.</p> <pre><code>public int MaxTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.inferenceparams/#antiprompts","title":"AntiPrompts","text":"<p>Sequences where the model will stop generating further tokens.</p> <pre><code>public IReadOnlyList&lt;string&gt; AntiPrompts { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_3","title":"Property Value","text":"<p>IReadOnlyList&lt;String&gt;</p>"},{"location":"xmldocs/llama.common.inferenceparams/#samplingpipeline","title":"SamplingPipeline","text":"<pre><code>public ISamplingPipeline SamplingPipeline { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_4","title":"Property Value","text":"<p>ISamplingPipeline</p>"},{"location":"xmldocs/llama.common.inferenceparams/#decodespecialtokens","title":"DecodeSpecialTokens","text":"<pre><code>public bool DecodeSpecialTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#property-value_5","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.inferenceparams/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.common.inferenceparams/#inferenceparamsinferenceparams","title":"InferenceParams(InferenceParams)","text":"<pre><code>protected InferenceParams(InferenceParams original)\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#parameters","title":"Parameters","text":"<p><code>original</code> InferenceParams</p>"},{"location":"xmldocs/llama.common.inferenceparams/#inferenceparams_1","title":"InferenceParams()","text":"<pre><code>public InferenceParams()\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.common.inferenceparams/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.common.inferenceparams/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#parameters_1","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.common.inferenceparams/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.inferenceparams/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.inferenceparams/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#parameters_2","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.common.inferenceparams/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.inferenceparams/#equalsinferenceparams","title":"Equals(InferenceParams)","text":"<pre><code>public bool Equals(InferenceParams other)\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#parameters_3","title":"Parameters","text":"<p><code>other</code> InferenceParams</p>"},{"location":"xmldocs/llama.common.inferenceparams/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.inferenceparams/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public InferenceParams &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.common.inferenceparams/#returns_5","title":"Returns","text":"<p>InferenceParams</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.mirostattype/","title":"llama.common.mirostattype","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.mirostattype/#mirostattype","title":"MirostatType","text":"<p>Namespace: LLama.Common</p> <p>Type of \"mirostat\" sampling to use.  https://github.com/basusourya/mirostat</p> <pre><code>public enum MirostatType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 MirostatType Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.common.mirostattype/#fields","title":"Fields","text":"Name Value Description Disable 0 Disable Mirostat sampling Mirostat 1 Original mirostat algorithm Mirostat2 2 Mirostat 2.0 algorithm <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.modelparams/","title":"llama.common.modelparams","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.common.modelparams/#modelparams","title":"ModelParams","text":"<p>Namespace: LLama.Common</p> <p>The parameters for initializing a LLama model.</p> <pre><code>public class ModelParams : LLama.Abstractions.ILLamaParams, LLama.Abstractions.IModelParams, LLama.Abstractions.IContextParams, System.IEquatable`1[[LLama.Common.ModelParams, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 ModelParams Implements ILLamaParams, IModelParams, IContextParams, IEquatable&lt;ModelParams&gt; Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.common.modelparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.common.modelparams/#equalitycontract","title":"EqualityContract","text":"<pre><code>protected Type EqualityContract { get; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value","title":"Property Value","text":"<p>Type</p>"},{"location":"xmldocs/llama.common.modelparams/#contextsize","title":"ContextSize","text":"<pre><code>public Nullable&lt;uint&gt; ContextSize { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_1","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#maingpu","title":"MainGpu","text":"<pre><code>public int MainGpu { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.modelparams/#splitmode","title":"SplitMode","text":"<pre><code>public Nullable&lt;GPUSplitMode&gt; SplitMode { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_3","title":"Property Value","text":"<p>Nullable&lt;GPUSplitMode&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#tensorbufferoverrides","title":"TensorBufferOverrides","text":"<pre><code>public List&lt;TensorBufferOverride&gt; TensorBufferOverrides { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_4","title":"Property Value","text":"<p>List&lt;TensorBufferOverride&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#gpulayercount","title":"GpuLayerCount","text":"<pre><code>public int GpuLayerCount { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_5","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.modelparams/#seqmax","title":"SeqMax","text":"<pre><code>public uint SeqMax { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_6","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.common.modelparams/#usememorymap","title":"UseMemorymap","text":"<pre><code>public bool UseMemorymap { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_7","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#usememorylock","title":"UseMemoryLock","text":"<pre><code>public bool UseMemoryLock { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_8","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#modelpath","title":"ModelPath","text":"<pre><code>public string ModelPath { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_9","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.common.modelparams/#threads","title":"Threads","text":"<pre><code>public Nullable&lt;int&gt; Threads { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_10","title":"Property Value","text":"<p>Nullable&lt;Int32&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#batchthreads","title":"BatchThreads","text":"<pre><code>public Nullable&lt;int&gt; BatchThreads { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_11","title":"Property Value","text":"<p>Nullable&lt;Int32&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#batchsize","title":"BatchSize","text":"<pre><code>public uint BatchSize { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_12","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.common.modelparams/#ubatchsize","title":"UBatchSize","text":"<pre><code>public uint UBatchSize { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_13","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.common.modelparams/#embeddings","title":"Embeddings","text":"<pre><code>public bool Embeddings { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_14","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#tensorsplits","title":"TensorSplits","text":"<pre><code>public TensorSplitsCollection TensorSplits { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_15","title":"Property Value","text":"<p>TensorSplitsCollection</p>"},{"location":"xmldocs/llama.common.modelparams/#checktensors","title":"CheckTensors","text":"<pre><code>public bool CheckTensors { get; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_16","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#metadataoverrides","title":"MetadataOverrides","text":"<pre><code>public List&lt;MetadataOverride&gt; MetadataOverrides { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_17","title":"Property Value","text":"<p>List&lt;MetadataOverride&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#ropefrequencybase","title":"RopeFrequencyBase","text":"<pre><code>public Nullable&lt;float&gt; RopeFrequencyBase { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_18","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#ropefrequencyscale","title":"RopeFrequencyScale","text":"<pre><code>public Nullable&lt;float&gt; RopeFrequencyScale { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_19","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnextrapolationfactor","title":"YarnExtrapolationFactor","text":"<pre><code>public Nullable&lt;float&gt; YarnExtrapolationFactor { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_20","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnattentionfactor","title":"YarnAttentionFactor","text":"<pre><code>public Nullable&lt;float&gt; YarnAttentionFactor { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_21","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnbetafast","title":"YarnBetaFast","text":"<pre><code>public Nullable&lt;float&gt; YarnBetaFast { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_22","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnbetaslow","title":"YarnBetaSlow","text":"<pre><code>public Nullable&lt;float&gt; YarnBetaSlow { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_23","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnoriginalcontext","title":"YarnOriginalContext","text":"<pre><code>public Nullable&lt;uint&gt; YarnOriginalContext { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_24","title":"Property Value","text":"<p>Nullable&lt;UInt32&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#yarnscalingtype","title":"YarnScalingType","text":"<pre><code>public Nullable&lt;RopeScalingType&gt; YarnScalingType { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_25","title":"Property Value","text":"<p>Nullable&lt;RopeScalingType&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#typek","title":"TypeK","text":"<pre><code>public Nullable&lt;GGMLType&gt; TypeK { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_26","title":"Property Value","text":"<p>Nullable&lt;GGMLType&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#typev","title":"TypeV","text":"<pre><code>public Nullable&lt;GGMLType&gt; TypeV { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_27","title":"Property Value","text":"<p>Nullable&lt;GGMLType&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#nokqvoffload","title":"NoKqvOffload","text":"<pre><code>public bool NoKqvOffload { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_28","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#flashattention","title":"FlashAttention","text":"<pre><code>public bool FlashAttention { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_29","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#defragthreshold","title":"DefragThreshold","text":"<pre><code>public Nullable&lt;float&gt; DefragThreshold { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_30","title":"Property Value","text":"<p>Nullable&lt;Single&gt;</p>"},{"location":"xmldocs/llama.common.modelparams/#poolingtype","title":"PoolingType","text":"<pre><code>public LLamaPoolingType PoolingType { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_31","title":"Property Value","text":"<p>LLamaPoolingType</p>"},{"location":"xmldocs/llama.common.modelparams/#attentiontype","title":"AttentionType","text":"<pre><code>public LLamaAttentionType AttentionType { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_32","title":"Property Value","text":"<p>LLamaAttentionType</p>"},{"location":"xmldocs/llama.common.modelparams/#vocabonly","title":"VocabOnly","text":"<pre><code>public bool VocabOnly { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_33","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#encoding","title":"Encoding","text":"<pre><code>public Encoding Encoding { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#property-value_34","title":"Property Value","text":"<p>Encoding</p>"},{"location":"xmldocs/llama.common.modelparams/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.common.modelparams/#modelparamsstring","title":"ModelParams(String)","text":"<pre><code>public ModelParams(string modelPath)\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#parameters","title":"Parameters","text":"<p><code>modelPath</code> String The model path.</p>"},{"location":"xmldocs/llama.common.modelparams/#modelparamsmodelparams","title":"ModelParams(ModelParams)","text":"<pre><code>protected ModelParams(ModelParams original)\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#parameters_1","title":"Parameters","text":"<p><code>original</code> ModelParams</p>"},{"location":"xmldocs/llama.common.modelparams/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.common.modelparams/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.common.modelparams/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#parameters_2","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.common.modelparams/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.common.modelparams/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#parameters_3","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.common.modelparams/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#equalsmodelparams","title":"Equals(ModelParams)","text":"<pre><code>public bool Equals(ModelParams other)\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#parameters_4","title":"Parameters","text":"<p><code>other</code> ModelParams</p>"},{"location":"xmldocs/llama.common.modelparams/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.common.modelparams/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public ModelParams &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.common.modelparams/#returns_5","title":"Returns","text":"<p>ModelParams</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/","title":"Llama.exceptions.getlogitsinvalidindexexception","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#getlogitsinvalidindexexception","title":"GetLogitsInvalidIndexException","text":"<p>Namespace: LLama.Exceptions</p> <p><code>llama_get_logits_ith</code> returned null, indicating that the index was invalid</p> <pre><code>public class GetLogitsInvalidIndexException : RuntimeError, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 RuntimeError \u2192 GetLogitsInvalidIndexException Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#index","title":"Index","text":"<p>The incorrect index passed to the <code>llama_get_logits_ith</code> call</p> <pre><code>public int Index { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#property-value_1","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#property-value_2","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#property-value_3","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#property-value_4","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#property-value_6","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#property-value_7","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#property-value_8","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#getlogitsinvalidindexexceptionint32","title":"GetLogitsInvalidIndexException(Int32)","text":"<pre><code>public GetLogitsInvalidIndexException(int index)\n</code></pre>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#parameters","title":"Parameters","text":"<p><code>index</code> Int32</p>"},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#events","title":"Events","text":""},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.exceptions.getlogitsinvalidindexexception/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/","title":"llama.exceptions.llamadecodeerror","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#llamadecodeerror","title":"LLamaDecodeError","text":"<p>Namespace: LLama.Exceptions</p> <p><code>llama_decode</code> return a non-zero status code</p> <pre><code>public class LLamaDecodeError : RuntimeError, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 RuntimeError \u2192 LLamaDecodeError Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#returncode","title":"ReturnCode","text":"<p>The return status code</p> <pre><code>public DecodeResult ReturnCode { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value","title":"Property Value","text":"<p>DecodeResult</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_1","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_2","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_3","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_4","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_6","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_7","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#property-value_8","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#llamadecodeerrordecoderesult","title":"LLamaDecodeError(DecodeResult)","text":"<pre><code>public LLamaDecodeError(DecodeResult returnCode)\n</code></pre>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#parameters","title":"Parameters","text":"<p><code>returnCode</code> DecodeResult</p>"},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#events","title":"Events","text":""},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.exceptions.llamadecodeerror/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/","title":"llama.exceptions.loadweightsfailedexception","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#loadweightsfailedexception","title":"LoadWeightsFailedException","text":"<p>Namespace: LLama.Exceptions</p> <p>Loading model weights failed</p> <pre><code>public class LoadWeightsFailedException : RuntimeError, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 RuntimeError \u2192 LoadWeightsFailedException Implements ISerializable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#modelpath","title":"ModelPath","text":"<p>The model path which failed to load</p> <pre><code>public string ModelPath { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_1","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_2","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_3","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_4","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_6","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_7","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#property-value_8","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#loadweightsfailedexceptionstring","title":"LoadWeightsFailedException(String)","text":"<pre><code>public LoadWeightsFailedException(string modelPath)\n</code></pre>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#parameters","title":"Parameters","text":"<p><code>modelPath</code> String</p>"},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#events","title":"Events","text":""},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.exceptions.loadweightsfailedexception/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/","title":"Llama.exceptions.missingtemplateexception","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#missingtemplateexception","title":"MissingTemplateException","text":"<p>Namespace: LLama.Exceptions</p> <p><code>llama_decode</code> return a non-zero status code</p> <pre><code>public class MissingTemplateException : RuntimeError, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 RuntimeError \u2192 MissingTemplateException Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#missingtemplateexception_1","title":"MissingTemplateException()","text":"<pre><code>public MissingTemplateException()\n</code></pre>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#missingtemplateexceptionstring","title":"MissingTemplateException(String)","text":"<pre><code>public MissingTemplateException(string message)\n</code></pre>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#parameters","title":"Parameters","text":"<p><code>message</code> String</p>"},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#events","title":"Events","text":""},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.exceptions.missingtemplateexception/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/","title":"llama.exceptions.runtimeerror","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#runtimeerror","title":"RuntimeError","text":"<p>Namespace: LLama.Exceptions</p> <p>Base class for LLamaSharp runtime errors (i.e. errors produced by llama.cpp, converted into exceptions)</p> <pre><code>public class RuntimeError : System.Exception, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 RuntimeError Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.runtimeerror/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.exceptions.runtimeerror/#runtimeerrorstring","title":"RuntimeError(String)","text":"<p>Create a new RuntimeError</p> <pre><code>public RuntimeError(string message)\n</code></pre>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#parameters","title":"Parameters","text":"<p><code>message</code> String</p>"},{"location":"xmldocs/llama.exceptions.runtimeerror/#events","title":"Events","text":""},{"location":"xmldocs/llama.exceptions.runtimeerror/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.exceptions.runtimeerror/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/","title":"Llama.exceptions.templatenotfoundexception","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#templatenotfoundexception","title":"TemplateNotFoundException","text":"<p>Namespace: LLama.Exceptions</p> <p><code>llama_decode</code> return a non-zero status code</p> <pre><code>public class TemplateNotFoundException : RuntimeError, System.Runtime.Serialization.ISerializable\n</code></pre> <p>Inheritance Object \u2192 Exception \u2192 RuntimeError \u2192 TemplateNotFoundException Implements ISerializable</p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#targetsite","title":"TargetSite","text":"<pre><code>public MethodBase TargetSite { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#property-value","title":"Property Value","text":"<p>MethodBase</p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#message","title":"Message","text":"<pre><code>public string Message { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#data","title":"Data","text":"<pre><code>public IDictionary Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#property-value_2","title":"Property Value","text":"<p>IDictionary</p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#innerexception","title":"InnerException","text":"<pre><code>public Exception InnerException { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#property-value_3","title":"Property Value","text":"<p>Exception</p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#helplink","title":"HelpLink","text":"<pre><code>public string HelpLink { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#property-value_4","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#source","title":"Source","text":"<pre><code>public string Source { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#hresult","title":"HResult","text":"<pre><code>public int HResult { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#stacktrace","title":"StackTrace","text":"<pre><code>public string StackTrace { get; }\n</code></pre>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#property-value_7","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#templatenotfoundexceptionstring","title":"TemplateNotFoundException(String)","text":"<pre><code>public TemplateNotFoundException(string name)\n</code></pre>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#parameters","title":"Parameters","text":"<p><code>name</code> String</p>"},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#events","title":"Events","text":""},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#serializeobjectstate","title":"SerializeObjectState","text":""},{"location":"xmldocs/llama.exceptions.templatenotfoundexception/#caution","title":"Caution","text":"<p>BinaryFormatter serialization is obsolete and should not be used. See https://aka.ms/binaryformatter for more information.</p> <pre><code>protected event EventHandler&lt;SafeSerializationEventArgs&gt; SerializeObjectState;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.extensions.icontextparamsextensions/","title":"llama.extensions.icontextparamsextensions","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.extensions.icontextparamsextensions/#icontextparamsextensions","title":"IContextParamsExtensions","text":"<p>Namespace: LLama.Extensions</p> <p>Extension methods to the IContextParams interface</p> <pre><code>public static class IContextParamsExtensions\n</code></pre> <p>Inheritance Object \u2192 IContextParamsExtensions Attributes ExtensionAttribute</p>"},{"location":"xmldocs/llama.extensions.icontextparamsextensions/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.extensions.icontextparamsextensions/#tollamacontextparamsicontextparams-llamacontextparams","title":"ToLlamaContextParams(IContextParams, LLamaContextParams&amp;)","text":"<p>Convert the given <code>IModelParams</code> into a <code>LLamaContextParams</code></p> <pre><code>public static void ToLlamaContextParams(IContextParams params, LLamaContextParams&amp; result)\n</code></pre>"},{"location":"xmldocs/llama.extensions.icontextparamsextensions/#parameters","title":"Parameters","text":"<p><code>params</code> IContextParams</p> <p><code>result</code> LLamaContextParams&amp;</p>"},{"location":"xmldocs/llama.extensions.icontextparamsextensions/#exceptions","title":"Exceptions","text":"<p>FileNotFoundException</p> <p>ArgumentException</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.extensions.imodelparamsextensions/","title":"llama.extensions.imodelparamsextensions","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.extensions.imodelparamsextensions/#imodelparamsextensions","title":"IModelParamsExtensions","text":"<p>Namespace: LLama.Extensions</p> <p>Extension methods to the IModelParams interface</p> <pre><code>public static class IModelParamsExtensions\n</code></pre> <p>Inheritance Object \u2192 IModelParamsExtensions Attributes NullableContextAttribute, NullableAttribute, ExtensionAttribute</p>"},{"location":"xmldocs/llama.extensions.imodelparamsextensions/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.extensions.imodelparamsextensions/#tollamamodelparamsimodelparams-llamamodelparams","title":"ToLlamaModelParams(IModelParams, LLamaModelParams&amp;)","text":"<p>Convert the given <code>IModelParams</code> into a <code>LLamaModelParams</code></p> <pre><code>public static IDisposable ToLlamaModelParams(IModelParams params, LLamaModelParams&amp; result)\n</code></pre>"},{"location":"xmldocs/llama.extensions.imodelparamsextensions/#parameters","title":"Parameters","text":"<p><code>params</code> IModelParams</p> <p><code>result</code> LLamaModelParams&amp;</p>"},{"location":"xmldocs/llama.extensions.imodelparamsextensions/#returns","title":"Returns","text":"<p>IDisposable</p>"},{"location":"xmldocs/llama.extensions.imodelparamsextensions/#exceptions","title":"Exceptions","text":"<p>FileNotFoundException</p> <p>ArgumentException</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/","title":"Llama.extensions.spannormalizationextensions","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#spannormalizationextensions","title":"SpanNormalizationExtensions","text":"<p>Namespace: LLama.Extensions</p> <p>Extensions to span which apply in-place normalization</p> <pre><code>public static class SpanNormalizationExtensions\n</code></pre> <p>Inheritance Object \u2192 SpanNormalizationExtensions Attributes ExtensionAttribute</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#maxabsolutenormalizationsingle","title":"MaxAbsoluteNormalization(Single[])","text":"<p>In-place multiple every element by 32760 and divide every element in the span by the max absolute value in the span</p> <pre><code>public static Single[] MaxAbsoluteNormalization(Single[] vector)\n</code></pre>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#parameters","title":"Parameters","text":"<p><code>vector</code> Single[]</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#returns","title":"Returns","text":"<p>Single[] The same array</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#maxabsolutenormalizationspansingle","title":"MaxAbsoluteNormalization(Span&lt;Single&gt;)","text":"<p>In-place multiple every element by 32760 and divide every element in the span by the max absolute value in the span</p> <pre><code>public static Span&lt;float&gt; MaxAbsoluteNormalization(Span&lt;float&gt; vector)\n</code></pre>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#parameters_1","title":"Parameters","text":"<p><code>vector</code> Span&lt;Single&gt;</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#returns_1","title":"Returns","text":"<p>Span&lt;Single&gt; The same span</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#taxicabnormalizationsingle","title":"TaxicabNormalization(Single[])","text":"<p>In-place divide every element in the array by the sum of absolute values in the array</p> <pre><code>public static Single[] TaxicabNormalization(Single[] vector)\n</code></pre>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#parameters_2","title":"Parameters","text":"<p><code>vector</code> Single[]</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#returns_2","title":"Returns","text":"<p>Single[] The same array</p> <p>Remarks:</p> <p>Also known as \"Manhattan normalization\".</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#taxicabnormalizationspansingle","title":"TaxicabNormalization(Span&lt;Single&gt;)","text":"<p>In-place divide every element in the span by the sum of absolute values in the span</p> <pre><code>public static Span&lt;float&gt; TaxicabNormalization(Span&lt;float&gt; vector)\n</code></pre>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#parameters_3","title":"Parameters","text":"<p><code>vector</code> Span&lt;Single&gt;</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#returns_3","title":"Returns","text":"<p>Span&lt;Single&gt; The same span</p> <p>Remarks:</p> <p>Also known as \"Manhattan normalization\".</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#euclideannormalizationsingle","title":"EuclideanNormalization(Single[])","text":"<p>In-place divide every element by the euclidean length of the vector</p> <pre><code>public static Single[] EuclideanNormalization(Single[] vector)\n</code></pre>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#parameters_4","title":"Parameters","text":"<p><code>vector</code> Single[]</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#returns_4","title":"Returns","text":"<p>Single[] The same array</p> <p>Remarks:</p> <p>Also known as \"L2 normalization\".</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#euclideannormalizationspansingle","title":"EuclideanNormalization(Span&lt;Single&gt;)","text":"<p>In-place divide every element by the euclidean length of the vector</p> <pre><code>public static Span&lt;float&gt; EuclideanNormalization(Span&lt;float&gt; vector)\n</code></pre>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#parameters_5","title":"Parameters","text":"<p><code>vector</code> Span&lt;Single&gt;</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#returns_5","title":"Returns","text":"<p>Span&lt;Single&gt; The same span</p> <p>Remarks:</p> <p>Also known as \"L2 normalization\".</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#euclideannormalizationreadonlyspansingle","title":"EuclideanNormalization(ReadOnlySpan&lt;Single&gt;)","text":"<p>Creates a new array containing an L2 normalization of the input vector.</p> <pre><code>public static Single[] EuclideanNormalization(ReadOnlySpan&lt;float&gt; vector)\n</code></pre>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#parameters_6","title":"Parameters","text":"<p><code>vector</code> ReadOnlySpan&lt;Single&gt;</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#returns_6","title":"Returns","text":"<p>Single[] The same span</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#pnormalizationsingle-int32","title":"PNormalization(Single[], Int32)","text":"<p>In-place apply p-normalization. https://en.wikipedia.org/wiki/Norm_(mathematics)#p-norm</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#-","title":"-","text":"<p>-</p> <pre><code>public static Single[] PNormalization(Single[] vector, int p)\n</code></pre>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#parameters_7","title":"Parameters","text":"<p><code>vector</code> Single[]</p> <p><code>p</code> Int32</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#returns_7","title":"Returns","text":"<p>Single[] The same array</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#pnormalizationspansingle-int32","title":"PNormalization(Span&lt;Single&gt;, Int32)","text":"<p>In-place apply p-normalization. https://en.wikipedia.org/wiki/Norm_(mathematics)#p-norm</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#-_1","title":"-","text":"<p>-</p> <pre><code>public static Span&lt;float&gt; PNormalization(Span&lt;float&gt; vector, int p)\n</code></pre>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#parameters_8","title":"Parameters","text":"<p><code>vector</code> Span&lt;Single&gt;</p> <p><code>p</code> Int32</p>"},{"location":"xmldocs/llama.extensions.spannormalizationextensions/#returns_8","title":"Returns","text":"<p>Span&lt;Single&gt; The same span</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.instructexecutor/","title":"Llama.instructexecutor","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.instructexecutor/#instructexecutor","title":"InstructExecutor","text":"<p>Namespace: LLama</p> <p>The LLama executor for instruct mode.</p> <pre><code>public class InstructExecutor : StatefulExecutorBase, LLama.Abstractions.ILLamaExecutor\n</code></pre> <p>Inheritance Object \u2192 StatefulExecutorBase \u2192 InstructExecutor Implements ILLamaExecutor Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.instructexecutor/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.instructexecutor/#_logger","title":"_logger","text":"<p>The logger used by this executor.</p> <pre><code>protected ILogger _logger;\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#_pasttokenscount","title":"_pastTokensCount","text":"<p>The tokens that were already processed by the model.</p> <pre><code>protected int _pastTokensCount;\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#_consumedtokenscount","title":"_consumedTokensCount","text":"<p>The tokens that were consumed by the model during the current inference.</p> <pre><code>protected int _consumedTokensCount;\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#_n_session_consumed","title":"_n_session_consumed","text":"<pre><code>protected int _n_session_consumed;\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#_n_matching_session_tokens","title":"_n_matching_session_tokens","text":"<pre><code>protected int _n_matching_session_tokens;\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#_pathsession","title":"_pathSession","text":"<p>The path of the session file.</p> <pre><code>protected string _pathSession;\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#_embeds","title":"_embeds","text":"<p>A container of the tokens to be processed and after processed.</p> <pre><code>protected List&lt;LLamaToken&gt; _embeds;\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#_embed_inps","title":"_embed_inps","text":"<p>A container for the tokens of input.</p> <pre><code>protected List&lt;LLamaToken&gt; _embed_inps;\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#_session_tokens","title":"_session_tokens","text":"<pre><code>protected List&lt;LLamaToken&gt; _session_tokens;\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#_last_n_tokens","title":"_last_n_tokens","text":"<p>The last tokens generated by the model.</p> <pre><code>protected FixedSizeQueue&lt;LLamaToken&gt; _last_n_tokens;\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.instructexecutor/#context","title":"Context","text":"<p>The context used by the executor.</p> <pre><code>public LLamaContext Context { get; }\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#property-value","title":"Property Value","text":"<p>LLamaContext</p>"},{"location":"xmldocs/llama.instructexecutor/#ismultimodal","title":"IsMultiModal","text":"<pre><code>public bool IsMultiModal { get; }\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.instructexecutor/#clipmodel","title":"ClipModel","text":"<pre><code>public LLavaWeights ClipModel { get; }\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#property-value_2","title":"Property Value","text":"<p>LLavaWeights</p>"},{"location":"xmldocs/llama.instructexecutor/#images","title":"Images","text":"<pre><code>public List&lt;Byte[]&gt; Images { get; }\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#property-value_3","title":"Property Value","text":"<p>List&lt;Byte[]&gt;</p>"},{"location":"xmldocs/llama.instructexecutor/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.instructexecutor/#instructexecutorllamacontext-string-string-ilogger","title":"InstructExecutor(LLamaContext, String, String, ILogger)","text":"<pre><code>public InstructExecutor(LLamaContext context, string instructionPrefix, string instructionSuffix, ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#parameters","title":"Parameters","text":"<p><code>context</code> LLamaContext</p> <p><code>instructionPrefix</code> String</p> <p><code>instructionSuffix</code> String</p> <p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.instructexecutor/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.instructexecutor/#getstatedata","title":"GetStateData()","text":"<pre><code>public ExecutorBaseState GetStateData()\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#returns","title":"Returns","text":"<p>ExecutorBaseState</p>"},{"location":"xmldocs/llama.instructexecutor/#loadstateexecutorbasestate","title":"LoadState(ExecutorBaseState)","text":"<pre><code>public Task LoadState(ExecutorBaseState data)\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#parameters_1","title":"Parameters","text":"<p><code>data</code> ExecutorBaseState</p>"},{"location":"xmldocs/llama.instructexecutor/#returns_1","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.instructexecutor/#savestatestring","title":"SaveState(String)","text":"<pre><code>public Task SaveState(string filename)\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#parameters_2","title":"Parameters","text":"<p><code>filename</code> String</p>"},{"location":"xmldocs/llama.instructexecutor/#returns_2","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.instructexecutor/#loadstatestring","title":"LoadState(String)","text":"<pre><code>public Task LoadState(string filename)\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#parameters_3","title":"Parameters","text":"<p><code>filename</code> String</p>"},{"location":"xmldocs/llama.instructexecutor/#returns_3","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.instructexecutor/#getloopconditioninferstateargs","title":"GetLoopCondition(InferStateArgs)","text":"<pre><code>protected Task&lt;bool&gt; GetLoopCondition(InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#parameters_4","title":"Parameters","text":"<p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.instructexecutor/#returns_4","title":"Returns","text":"<p>Task&lt;Boolean&gt;</p>"},{"location":"xmldocs/llama.instructexecutor/#preprocessinputsstring-inferstateargs","title":"PreprocessInputs(String, InferStateArgs)","text":"<pre><code>protected Task PreprocessInputs(string text, InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#parameters_5","title":"Parameters","text":"<p><code>text</code> String</p> <p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.instructexecutor/#returns_5","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.instructexecutor/#postprocessiinferenceparams-inferstateargs","title":"PostProcess(IInferenceParams, InferStateArgs)","text":"<pre><code>protected Task&lt;ValueTuple&lt;bool, IReadOnlyList&lt;string&gt;&gt;&gt; PostProcess(IInferenceParams inferenceParams, InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#parameters_6","title":"Parameters","text":"<p><code>inferenceParams</code> IInferenceParams</p> <p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.instructexecutor/#returns_6","title":"Returns","text":"<p>Task&lt;ValueTuple&lt;Boolean, IReadOnlyList&lt;String&gt;&gt;&gt;</p>"},{"location":"xmldocs/llama.instructexecutor/#inferinternaliinferenceparams-inferstateargs","title":"InferInternal(IInferenceParams, InferStateArgs)","text":"<pre><code>protected Task InferInternal(IInferenceParams inferenceParams, InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.instructexecutor/#parameters_7","title":"Parameters","text":"<p><code>inferenceParams</code> IInferenceParams</p> <p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.instructexecutor/#returns_7","title":"Returns","text":"<p>Task</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.interactiveexecutor/","title":"Llama.interactiveexecutor","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.interactiveexecutor/#interactiveexecutor","title":"InteractiveExecutor","text":"<p>Namespace: LLama</p> <p>The LLama executor for interactive mode.</p> <pre><code>public class InteractiveExecutor : StatefulExecutorBase, LLama.Abstractions.ILLamaExecutor\n</code></pre> <p>Inheritance Object \u2192 StatefulExecutorBase \u2192 InteractiveExecutor Implements ILLamaExecutor Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.interactiveexecutor/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.interactiveexecutor/#_logger","title":"_logger","text":"<p>The logger used by this executor.</p> <pre><code>protected ILogger _logger;\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#_pasttokenscount","title":"_pastTokensCount","text":"<p>The tokens that were already processed by the model.</p> <pre><code>protected int _pastTokensCount;\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#_consumedtokenscount","title":"_consumedTokensCount","text":"<p>The tokens that were consumed by the model during the current inference.</p> <pre><code>protected int _consumedTokensCount;\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#_n_session_consumed","title":"_n_session_consumed","text":"<pre><code>protected int _n_session_consumed;\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#_n_matching_session_tokens","title":"_n_matching_session_tokens","text":"<pre><code>protected int _n_matching_session_tokens;\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#_pathsession","title":"_pathSession","text":"<p>The path of the session file.</p> <pre><code>protected string _pathSession;\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#_embeds","title":"_embeds","text":"<p>A container of the tokens to be processed and after processed.</p> <pre><code>protected List&lt;LLamaToken&gt; _embeds;\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#_embed_inps","title":"_embed_inps","text":"<p>A container for the tokens of input.</p> <pre><code>protected List&lt;LLamaToken&gt; _embed_inps;\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#_session_tokens","title":"_session_tokens","text":"<pre><code>protected List&lt;LLamaToken&gt; _session_tokens;\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#_last_n_tokens","title":"_last_n_tokens","text":"<p>The last tokens generated by the model.</p> <pre><code>protected FixedSizeQueue&lt;LLamaToken&gt; _last_n_tokens;\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.interactiveexecutor/#context","title":"Context","text":"<p>The context used by the executor.</p> <pre><code>public LLamaContext Context { get; }\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#property-value","title":"Property Value","text":"<p>LLamaContext</p>"},{"location":"xmldocs/llama.interactiveexecutor/#ismultimodal","title":"IsMultiModal","text":"<pre><code>public bool IsMultiModal { get; }\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.interactiveexecutor/#clipmodel","title":"ClipModel","text":"<pre><code>public LLavaWeights ClipModel { get; }\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#property-value_2","title":"Property Value","text":"<p>LLavaWeights</p>"},{"location":"xmldocs/llama.interactiveexecutor/#images","title":"Images","text":"<pre><code>public List&lt;Byte[]&gt; Images { get; }\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#property-value_3","title":"Property Value","text":"<p>List&lt;Byte[]&gt;</p>"},{"location":"xmldocs/llama.interactiveexecutor/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.interactiveexecutor/#interactiveexecutorllamacontext-ilogger","title":"InteractiveExecutor(LLamaContext, ILogger)","text":"<pre><code>public InteractiveExecutor(LLamaContext context, ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#parameters","title":"Parameters","text":"<p><code>context</code> LLamaContext</p> <p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.interactiveexecutor/#interactiveexecutorllamacontext-llavaweights-ilogger","title":"InteractiveExecutor(LLamaContext, LLavaWeights, ILogger)","text":"<pre><code>public InteractiveExecutor(LLamaContext context, LLavaWeights clipModel, ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#parameters_1","title":"Parameters","text":"<p><code>context</code> LLamaContext</p> <p><code>clipModel</code> LLavaWeights</p> <p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.interactiveexecutor/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.interactiveexecutor/#getstatedata","title":"GetStateData()","text":"<pre><code>public ExecutorBaseState GetStateData()\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#returns","title":"Returns","text":"<p>ExecutorBaseState</p>"},{"location":"xmldocs/llama.interactiveexecutor/#loadstateexecutorbasestate","title":"LoadState(ExecutorBaseState)","text":"<pre><code>public Task LoadState(ExecutorBaseState data)\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#parameters_2","title":"Parameters","text":"<p><code>data</code> ExecutorBaseState</p>"},{"location":"xmldocs/llama.interactiveexecutor/#returns_1","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.interactiveexecutor/#savestatestring","title":"SaveState(String)","text":"<pre><code>public Task SaveState(string filename)\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#parameters_3","title":"Parameters","text":"<p><code>filename</code> String</p>"},{"location":"xmldocs/llama.interactiveexecutor/#returns_2","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.interactiveexecutor/#loadstatestring","title":"LoadState(String)","text":"<pre><code>public Task LoadState(string filename)\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#parameters_4","title":"Parameters","text":"<p><code>filename</code> String</p>"},{"location":"xmldocs/llama.interactiveexecutor/#returns_3","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.interactiveexecutor/#getloopconditioninferstateargs","title":"GetLoopCondition(InferStateArgs)","text":"<p>Define whether to continue the loop to generate responses.</p> <pre><code>protected Task&lt;bool&gt; GetLoopCondition(InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#parameters_5","title":"Parameters","text":"<p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.interactiveexecutor/#returns_4","title":"Returns","text":"<p>Task&lt;Boolean&gt;</p>"},{"location":"xmldocs/llama.interactiveexecutor/#preprocessinputsstring-inferstateargs","title":"PreprocessInputs(String, InferStateArgs)","text":"<pre><code>protected Task PreprocessInputs(string text, InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#parameters_6","title":"Parameters","text":"<p><code>text</code> String</p> <p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.interactiveexecutor/#returns_5","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.interactiveexecutor/#postprocessiinferenceparams-inferstateargs","title":"PostProcess(IInferenceParams, InferStateArgs)","text":"<p>Return whether to break the generation.</p> <pre><code>protected Task&lt;ValueTuple&lt;bool, IReadOnlyList&lt;string&gt;&gt;&gt; PostProcess(IInferenceParams inferenceParams, InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#parameters_7","title":"Parameters","text":"<p><code>inferenceParams</code> IInferenceParams</p> <p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.interactiveexecutor/#returns_6","title":"Returns","text":"<p>Task&lt;ValueTuple&lt;Boolean, IReadOnlyList&lt;String&gt;&gt;&gt;</p>"},{"location":"xmldocs/llama.interactiveexecutor/#inferinternaliinferenceparams-inferstateargs","title":"InferInternal(IInferenceParams, InferStateArgs)","text":"<pre><code>protected Task InferInternal(IInferenceParams inferenceParams, InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.interactiveexecutor/#parameters_8","title":"Parameters","text":"<p><code>inferenceParams</code> IInferenceParams</p> <p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.interactiveexecutor/#returns_7","title":"Returns","text":"<p>Task</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamacontext/","title":"Llama.llamacontext","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamacontext/#llamacontext","title":"LLamaContext","text":"<p>Namespace: LLama</p> <p>A llama_context, which holds all the context required to interact with a model</p> <pre><code>public sealed class LLamaContext : System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 LLamaContext Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.llamacontext/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.llamacontext/#contextsize","title":"ContextSize","text":"<p>Total number of tokens in the context</p> <pre><code>public uint ContextSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#property-value","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.llamacontext/#embeddingsize","title":"EmbeddingSize","text":"<p>Dimension of embedding vectors</p> <pre><code>public int EmbeddingSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamacontext/#params","title":"Params","text":"<p>The context params set for this context</p> <pre><code>public IContextParams Params { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#property-value_2","title":"Property Value","text":"<p>IContextParams</p>"},{"location":"xmldocs/llama.llamacontext/#nativehandle","title":"NativeHandle","text":"<p>The native handle, which is used to be passed to the native APIs</p> <pre><code>public SafeLLamaContextHandle NativeHandle { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#property-value_3","title":"Property Value","text":"<p>SafeLLamaContextHandle</p> <p>Remarks:</p> <p>Be careful how you use this!</p>"},{"location":"xmldocs/llama.llamacontext/#encoding","title":"Encoding","text":"<p>The encoding set for this model to deal with text input.</p> <pre><code>public Encoding Encoding { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#property-value_4","title":"Property Value","text":"<p>Encoding</p>"},{"location":"xmldocs/llama.llamacontext/#generationthreads","title":"GenerationThreads","text":"<p>Get or set the number of threads to use for generation</p> <pre><code>public int GenerationThreads { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#property-value_5","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamacontext/#batchthreads","title":"BatchThreads","text":"<p>Get or set the number of threads to use for batch processing</p> <pre><code>public int BatchThreads { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamacontext/#batchsize","title":"BatchSize","text":"<p>Get the maximum batch size for this context</p> <pre><code>public uint BatchSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#property-value_7","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.llamacontext/#vocab","title":"Vocab","text":"<p>Get the special tokens for the model associated with this context</p> <pre><code>public Vocabulary Vocab { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#property-value_8","title":"Property Value","text":"<p>Vocabulary</p>"},{"location":"xmldocs/llama.llamacontext/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamacontext/#llamacontextllamaweights-icontextparams-ilogger","title":"LLamaContext(LLamaWeights, IContextParams, ILogger)","text":"<p>Create a new LLamaContext for the given LLamaWeights</p> <pre><code>public LLamaContext(LLamaWeights model, IContextParams params, ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters","title":"Parameters","text":"<p><code>model</code> LLamaWeights</p> <p><code>params</code> IContextParams</p> <p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.llamacontext/#exceptions","title":"Exceptions","text":"<p>ObjectDisposedException</p>"},{"location":"xmldocs/llama.llamacontext/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamacontext/#tokenizestring-boolean-boolean","title":"Tokenize(String, Boolean, Boolean)","text":"<p>Tokenize a string.</p> <pre><code>public LLamaToken[] Tokenize(string text, bool addBos, bool special)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_1","title":"Parameters","text":"<p><code>text</code> String</p> <p><code>addBos</code> Boolean Whether to add a bos to the text.</p> <p><code>special</code> Boolean Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.</p>"},{"location":"xmldocs/llama.llamacontext/#returns","title":"Returns","text":"<p>LLamaToken[]</p>"},{"location":"xmldocs/llama.llamacontext/#detokenizeireadonlylistllamatoken","title":"DeTokenize(IReadOnlyList&lt;LLamaToken&gt;)","text":""},{"location":"xmldocs/llama.llamacontext/#caution","title":"Caution","text":"<p>Use a <code>StreamingTokenDecoder</code> instead</p> <p>Detokenize the tokens to text.</p> <pre><code>public string DeTokenize(IReadOnlyList&lt;LLamaToken&gt; tokens)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_2","title":"Parameters","text":"<p><code>tokens</code> IReadOnlyList&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.llamacontext/#returns_1","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.llamacontext/#savestatestring","title":"SaveState(String)","text":"<p>Save the state to specified path.</p> <pre><code>public void SaveState(string filename)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_3","title":"Parameters","text":"<p><code>filename</code> String</p>"},{"location":"xmldocs/llama.llamacontext/#savestatestring-llamaseqid","title":"SaveState(String, LLamaSeqId)","text":"<p>Save the state of a particular sequence to specified path.</p> <pre><code>public void SaveState(string filename, LLamaSeqId sequence)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_4","title":"Parameters","text":"<p><code>filename</code> String</p> <p><code>sequence</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.llamacontext/#getstate","title":"GetState()","text":"<p>Get the state data as an opaque handle, which can be loaded later using LLamaContext.LoadState(String)</p> <pre><code>public State GetState()\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#returns_2","title":"Returns","text":"<p>State</p> <p>Remarks:</p> <p>Use LLamaContext.SaveState(String) if you intend to save this state to disk.</p>"},{"location":"xmldocs/llama.llamacontext/#getstatellamaseqid","title":"GetState(LLamaSeqId)","text":"<p>Get the state data as an opaque handle, which can be loaded later using LLamaContext.LoadState(String)</p> <pre><code>public SequenceState GetState(LLamaSeqId sequence)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_5","title":"Parameters","text":"<p><code>sequence</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.llamacontext/#returns_3","title":"Returns","text":"<p>SequenceState</p> <p>Remarks:</p> <p>Use LLamaContext.SaveState(String, LLamaSeqId) if you intend to save this state to disk.</p>"},{"location":"xmldocs/llama.llamacontext/#loadstatestring","title":"LoadState(String)","text":"<p>Load the state from specified path.</p> <pre><code>public void LoadState(string filename)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_6","title":"Parameters","text":"<p><code>filename</code> String</p>"},{"location":"xmldocs/llama.llamacontext/#loadstatestring-llamaseqid","title":"LoadState(String, LLamaSeqId)","text":"<p>Load the state from specified path into a particular sequence</p> <pre><code>public void LoadState(string filename, LLamaSeqId sequence)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_7","title":"Parameters","text":"<p><code>filename</code> String</p> <p><code>sequence</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.llamacontext/#loadstatestate","title":"LoadState(State)","text":"<p>Load the state from memory.</p> <pre><code>public void LoadState(State state)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_8","title":"Parameters","text":"<p><code>state</code> State</p>"},{"location":"xmldocs/llama.llamacontext/#loadstatesequencestate-llamaseqid","title":"LoadState(SequenceState, LLamaSeqId)","text":"<p>Load the state from memory into a particular sequence</p> <pre><code>public void LoadState(SequenceState state, LLamaSeqId sequence)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_9","title":"Parameters","text":"<p><code>state</code> SequenceState</p> <p><code>sequence</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.llamacontext/#encodellamabatch","title":"Encode(LLamaBatch)","text":"<pre><code>public EncodeResult Encode(LLamaBatch batch)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_10","title":"Parameters","text":"<p><code>batch</code> LLamaBatch</p>"},{"location":"xmldocs/llama.llamacontext/#returns_4","title":"Returns","text":"<p>EncodeResult</p>"},{"location":"xmldocs/llama.llamacontext/#encodeasyncllamabatch-cancellationtoken","title":"EncodeAsync(LLamaBatch, CancellationToken)","text":"<pre><code>public Task&lt;EncodeResult&gt; EncodeAsync(LLamaBatch batch, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_11","title":"Parameters","text":"<p><code>batch</code> LLamaBatch</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.llamacontext/#returns_5","title":"Returns","text":"<p>Task&lt;EncodeResult&gt;</p>"},{"location":"xmldocs/llama.llamacontext/#decodellamabatch","title":"Decode(LLamaBatch)","text":"<pre><code>public DecodeResult Decode(LLamaBatch batch)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_12","title":"Parameters","text":"<p><code>batch</code> LLamaBatch</p>"},{"location":"xmldocs/llama.llamacontext/#returns_6","title":"Returns","text":"<p>DecodeResult</p>"},{"location":"xmldocs/llama.llamacontext/#decodeasyncllamabatch-cancellationtoken","title":"DecodeAsync(LLamaBatch, CancellationToken)","text":"<pre><code>public Task&lt;DecodeResult&gt; DecodeAsync(LLamaBatch batch, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_13","title":"Parameters","text":"<p><code>batch</code> LLamaBatch</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.llamacontext/#returns_7","title":"Returns","text":"<p>Task&lt;DecodeResult&gt;</p>"},{"location":"xmldocs/llama.llamacontext/#decodellamabatchembeddings","title":"Decode(LLamaBatchEmbeddings)","text":"<pre><code>public DecodeResult Decode(LLamaBatchEmbeddings batch)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_14","title":"Parameters","text":"<p><code>batch</code> LLamaBatchEmbeddings</p>"},{"location":"xmldocs/llama.llamacontext/#returns_8","title":"Returns","text":"<p>DecodeResult</p>"},{"location":"xmldocs/llama.llamacontext/#decodeasyncllamabatchembeddings-cancellationtoken","title":"DecodeAsync(LLamaBatchEmbeddings, CancellationToken)","text":"<pre><code>public Task&lt;DecodeResult&gt; DecodeAsync(LLamaBatchEmbeddings batch, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_15","title":"Parameters","text":"<p><code>batch</code> LLamaBatchEmbeddings</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.llamacontext/#returns_9","title":"Returns","text":"<p>Task&lt;DecodeResult&gt;</p>"},{"location":"xmldocs/llama.llamacontext/#decodeasynclistllamatoken-llamaseqid-llamabatch-int32","title":"DecodeAsync(List&lt;LLamaToken&gt;, LLamaSeqId, LLamaBatch, Int32)","text":"<pre><code>public Task&lt;ValueTuple&lt;DecodeResult, int, int&gt;&gt; DecodeAsync(List&lt;LLamaToken&gt; tokens, LLamaSeqId id, LLamaBatch batch, int n_past)\n</code></pre>"},{"location":"xmldocs/llama.llamacontext/#parameters_16","title":"Parameters","text":"<p><code>tokens</code> List&lt;LLamaToken&gt;</p> <p><code>id</code> LLamaSeqId</p> <p><code>batch</code> LLamaBatch</p> <p><code>n_past</code> Int32</p>"},{"location":"xmldocs/llama.llamacontext/#returns_10","title":"Returns","text":"<p>Task&lt;ValueTuple&lt;DecodeResult, Int32, Int32&gt;&gt; A tuple, containing the decode result, the number of tokens that have not been decoded yet and the total number of tokens that have been decoded.</p>"},{"location":"xmldocs/llama.llamacontext/#dispose","title":"Dispose()","text":"<pre><code>public void Dispose()\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamaembedder/","title":"llama.llamaembedder","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamaembedder/#llamaembedder","title":"LLamaEmbedder","text":"<p>Namespace: LLama</p> <p>Generate high dimensional embedding vectors from text</p> <pre><code>public sealed class LLamaEmbedder : System.IDisposable, Microsoft.Extensions.AI.IEmbeddingGenerator`2[[System.String, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e],[Microsoft.Extensions.AI.Embedding`1[[System.Single, System.Private.CoreLib, Version=8.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]], Microsoft.Extensions.AI.Abstractions, Version=9.5.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35]], Microsoft.Extensions.AI.IEmbeddingGenerator\n</code></pre> <p>Inheritance Object \u2192 LLamaEmbedder Implements IDisposable, IEmbeddingGenerator&lt;String, Embedding&lt;Single&gt;&gt;, IEmbeddingGenerator Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.llamaembedder/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.llamaembedder/#embeddingsize","title":"EmbeddingSize","text":"<p>Dimension of embedding vectors</p> <pre><code>public int EmbeddingSize { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.llamaembedder/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamaembedder/#context","title":"Context","text":"<p>LLama Context</p> <pre><code>public LLamaContext Context { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.llamaembedder/#property-value_1","title":"Property Value","text":"<p>LLamaContext</p>"},{"location":"xmldocs/llama.llamaembedder/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamaembedder/#llamaembedderllamaweights-icontextparams-ilogger","title":"LLamaEmbedder(LLamaWeights, IContextParams, ILogger)","text":"<p>Create a new embedder, using the given LLamaWeights</p> <pre><code>public LLamaEmbedder(LLamaWeights weights, IContextParams params, ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.llamaembedder/#parameters","title":"Parameters","text":"<p><code>weights</code> LLamaWeights</p> <p><code>params</code> IContextParams</p> <p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.llamaembedder/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamaembedder/#dispose","title":"Dispose()","text":"<pre><code>public void Dispose()\n</code></pre>"},{"location":"xmldocs/llama.llamaembedder/#getembeddingsstring-cancellationtoken","title":"GetEmbeddings(String, CancellationToken)","text":"<p>Get high dimensional embedding vectors for the given text. Depending on the pooling type used when constructing  this LLamaEmbedder this may return an embedding vector per token, or one single embedding vector for the entire string.</p> <pre><code>public Task&lt;IReadOnlyList&lt;Single[]&gt;&gt; GetEmbeddings(string input, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.llamaembedder/#parameters_1","title":"Parameters","text":"<p><code>input</code> String</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.llamaembedder/#returns","title":"Returns","text":"<p>Task&lt;IReadOnlyList&lt;Single[]&gt;&gt;</p>"},{"location":"xmldocs/llama.llamaembedder/#exceptions","title":"Exceptions","text":"<p>RuntimeError</p> <p>NotSupportedException</p> <p>Remarks:</p> <p>Embedding vectors are not normalized, consider using one of the extensions in SpanNormalizationExtensions.</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamaquantizer/","title":"llama.llamaquantizer","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamaquantizer/#llamaquantizer","title":"LLamaQuantizer","text":"<p>Namespace: LLama</p> <p>The quantizer to quantize the model.</p> <pre><code>public static class LLamaQuantizer\n</code></pre> <p>Inheritance Object \u2192 LLamaQuantizer Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.llamaquantizer/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamaquantizer/#quantizestring-string-llamaftype-int32-boolean-boolean","title":"Quantize(String, String, LLamaFtype, Int32, Boolean, Boolean)","text":"<p>Quantize the model.</p> <pre><code>public static bool Quantize(string srcFileName, string dstFilename, LLamaFtype ftype, int nthread, bool allowRequantize, bool quantizeOutputTensor)\n</code></pre>"},{"location":"xmldocs/llama.llamaquantizer/#parameters","title":"Parameters","text":"<p><code>srcFileName</code> String The model file to be quantized.</p> <p><code>dstFilename</code> String The path to save the quantized model.</p> <p><code>ftype</code> LLamaFtype The type of quantization.</p> <p><code>nthread</code> Int32 Thread to be used during the quantization. By default it's the physical core number.</p> <p><code>allowRequantize</code> Boolean</p> <p><code>quantizeOutputTensor</code> Boolean</p>"},{"location":"xmldocs/llama.llamaquantizer/#returns","title":"Returns","text":"<p>Boolean Whether the quantization is successful.</p>"},{"location":"xmldocs/llama.llamaquantizer/#exceptions","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.llamaquantizer/#quantizestring-string-string-int32-boolean-boolean","title":"Quantize(String, String, String, Int32, Boolean, Boolean)","text":"<p>Quantize the model.</p> <pre><code>public static bool Quantize(string srcFileName, string dstFilename, string ftype, int nthread, bool allowRequantize, bool quantizeOutputTensor)\n</code></pre>"},{"location":"xmldocs/llama.llamaquantizer/#parameters_1","title":"Parameters","text":"<p><code>srcFileName</code> String The model file to be quantized.</p> <p><code>dstFilename</code> String The path to save the quantized model.</p> <p><code>ftype</code> String The type of quantization.</p> <p><code>nthread</code> Int32 Thread to be used during the quantization. By default it's the physical core number.</p> <p><code>allowRequantize</code> Boolean</p> <p><code>quantizeOutputTensor</code> Boolean</p>"},{"location":"xmldocs/llama.llamaquantizer/#returns_1","title":"Returns","text":"<p>Boolean Whether the quantization is successful.</p>"},{"location":"xmldocs/llama.llamaquantizer/#exceptions_1","title":"Exceptions","text":"<p>ArgumentException</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamareranker/","title":"Llama.llamareranker","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamareranker/#llamareranker","title":"LLamaReranker","text":"<p>Namespace: LLama</p> <p>Get rank scores between prompt and documents</p> <pre><code>public sealed class LLamaReranker : System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 LLamaReranker Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.llamareranker/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.llamareranker/#embeddingsize","title":"EmbeddingSize","text":"<p>Dimension of embedding vectors</p> <pre><code>public int EmbeddingSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamareranker/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamareranker/#context","title":"Context","text":"<p>LLama Context</p> <pre><code>public LLamaContext Context { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamareranker/#property-value_1","title":"Property Value","text":"<p>LLamaContext</p>"},{"location":"xmldocs/llama.llamareranker/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamareranker/#llamarerankerllamaweights-icontextparams-ilogger","title":"LLamaReranker(LLamaWeights, IContextParams, ILogger)","text":"<p>Create a new reranker, using the given LLamaWeights</p> <pre><code>public LLamaReranker(LLamaWeights weights, IContextParams params, ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.llamareranker/#parameters","title":"Parameters","text":"<p><code>weights</code> LLamaWeights</p> <p><code>params</code> IContextParams</p> <p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.llamareranker/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamareranker/#dispose","title":"Dispose()","text":"<pre><code>public void Dispose()\n</code></pre>"},{"location":"xmldocs/llama.llamareranker/#getrelevancescoresstring-ireadonlyliststring-boolean-cancellationtoken","title":"GetRelevanceScores(String, IReadOnlyList&lt;String&gt;, Boolean, CancellationToken)","text":"<p>Retrieve relevance scores for input and documents by reranking, execute once.</p> <pre><code>public Task&lt;IReadOnlyList&lt;float&gt;&gt; GetRelevanceScores(string input, IReadOnlyList&lt;string&gt; documents, bool normalize, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.llamareranker/#parameters_1","title":"Parameters","text":"<p><code>input</code> String</p> <p><code>documents</code> IReadOnlyList&lt;String&gt;</p> <p><code>normalize</code> Boolean Whether to normalize the score to the range (0, 1)</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.llamareranker/#returns","title":"Returns","text":"<p>Task&lt;IReadOnlyList&lt;Single&gt;&gt;</p>"},{"location":"xmldocs/llama.llamareranker/#exceptions","title":"Exceptions","text":"<p>RuntimeError</p> <p>NotSupportedException</p>"},{"location":"xmldocs/llama.llamareranker/#getrelevancescorewithtokencountstring-string-boolean-cancellationtoken","title":"GetRelevanceScoreWithTokenCount(String, String, Boolean, CancellationToken)","text":"<p>Retrieve relevance score for input and document by reranking</p> <pre><code>public Task&lt;ValueTuple&lt;float, int&gt;&gt; GetRelevanceScoreWithTokenCount(string input, string document, bool normalize, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.llamareranker/#parameters_2","title":"Parameters","text":"<p><code>input</code> String</p> <p><code>document</code> String</p> <p><code>normalize</code> Boolean Whether to normalize the score to the range (0, 1)</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.llamareranker/#returns_1","title":"Returns","text":"<p>Task&lt;ValueTuple&lt;Single, Int32&gt;&gt;</p>"},{"location":"xmldocs/llama.llamareranker/#exceptions_1","title":"Exceptions","text":"<p>RuntimeError</p> <p>NotSupportedException</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamatemplate/","title":"Llama.llamatemplate","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamatemplate/#llamatemplate","title":"LLamaTemplate","text":"<p>Namespace: LLama</p> <p>Converts a sequence of messages into text according to a model template</p> <pre><code>public sealed class LLamaTemplate\n</code></pre> <p>Inheritance Object \u2192 LLamaTemplate Attributes NullableContextAttribute, NullableAttribute, DefaultMemberAttribute</p>"},{"location":"xmldocs/llama.llamatemplate/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.llamatemplate/#encoding","title":"Encoding","text":"<p>The encoding algorithm to use</p> <pre><code>public static Encoding Encoding;\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.llamatemplate/#count","title":"Count","text":"<p>Number of messages added to this template</p> <pre><code>public int Count { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamatemplate/#item","title":"Item","text":"<pre><code>public TextMessage Item { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#property-value_1","title":"Property Value","text":"<p>TextMessage</p>"},{"location":"xmldocs/llama.llamatemplate/#addassistant","title":"AddAssistant","text":"<p>Whether to end the prompt with the token(s) that indicate the start of an assistant message.</p> <pre><code>public bool AddAssistant { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#property-value_2","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.llamatemplate/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamatemplate/#llamatemplatesafellamamodelhandle-string-boolean","title":"LLamaTemplate(SafeLlamaModelHandle, String, Boolean)","text":"<p>Construct a new template, using the default model template</p> <pre><code>public LLamaTemplate(SafeLlamaModelHandle model, string name, bool strict)\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#parameters","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle The native handle of the loaded model.</p> <p><code>name</code> String The name of the template, in case there are many or differently named. Set to 'null' for the default behaviour of finding an appropriate match.</p> <p><code>strict</code> Boolean Setting this to true will cause the call to throw if no valid templates are found.</p>"},{"location":"xmldocs/llama.llamatemplate/#llamatemplatellamaweights-boolean","title":"LLamaTemplate(LLamaWeights, Boolean)","text":"<p>Construct a new template, using the default model template</p> <pre><code>public LLamaTemplate(LLamaWeights weights, bool strict)\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#parameters_1","title":"Parameters","text":"<p><code>weights</code> LLamaWeights The handle of the loaded model's weights.</p> <p><code>strict</code> Boolean Setting this to true will cause the call to throw if no valid templates are found.</p>"},{"location":"xmldocs/llama.llamatemplate/#llamatemplatestring","title":"LLamaTemplate(String)","text":"<p>Construct a new template, using a custom template.</p> <pre><code>public LLamaTemplate(string customTemplate)\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#parameters_2","title":"Parameters","text":"<p><code>customTemplate</code> String</p> <p>Remarks:</p> <p>Only support a pre-defined list of templates. See more: https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template</p>"},{"location":"xmldocs/llama.llamatemplate/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamatemplate/#addstring-string","title":"Add(String, String)","text":"<p>Add a new message to the end of this template</p> <pre><code>public LLamaTemplate Add(string role, string content)\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#parameters_3","title":"Parameters","text":"<p><code>role</code> String</p> <p><code>content</code> String</p>"},{"location":"xmldocs/llama.llamatemplate/#returns","title":"Returns","text":"<p>LLamaTemplate This template, for chaining calls.</p>"},{"location":"xmldocs/llama.llamatemplate/#addtextmessage","title":"Add(TextMessage)","text":"<p>Add a new message to the end of this template</p> <pre><code>public LLamaTemplate Add(TextMessage message)\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#parameters_4","title":"Parameters","text":"<p><code>message</code> TextMessage</p>"},{"location":"xmldocs/llama.llamatemplate/#returns_1","title":"Returns","text":"<p>LLamaTemplate This template, for chaining calls.</p>"},{"location":"xmldocs/llama.llamatemplate/#removeatint32","title":"RemoveAt(Int32)","text":"<p>Remove a message at the given index</p> <pre><code>public LLamaTemplate RemoveAt(int index)\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#parameters_5","title":"Parameters","text":"<p><code>index</code> Int32</p>"},{"location":"xmldocs/llama.llamatemplate/#returns_2","title":"Returns","text":"<p>LLamaTemplate This template, for chaining calls.</p>"},{"location":"xmldocs/llama.llamatemplate/#clear","title":"Clear()","text":"<p>Remove all messages from the template and resets internal state to accept/generate new messages</p> <pre><code>public void Clear()\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#apply","title":"Apply()","text":"<p>Apply the template to the messages and return a span containing the results</p> <pre><code>public ReadOnlySpan&lt;byte&gt; Apply()\n</code></pre>"},{"location":"xmldocs/llama.llamatemplate/#returns_3","title":"Returns","text":"<p>ReadOnlySpan&lt;Byte&gt; A span over the buffer that holds the applied template</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamatransforms/","title":"llama.llamatransforms","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamatransforms/#llamatransforms","title":"LLamaTransforms","text":"<p>Namespace: LLama</p> <p>A class that contains all the transforms provided internally by LLama.</p> <pre><code>public class LLamaTransforms\n</code></pre> <p>Inheritance Object \u2192 LLamaTransforms</p>"},{"location":"xmldocs/llama.llamatransforms/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.llamatransforms/#llamatransforms_1","title":"LLamaTransforms()","text":"<pre><code>public LLamaTransforms()\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamaweights/","title":"Llama.llamaweights","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llamaweights/#llamaweights","title":"LLamaWeights","text":"<p>Namespace: LLama</p> <p>A set of model weights, loaded into memory.</p> <pre><code>public sealed class LLamaWeights : System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 LLamaWeights Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.llamaweights/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.llamaweights/#nativehandle","title":"NativeHandle","text":"<p>The native handle, which is used in the native APIs</p> <pre><code>public SafeLlamaModelHandle NativeHandle { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#property-value","title":"Property Value","text":"<p>SafeLlamaModelHandle</p> <p>Remarks:</p> <p>Be careful how you use this!</p>"},{"location":"xmldocs/llama.llamaweights/#contextsize","title":"ContextSize","text":"<p>Total number of tokens in the context</p> <pre><code>public int ContextSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamaweights/#sizeinbytes","title":"SizeInBytes","text":"<p>Get the size of this model in bytes</p> <pre><code>public ulong SizeInBytes { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#property-value_2","title":"Property Value","text":"<p>UInt64</p>"},{"location":"xmldocs/llama.llamaweights/#parametercount","title":"ParameterCount","text":"<p>Get the number of parameters in this model</p> <pre><code>public ulong ParameterCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#property-value_3","title":"Property Value","text":"<p>UInt64</p>"},{"location":"xmldocs/llama.llamaweights/#embeddingsize","title":"EmbeddingSize","text":"<p>Dimension of embedding vectors</p> <pre><code>public int EmbeddingSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#property-value_4","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.llamaweights/#vocab","title":"Vocab","text":"<p>Get the special tokens of this model</p> <pre><code>public Vocabulary Vocab { get; }\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#property-value_5","title":"Property Value","text":"<p>Vocabulary</p>"},{"location":"xmldocs/llama.llamaweights/#metadata","title":"Metadata","text":"<p>All metadata keys in this model</p> <pre><code>public IReadOnlyDictionary&lt;string, string&gt; Metadata { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#property-value_6","title":"Property Value","text":"<p>IReadOnlyDictionary&lt;String, String&gt;</p>"},{"location":"xmldocs/llama.llamaweights/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llamaweights/#loadfromfileimodelparams","title":"LoadFromFile(IModelParams)","text":"<p>Load weights into memory</p> <pre><code>public static LLamaWeights LoadFromFile(IModelParams params)\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#parameters","title":"Parameters","text":"<p><code>params</code> IModelParams</p>"},{"location":"xmldocs/llama.llamaweights/#returns","title":"Returns","text":"<p>LLamaWeights</p>"},{"location":"xmldocs/llama.llamaweights/#loadfromfileasyncimodelparams-cancellationtoken-iprogresssingle","title":"LoadFromFileAsync(IModelParams, CancellationToken, IProgress&lt;Single&gt;)","text":"<p>Load weights into memory</p> <pre><code>public static Task&lt;LLamaWeights&gt; LoadFromFileAsync(IModelParams params, CancellationToken token, IProgress&lt;float&gt; progressReporter)\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#parameters_1","title":"Parameters","text":"<p><code>params</code> IModelParams Parameters to use to load the model</p> <p><code>token</code> CancellationToken A cancellation token that can interrupt model loading</p> <p><code>progressReporter</code> IProgress&lt;Single&gt; Receives progress updates as the model loads (0 to 1)</p>"},{"location":"xmldocs/llama.llamaweights/#returns_1","title":"Returns","text":"<p>Task&lt;LLamaWeights&gt;</p>"},{"location":"xmldocs/llama.llamaweights/#exceptions","title":"Exceptions","text":"<p>LoadWeightsFailedException Thrown if weights failed to load for any reason. e.g. Invalid file format or loading cancelled.</p> <p>OperationCanceledException Thrown if the cancellation token is cancelled.</p>"},{"location":"xmldocs/llama.llamaweights/#dispose","title":"Dispose()","text":"<pre><code>public void Dispose()\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#createcontexticontextparams-ilogger","title":"CreateContext(IContextParams, ILogger)","text":"<p>Create a llama_context using this model</p> <pre><code>public LLamaContext CreateContext(IContextParams params, ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#parameters_2","title":"Parameters","text":"<p><code>params</code> IContextParams</p> <p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.llamaweights/#returns_2","title":"Returns","text":"<p>LLamaContext</p>"},{"location":"xmldocs/llama.llamaweights/#tokenizestring-boolean-boolean-encoding","title":"Tokenize(String, Boolean, Boolean, Encoding)","text":"<p>Convert a string of text into tokens</p> <pre><code>public LLamaToken[] Tokenize(string text, bool add_bos, bool special, Encoding encoding)\n</code></pre>"},{"location":"xmldocs/llama.llamaweights/#parameters_3","title":"Parameters","text":"<p><code>text</code> String</p> <p><code>add_bos</code> Boolean</p> <p><code>special</code> Boolean Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.</p> <p><code>encoding</code> Encoding</p>"},{"location":"xmldocs/llama.llamaweights/#returns_3","title":"Returns","text":"<p>LLamaToken[]</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llavaweights/","title":"llama.llavaweights","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.llavaweights/#llavaweights","title":"LLavaWeights","text":"<p>Namespace: LLama</p> <p>A set of llava model weights (mmproj), loaded into memory.</p> <pre><code>public sealed class LLavaWeights : System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 LLavaWeights Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.llavaweights/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.llavaweights/#nativehandle","title":"NativeHandle","text":"<p>The native handle, which is used in the native APIs</p> <pre><code>public SafeLlavaModelHandle NativeHandle { get; }\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#property-value","title":"Property Value","text":"<p>SafeLlavaModelHandle</p> <p>Remarks:</p> <p>Be careful how you use this!</p>"},{"location":"xmldocs/llama.llavaweights/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.llavaweights/#loadfromfilestring","title":"LoadFromFile(String)","text":"<p>Load weights into memory</p> <pre><code>public static LLavaWeights LoadFromFile(string mmProject)\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#parameters","title":"Parameters","text":"<p><code>mmProject</code> String path to the \"mmproj\" model file</p>"},{"location":"xmldocs/llama.llavaweights/#returns","title":"Returns","text":"<p>LLavaWeights</p>"},{"location":"xmldocs/llama.llavaweights/#loadfromfileasyncstring-cancellationtoken","title":"LoadFromFileAsync(String, CancellationToken)","text":"<p>Load weights into memory</p> <pre><code>public static Task&lt;LLavaWeights&gt; LoadFromFileAsync(string mmProject, CancellationToken token)\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#parameters_1","title":"Parameters","text":"<p><code>mmProject</code> String path to the \"mmproj\" model file</p> <p><code>token</code> CancellationToken</p>"},{"location":"xmldocs/llama.llavaweights/#returns_1","title":"Returns","text":"<p>Task&lt;LLavaWeights&gt;</p>"},{"location":"xmldocs/llama.llavaweights/#createimageembeddingsllamacontext-byte","title":"CreateImageEmbeddings(LLamaContext, Byte[])","text":"<p>Create the Image Embeddings from the bytes of an image.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(LLamaContext ctxLlama, Byte[] image)\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#parameters_2","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext</p> <p><code>image</code> Byte[] Image bytes. Supported formats:</p>"},{"location":"xmldocs/llama.llavaweights/#-","title":"-","text":""},{"location":"xmldocs/llama.llavaweights/#-_1","title":"-","text":""},{"location":"xmldocs/llama.llavaweights/#returns_2","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.llavaweights/#createimageembeddingsbyte-int32","title":"CreateImageEmbeddings(Byte[], Int32)","text":"<p>Create the Image Embeddings.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(Byte[] image, int threads)\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#parameters_3","title":"Parameters","text":"<p><code>image</code> Byte[] Image in binary format (it supports jpeg format only)</p> <p><code>threads</code> Int32 Number of threads to use</p>"},{"location":"xmldocs/llama.llavaweights/#returns_3","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle return the SafeHandle of these embeddings</p>"},{"location":"xmldocs/llama.llavaweights/#createimageembeddingsllamacontext-string","title":"CreateImageEmbeddings(LLamaContext, String)","text":"<p>Create the Image Embeddings from the bytes of an image.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(LLamaContext ctxLlama, string image)\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#parameters_4","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext</p> <p><code>image</code> String Path to the image file. Supported formats:</p>"},{"location":"xmldocs/llama.llavaweights/#-_2","title":"-","text":""},{"location":"xmldocs/llama.llavaweights/#-_3","title":"-","text":""},{"location":"xmldocs/llama.llavaweights/#returns_4","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.llavaweights/#exceptions","title":"Exceptions","text":"<p>InvalidOperationException</p>"},{"location":"xmldocs/llama.llavaweights/#createimageembeddingsstring-int32","title":"CreateImageEmbeddings(String, Int32)","text":"<p>Create the Image Embeddings from the bytes of an image.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(string image, int threads)\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#parameters_5","title":"Parameters","text":"<p><code>image</code> String Path to the image file. Supported formats:</p>"},{"location":"xmldocs/llama.llavaweights/#-_4","title":"-","text":""},{"location":"xmldocs/llama.llavaweights/#-_5","title":"-","text":"<p><code>threads</code> Int32</p>"},{"location":"xmldocs/llama.llavaweights/#returns_5","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.llavaweights/#exceptions_1","title":"Exceptions","text":"<p>InvalidOperationException</p>"},{"location":"xmldocs/llama.llavaweights/#evalimageembedllamacontext-safellavaimageembedhandle-int32","title":"EvalImageEmbed(LLamaContext, SafeLlavaImageEmbedHandle, Int32&amp;)","text":"<p>Eval the image embeddings</p> <pre><code>public bool EvalImageEmbed(LLamaContext ctxLlama, SafeLlavaImageEmbedHandle imageEmbed, Int32&amp; n_past)\n</code></pre>"},{"location":"xmldocs/llama.llavaweights/#parameters_6","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext</p> <p><code>imageEmbed</code> SafeLlavaImageEmbedHandle</p> <p><code>n_past</code> Int32&amp;</p>"},{"location":"xmldocs/llama.llavaweights/#returns_6","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.llavaweights/#dispose","title":"Dispose()","text":"<pre><code>public void Dispose()\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.avxlevel/","title":"Llama.native.avxlevel","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.avxlevel/#avxlevel","title":"AvxLevel","text":"<p>Namespace: LLama.Native</p> <p>Avx support configuration</p> <pre><code>public enum AvxLevel\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 AvxLevel Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.avxlevel/#fields","title":"Fields","text":"Name Value Description None 0 No AVX Avx 1 Advanced Vector Extensions (supported by most processors after 2011) Avx2 2 AVX2 (supported by most processors after 2013) Avx512 3 AVX512 (supported by some processors after 2016, not widely supported) <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.decoderesult/","title":"llama.native.decoderesult","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.decoderesult/#decoderesult","title":"DecodeResult","text":"<p>Namespace: LLama.Native</p> <p>Return codes from llama_decode</p> <pre><code>public enum DecodeResult\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 DecodeResult Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.decoderesult/#fields","title":"Fields","text":"Name Value Description Error -1 An unspecified error Ok 0 Ok. NoKvSlot 1 Could not find a KV slot for the batch (try reducing the size of the batch or increase the context) ComputeAborted 2 Compute was aborted (e.g. due to callback request or timeout) AllocationFailed -2 Failed to allocate memory or reserve output space DecodeFailed -3 General failure during decode (e.g. internal error, slot failure) <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.defaultnativelibraryselectingpolicy/","title":"Llama.native.defaultnativelibraryselectingpolicy","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.defaultnativelibraryselectingpolicy/#defaultnativelibraryselectingpolicy","title":"DefaultNativeLibrarySelectingPolicy","text":"<p>Namespace: LLama.Native</p> <pre><code>public class DefaultNativeLibrarySelectingPolicy : LLama.Abstractions.INativeLibrarySelectingPolicy\n</code></pre> <p>Inheritance Object \u2192 DefaultNativeLibrarySelectingPolicy Implements INativeLibrarySelectingPolicy</p>"},{"location":"xmldocs/llama.native.defaultnativelibraryselectingpolicy/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.defaultnativelibraryselectingpolicy/#defaultnativelibraryselectingpolicy_1","title":"DefaultNativeLibrarySelectingPolicy()","text":"<pre><code>public DefaultNativeLibrarySelectingPolicy()\n</code></pre>"},{"location":"xmldocs/llama.native.defaultnativelibraryselectingpolicy/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.defaultnativelibraryselectingpolicy/#applydescription-systeminfo-llamalogcallback","title":"Apply(Description, SystemInfo, LLamaLogCallback)","text":"<pre><code>public IEnumerable&lt;INativeLibrary&gt; Apply(Description description, SystemInfo systemInfo, LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.native.defaultnativelibraryselectingpolicy/#parameters","title":"Parameters","text":"<p><code>description</code> Description</p> <p><code>systemInfo</code> SystemInfo</p> <p><code>logCallback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.defaultnativelibraryselectingpolicy/#returns","title":"Returns","text":"<p>IEnumerable&lt;INativeLibrary&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.encoderesult/","title":"Llama.native.encoderesult","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.encoderesult/#encoderesult","title":"EncodeResult","text":"<p>Namespace: LLama.Native</p> <p>Return codes from llama_encode</p> <pre><code>public enum EncodeResult\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 EncodeResult Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.encoderesult/#fields","title":"Fields","text":"Name Value Description Error -1 An unspecified error Ok 0 Ok. <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.ggmltype/","title":"llama.native.ggmltype","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.ggmltype/#ggmltype","title":"GGMLType","text":"<p>Namespace: LLama.Native</p> <p>Possible GGML quantisation types</p> <pre><code>public enum GGMLType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 GGMLType Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.ggmltype/#fields","title":"Fields","text":"Name Value Description GGML_TYPE_F32 0 Full 32 bit float GGML_TYPE_F16 1 16 bit float GGML_TYPE_Q4_0 2 4 bit float GGML_TYPE_Q4_1 3 4 bit float GGML_TYPE_Q5_0 6 5 bit float GGML_TYPE_Q5_1 7 5 bit float GGML_TYPE_Q8_0 8 8 bit float GGML_TYPE_Q8_1 9 8 bit float GGML_TYPE_Q2_K 10 \"type-1\" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw) GGML_TYPE_Q3_K 11 \"type-0\" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw. GGML_TYPE_Q4_K 12 \"type-1\" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw. GGML_TYPE_Q5_K 13 \"type-1\" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw GGML_TYPE_Q6_K 14 \"type-0\" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw GGML_TYPE_Q8_K 15 \"type-0\" 8-bit quantization. Only used for quantizing intermediate results. The difference to the existing Q8_0 is that the block size is 256. All 2-6 bit dot products are implemented for this quantization type. GGML_TYPE_I8 16 Integer, 8 bit GGML_TYPE_I16 17 Integer, 16 bit GGML_TYPE_I32 18 Integer, 32 bit GGML_TYPE_COUNT 19 The value of this entry is the count of the number of possible quant types. <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.gpusplitmode/","title":"llama.native.gpusplitmode","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.gpusplitmode/#gpusplitmode","title":"GPUSplitMode","text":"<p>Namespace: LLama.Native</p> <pre><code>public enum GPUSplitMode\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 GPUSplitMode Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>llama_split_mode</p>"},{"location":"xmldocs/llama.native.gpusplitmode/#fields","title":"Fields","text":"Name Value Description None 0 Single GPU Layer 1 Split layers and KV across GPUs Row 2 split layers and KV across GPUs, use tensor parallelism if supported <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.icustomsampler/","title":"Llama.native.icustomsampler","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.icustomsampler/#icustomsampler","title":"ICustomSampler","text":"<p>Namespace: LLama.Native</p> <p>A custom sampler stage for modifying logits or selecting a token</p> <pre><code>public interface ICustomSampler : System.IDisposable\n</code></pre> <p>Implements IDisposable Attributes NullableContextAttribute</p>"},{"location":"xmldocs/llama.native.icustomsampler/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.icustomsampler/#name","title":"Name","text":"<p>The human readable name of this stage</p> <pre><code>public abstract string Name { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.icustomsampler/#property-value","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.native.icustomsampler/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.icustomsampler/#applyllamatokendataarraynative","title":"Apply(LLamaTokenDataArrayNative&amp;)","text":"<p>Apply this stage to a set of logits.  This can modify logits or select a token (or both).  If logits are modified the Sorted flag must be set to false.</p> <pre><code>void Apply(LLamaTokenDataArrayNative&amp; tokenData)\n</code></pre>"},{"location":"xmldocs/llama.native.icustomsampler/#parameters","title":"Parameters","text":"<p><code>tokenData</code> LLamaTokenDataArrayNative&amp;</p> <p>Remarks:</p> <p>If the logits are no longer sorted after the custom sampler has run it is critically important to  set Sorted=false. If unsure, always set it to false, this is a safe default.</p>"},{"location":"xmldocs/llama.native.icustomsampler/#acceptllamatoken","title":"Accept(LLamaToken)","text":"<p>Update the internal state of the sampler when a token is chosen</p> <pre><code>void Accept(LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.native.icustomsampler/#parameters_1","title":"Parameters","text":"<p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.native.icustomsampler/#reset","title":"Reset()","text":"<p>Reset the internal state of this sampler</p> <pre><code>void Reset()\n</code></pre>"},{"location":"xmldocs/llama.native.icustomsampler/#clone","title":"Clone()","text":"<p>Create a clone of this sampler</p> <pre><code>ICustomSampler Clone()\n</code></pre>"},{"location":"xmldocs/llama.native.icustomsampler/#returns","title":"Returns","text":"<p>ICustomSampler</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaattentiontype/","title":"Llama.native.llamaattentiontype","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaattentiontype/#llamaattentiontype","title":"LLamaAttentionType","text":"<p>Namespace: LLama.Native</p> <pre><code>public enum LLamaAttentionType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaAttentionType Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>llama_attention_type</p>"},{"location":"xmldocs/llama.native.llamaattentiontype/#fields","title":"Fields","text":"Name Value Description Unspecified -1 Unspecified attention type. The library will attempt to find the best fit Causal 0 The causal mask will be applied, causing tokens to only see previous tokens in the same sequence, and not future ones NonCausal 1 The causal mask will not be applied, and tokens of the same sequence will be able to see each other <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamabatch/","title":"llama.native.llamabatch","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamabatch/#llamabatch","title":"LLamaBatch","text":"<p>Namespace: LLama.Native</p> <p>A batch allows submitting multiple tokens to multiple sequences simultaneously</p> <pre><code>public class LLamaBatch\n</code></pre> <p>Inheritance Object \u2192 LLamaBatch Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.llamabatch/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamabatch/#tokencount","title":"TokenCount","text":"<p>The number of tokens in this batch</p> <pre><code>public int TokenCount { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamabatch/#sequencecapacity","title":"SequenceCapacity","text":"<p>Maximum number of sequences a token can be assigned to (automatically grows if exceeded)</p> <pre><code>public int SequenceCapacity { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamabatch/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.llamabatch/#llamabatch_1","title":"LLamaBatch()","text":"<p>Create a new batch for submitting inputs to llama.cpp</p> <pre><code>public LLamaBatch()\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamabatch/#addllamatoken-llamapos-readonlyspanllamaseqid-boolean","title":"Add(LLamaToken, LLamaPos, ReadOnlySpan&lt;LLamaSeqId&gt;, Boolean)","text":"<p>Add a single token to the batch at the same position in several sequences</p> <pre><code>public int Add(LLamaToken token, LLamaPos pos, ReadOnlySpan&lt;LLamaSeqId&gt; sequences, bool logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#parameters","title":"Parameters","text":"<p><code>token</code> LLamaToken The token to add</p> <p><code>pos</code> LLamaPos The position to add it att</p> <p><code>sequences</code> ReadOnlySpan&lt;LLamaSeqId&gt; The set of sequences to add this token to</p> <p><code>logits</code> Boolean</p>"},{"location":"xmldocs/llama.native.llamabatch/#returns","title":"Returns","text":"<p>Int32 The index that the token was added at. Use this for GetLogitsIth</p> <p>Remarks:</p> <p>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</p>"},{"location":"xmldocs/llama.native.llamabatch/#addllamatoken-llamapos-listllamaseqid-boolean","title":"Add(LLamaToken, LLamaPos, List&lt;LLamaSeqId&gt;, Boolean)","text":"<p>Add a single token to the batch at the same position in several sequences</p> <pre><code>public int Add(LLamaToken token, LLamaPos pos, List&lt;LLamaSeqId&gt; sequences, bool logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#parameters_1","title":"Parameters","text":"<p><code>token</code> LLamaToken The token to add</p> <p><code>pos</code> LLamaPos The position to add it att</p> <p><code>sequences</code> List&lt;LLamaSeqId&gt; The set of sequences to add this token to</p> <p><code>logits</code> Boolean</p>"},{"location":"xmldocs/llama.native.llamabatch/#returns_1","title":"Returns","text":"<p>Int32 The index that the token was added at. Use this for GetLogitsIth</p> <p>Remarks:</p> <p>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</p>"},{"location":"xmldocs/llama.native.llamabatch/#addllamatoken-llamapos-llamaseqid-boolean","title":"Add(LLamaToken, LLamaPos, LLamaSeqId, Boolean)","text":"<p>Add a single token to the batch at a certain position for a single sequences</p> <pre><code>public int Add(LLamaToken token, LLamaPos pos, LLamaSeqId sequence, bool logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#parameters_2","title":"Parameters","text":"<p><code>token</code> LLamaToken The token to add</p> <p><code>pos</code> LLamaPos The position to add it att</p> <p><code>sequence</code> LLamaSeqId The sequence to add this token to</p> <p><code>logits</code> Boolean</p>"},{"location":"xmldocs/llama.native.llamabatch/#returns_2","title":"Returns","text":"<p>Int32 The index that the token was added at. Use this for GetLogitsIth</p> <p>Remarks:</p> <p>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</p>"},{"location":"xmldocs/llama.native.llamabatch/#addrangereadonlyspanllamatoken-llamapos-llamaseqid-boolean","title":"AddRange(ReadOnlySpan&lt;LLamaToken&gt;, LLamaPos, LLamaSeqId, Boolean)","text":"<p>Add a range of tokens to a single sequence, start at the given position.</p> <pre><code>public int AddRange(ReadOnlySpan&lt;LLamaToken&gt; tokens, LLamaPos start, LLamaSeqId sequence, bool logitsLast)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatch/#parameters_3","title":"Parameters","text":"<p><code>tokens</code> ReadOnlySpan&lt;LLamaToken&gt; The tokens to add</p> <p><code>start</code> LLamaPos The starting position to add tokens at</p> <p><code>sequence</code> LLamaSeqId The sequence to add this token to</p> <p><code>logitsLast</code> Boolean Whether the final token should generate logits</p>"},{"location":"xmldocs/llama.native.llamabatch/#returns_3","title":"Returns","text":"<p>Int32 The index that the final token was added at. Use this for GetLogitsIth</p>"},{"location":"xmldocs/llama.native.llamabatch/#clear","title":"Clear()","text":"<p>Set TokenCount to zero for this batch</p> <pre><code>public void Clear()\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/","title":"Llama.native.llamabatchembeddings","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#llamabatchembeddings","title":"LLamaBatchEmbeddings","text":"<p>Namespace: LLama.Native</p> <p>An embeddings batch allows submitting embeddings to multiple sequences simultaneously</p> <pre><code>public class LLamaBatchEmbeddings\n</code></pre> <p>Inheritance Object \u2192 LLamaBatchEmbeddings Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamabatchembeddings/#embeddingdimensions","title":"EmbeddingDimensions","text":"<p>Size of an individual embedding</p> <pre><code>public int EmbeddingDimensions { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#embeddingscount","title":"EmbeddingsCount","text":"<p>The number of items in this batch</p> <pre><code>public int EmbeddingsCount { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#sequencecapacity","title":"SequenceCapacity","text":"<p>Maximum number of sequences an item can be assigned to (automatically grows if exceeded)</p> <pre><code>public int SequenceCapacity { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.llamabatchembeddings/#llamabatchembeddingsint32","title":"LLamaBatchEmbeddings(Int32)","text":"<p>Create a new batch for submitting inputs to llama.cpp</p> <pre><code>public LLamaBatchEmbeddings(int embeddingDimensions)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#parameters","title":"Parameters","text":"<p><code>embeddingDimensions</code> Int32</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamabatchembeddings/#addreadonlyspansingle-llamapos-readonlyspanllamaseqid-boolean","title":"Add(ReadOnlySpan&lt;Single&gt;, LLamaPos, ReadOnlySpan&lt;LLamaSeqId&gt;, Boolean)","text":"<p>Add a single embedding to the batch at the same position in several sequences</p> <pre><code>public int Add(ReadOnlySpan&lt;float&gt; embedding, LLamaPos pos, ReadOnlySpan&lt;LLamaSeqId&gt; sequences, bool logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#parameters_1","title":"Parameters","text":"<p><code>embedding</code> ReadOnlySpan&lt;Single&gt; The embedding to add</p> <p><code>pos</code> LLamaPos The position to add it att</p> <p><code>sequences</code> ReadOnlySpan&lt;LLamaSeqId&gt; The set of sequences to add this token to</p> <p><code>logits</code> Boolean</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#returns","title":"Returns","text":"<p>Int32 The index that the token was added at. Use this for GetLogitsIth</p> <p>Remarks:</p> <p>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#addreadonlyspansingle-llamapos-llamaseqid-boolean","title":"Add(ReadOnlySpan&lt;Single&gt;, LLamaPos, LLamaSeqId, Boolean)","text":"<p>Add a single embedding to the batch for a single sequence</p> <pre><code>public int Add(ReadOnlySpan&lt;float&gt; embedding, LLamaPos pos, LLamaSeqId sequence, bool logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#parameters_2","title":"Parameters","text":"<p><code>embedding</code> ReadOnlySpan&lt;Single&gt;</p> <p><code>pos</code> LLamaPos</p> <p><code>sequence</code> LLamaSeqId</p> <p><code>logits</code> Boolean</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#returns_1","title":"Returns","text":"<p>Int32 The index that the token was added at. Use this for GetLogitsIth</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#addtparamtparam-writeembeddingsdelegatetparam-llamapos-readonlyspanllamaseqid-boolean","title":"Add&lt;TParam&gt;(TParam, WriteEmbeddingsDelegate&lt;TParam&gt;, LLamaPos, ReadOnlySpan&lt;LLamaSeqId&gt;, Boolean)","text":"<p>Add a single embedding to the batch at the same position in several sequences</p> <pre><code>public int Add&lt;TParam&gt;(TParam parameter, WriteEmbeddingsDelegate&lt;TParam&gt; write, LLamaPos pos, ReadOnlySpan&lt;LLamaSeqId&gt; sequences, bool logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#type-parameters","title":"Type Parameters","text":"<p><code>TParam</code> Type of userdata passed to write delegate</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#parameters_3","title":"Parameters","text":"<p><code>parameter</code> TParam Userdata passed to write delegate</p> <p><code>write</code> WriteEmbeddingsDelegate&lt;TParam&gt; Delegate called once to write data into a span</p> <p><code>pos</code> LLamaPos Position to write this embedding to</p> <p><code>sequences</code> ReadOnlySpan&lt;LLamaSeqId&gt; All sequences to assign this embedding to</p> <p><code>logits</code> Boolean Whether logits should be generated for this embedding</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#returns_2","title":"Returns","text":"<p>Int32 The index that the token was added at. Use this for GetLogitsIth</p> <p>Remarks:</p> <p>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#addtparamtparam-writeembeddingsdelegatetparam-llamapos-llamaseqid-boolean","title":"Add&lt;TParam&gt;(TParam, WriteEmbeddingsDelegate&lt;TParam&gt;, LLamaPos, LLamaSeqId, Boolean)","text":"<p>Add a single embedding to the batch at a position for one sequence</p> <pre><code>public int Add&lt;TParam&gt;(TParam parameter, WriteEmbeddingsDelegate&lt;TParam&gt; write, LLamaPos pos, LLamaSeqId sequence, bool logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#type-parameters_1","title":"Type Parameters","text":"<p><code>TParam</code> Type of userdata passed to write delegate</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#parameters_4","title":"Parameters","text":"<p><code>parameter</code> TParam Userdata passed to write delegate</p> <p><code>write</code> WriteEmbeddingsDelegate&lt;TParam&gt; Delegate called once to write data into a span</p> <p><code>pos</code> LLamaPos Position to write this embedding to</p> <p><code>sequence</code> LLamaSeqId Sequence to assign this embedding to</p> <p><code>logits</code> Boolean Whether logits should be generated for this embedding</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#returns_3","title":"Returns","text":"<p>Int32 The index that the token was added at. Use this for GetLogitsIth</p> <p>Remarks:</p> <p>https://github.com/ggerganov/llama.cpp/blob/ad939626577cd25b462e8026cc543efb71528472/common/common.cpp#L829C2-L829C2</p>"},{"location":"xmldocs/llama.native.llamabatchembeddings/#clear","title":"Clear()","text":"<p>Set EmbeddingsCount to zero for this batch</p> <pre><code>public void Clear()\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamachatmessage/","title":"llama.native.llamachatmessage","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamachatmessage/#llamachatmessage","title":"LLamaChatMessage","text":"<p>Namespace: LLama.Native</p> <pre><code>public struct LLamaChatMessage\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaChatMessage</p> <p>Remarks:</p> <p>llama_chat_message</p>"},{"location":"xmldocs/llama.native.llamachatmessage/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamachatmessage/#role","title":"role","text":"<p>Pointer to the null terminated bytes that make up the role string</p> <pre><code>public Byte* role;\n</code></pre>"},{"location":"xmldocs/llama.native.llamachatmessage/#content","title":"content","text":"<p>Pointer to the null terminated bytes that make up the content string</p> <pre><code>public Byte* content;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamacontextparams/","title":"llama.native.llamacontextparams","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamacontextparams/#llamacontextparams","title":"LLamaContextParams","text":"<p>Namespace: LLama.Native</p> <p>A C# representation of the llama.cpp <code>llama_context_params</code> struct</p> <pre><code>public struct LLamaContextParams\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaContextParams</p> <p>Remarks:</p> <p>changing the default values of parameters marked as [EXPERIMENTAL] may cause crashes or incorrect results in certain configurations  https://github.com/ggerganov/llama.cpp/pull/7544</p>"},{"location":"xmldocs/llama.native.llamacontextparams/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamacontextparams/#n_ctx","title":"n_ctx","text":"<p>text context, 0 = from model</p> <pre><code>public uint n_ctx;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#n_batch","title":"n_batch","text":"<p>logical maximum batch size that can be submitted to llama_decode</p> <pre><code>public uint n_batch;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#n_ubatch","title":"n_ubatch","text":"<p>physical maximum batch size</p> <pre><code>public uint n_ubatch;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#n_seq_max","title":"n_seq_max","text":"<p>max number of sequences (i.e. distinct states for recurrent models)</p> <pre><code>public uint n_seq_max;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#n_threads","title":"n_threads","text":"<p>number of threads to use for generation</p> <pre><code>public int n_threads;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#n_threads_batch","title":"n_threads_batch","text":"<p>number of threads to use for batch processing</p> <pre><code>public int n_threads_batch;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#rope_scaling_type","title":"rope_scaling_type","text":"<p>RoPE scaling type, from <code>enum llama_rope_scaling_type</code></p> <pre><code>public RopeScalingType rope_scaling_type;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#llama_pooling_type","title":"llama_pooling_type","text":"<p>whether to pool (sum) embedding results by sequence id</p> <pre><code>public LLamaPoolingType llama_pooling_type;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#attention_type","title":"attention_type","text":"<p>Attention type to use for embeddings</p> <pre><code>public LLamaAttentionType attention_type;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#rope_freq_base","title":"rope_freq_base","text":"<p>RoPE base frequency, 0 = from model</p> <pre><code>public float rope_freq_base;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#rope_freq_scale","title":"rope_freq_scale","text":"<p>RoPE frequency scaling factor, 0 = from model</p> <pre><code>public float rope_freq_scale;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#yarn_ext_factor","title":"yarn_ext_factor","text":"<p>YaRN extrapolation mix factor, negative = from model</p> <pre><code>public float yarn_ext_factor;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#yarn_attn_factor","title":"yarn_attn_factor","text":"<p>YaRN magnitude scaling factor</p> <pre><code>public float yarn_attn_factor;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#yarn_beta_fast","title":"yarn_beta_fast","text":"<p>YaRN low correction dim</p> <pre><code>public float yarn_beta_fast;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#yarn_beta_slow","title":"yarn_beta_slow","text":"<p>YaRN high correction dim</p> <pre><code>public float yarn_beta_slow;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#yarn_orig_ctx","title":"yarn_orig_ctx","text":"<p>YaRN original context size</p> <pre><code>public uint yarn_orig_ctx;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#defrag_threshold","title":"defrag_threshold","text":"<p>defragment the KV cache if holes/size &gt; defrag_threshold, Set to &lt; 0 to disable (default)</p> <pre><code>public float defrag_threshold;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#cb_eval","title":"cb_eval","text":"<p>ggml_backend_sched_eval_callback</p> <pre><code>public IntPtr cb_eval;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#cb_eval_user_data","title":"cb_eval_user_data","text":"<p>User data passed into cb_eval</p> <pre><code>public IntPtr cb_eval_user_data;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#type_k","title":"type_k","text":"<p>data type for K cache. EXPERIMENTAL</p> <pre><code>public GGMLType type_k;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#type_v","title":"type_v","text":"<p>data type for V cache. EXPERIMENTAL</p> <pre><code>public GGMLType type_v;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#abort_callback","title":"abort_callback","text":"<p>ggml_abort_callback</p> <pre><code>public IntPtr abort_callback;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#abort_callback_user_data","title":"abort_callback_user_data","text":"<p>User data passed into abort_callback</p> <pre><code>public IntPtr abort_callback_user_data;\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamacontextparams/#embeddings","title":"embeddings","text":"<p>if true, extract embeddings (together with logits)</p> <pre><code>public bool embeddings { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamacontextparams/#offload_kqv","title":"offload_kqv","text":"<p>whether to offload the KQV ops (including the KV cache) to GPU</p> <pre><code>public bool offload_kqv { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamacontextparams/#flash_attention","title":"flash_attention","text":"<p>whether to use flash attention. EXPERIMENTAL</p> <pre><code>public bool flash_attention { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#property-value_2","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamacontextparams/#no_perf","title":"no_perf","text":"<p>whether to measure performance timings</p> <pre><code>public bool no_perf { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamacontextparams/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamacontextparams/#default","title":"Default()","text":"<p>Get the default LLamaContextParams</p> <pre><code>LLamaContextParams Default()\n</code></pre>"},{"location":"xmldocs/llama.native.llamacontextparams/#returns","title":"Returns","text":"<p>LLamaContextParams</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaftype/","title":"llama.native.llamaftype","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaftype/#llamaftype","title":"LLamaFtype","text":"<p>Namespace: LLama.Native</p> <p>Supported model file types</p> <pre><code>public enum LLamaFtype\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaFtype Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>C# representation of llama_ftype</p>"},{"location":"xmldocs/llama.native.llamaftype/#fields","title":"Fields","text":"Name Value Description ALL_F32 0 All f32 MOSTLY_F16 1 Mostly f16 MOSTLY_Q8_0 7 Mostly 8 bit MOSTLY_Q4_0 2 Mostly 4 bit MOSTLY_Q4_1 3 Mostly 4 bit MOSTLY_Q5_0 8 Mostly 5 bit MOSTLY_Q5_1 9 Mostly 5 bit MOSTLY_Q2_K 10 K-Quant 2 bit MOSTLY_Q3_K_S 11 K-Quant 3 bit (Small) MOSTLY_Q3_K_M 12 K-Quant 3 bit (Medium) MOSTLY_Q3_K_L 13 K-Quant 3 bit (Large) MOSTLY_Q4_K_S 14 K-Quant 4 bit (Small) MOSTLY_Q4_K_M 15 K-Quant 4 bit (Medium) MOSTLY_Q5_K_S 16 K-Quant 5 bit (Small) MOSTLY_Q5_K_M 17 K-Quant 5 bit (Medium) MOSTLY_Q6_K 18 K-Quant 6 bit MOSTLY_IQ2_XXS 19 except 1d tensors MOSTLY_IQ2_XS 20 except 1d tensors MOSTLY_Q2_K_S 21 except 1d tensors MOSTLY_IQ3_K_XS 22 except 1d tensors MOSTLY_IQ3_XXS 23 except 1d tensors MOSTLY_IQ1_S 24 except 1d tensors MOSTLY_IQ4_NL 25 except 1d tensors MOSTLY_IQ3_S 26 except 1d tensors MOSTLY_IQ3_M 27 except 1d tensors MOSTLY_IQ2_S 28 except 1d tensors MOSTLY_IQ2_M 29 except 1d tensors MOSTLY_IQ4_XS 30 except 1d tensors MOSTLY_IQ1_M 31 except 1d tensors MOSTLY_BF16 32 except 1d tensors LLAMA_FTYPE_MOSTLY_TQ1_0 36 except 1d tensors LLAMA_FTYPE_MOSTLY_TQ2_0 37 except 1d tensors GUESSED 1024 File type was not specified <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/","title":"llama.native.llamakvcacheviewsafehandle","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#llamakvcacheviewsafehandle","title":"LLamaKvCacheViewSafeHandle","text":"<p>Namespace: LLama.Native</p> <p>A safe handle for a LLamaKvCacheView</p> <pre><code>public sealed class LLamaKvCacheViewSafeHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 LLamaKvCacheViewSafeHandle Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#handle","title":"handle","text":"<pre><code>protected IntPtr handle;\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#cellcount","title":"CellCount","text":"<p>Number of KV cache cells. This will be the same as the context size.</p> <pre><code>public int CellCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#tokencount","title":"TokenCount","text":"<p>Get the total number of tokens in the KV cache.</p> <p>For example, if there are two populated  cells, the first with 1 sequence id in it and the second with 2 sequence  ids then you'll have 3 tokens.</p> <pre><code>public int TokenCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#maxsequencecount","title":"MaxSequenceCount","text":"<p>Maximum number of sequences visible for a cell. There may be more sequences than this  in reality, this is simply the maximum number this view can see.</p> <pre><code>public int MaxSequenceCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#usedcellcount","title":"UsedCellCount","text":"<p>Number of populated cache cells</p> <pre><code>public int UsedCellCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#property-value_3","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#maxcontiguous","title":"MaxContiguous","text":"<p>Maximum contiguous empty slots in the cache.</p> <pre><code>public int MaxContiguous { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#property-value_4","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#maxcontiguousidx","title":"MaxContiguousIdx","text":"<p>Index to the start of the MaxContiguous slot range. Can be negative when cache is full.</p> <pre><code>public int MaxContiguousIdx { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#property-value_5","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#property-value_6","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#property-value_7","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#allocatesafellamacontexthandle-int32","title":"Allocate(SafeLLamaContextHandle, Int32)","text":"<p>Allocate a new KV cache view which can be used to inspect the KV cache</p> <pre><code>public static LLamaKvCacheViewSafeHandle Allocate(SafeLLamaContextHandle ctx, int maxSequences)\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#parameters","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>maxSequences</code> Int32 The maximum number of sequences visible in this view per cell</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#returns","title":"Returns","text":"<p>LLamaKvCacheViewSafeHandle</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#update","title":"Update()","text":"<p>Read the current KV cache state into this view.</p> <pre><code>public void Update()\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#getcellint32","title":"GetCell(Int32)","text":"<p>Get the cell at the given index</p> <pre><code>public LLamaPos GetCell(int index)\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#parameters_1","title":"Parameters","text":"<p><code>index</code> Int32 The index of the cell [0, CellCount)</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#returns_2","title":"Returns","text":"<p>LLamaPos Data about the cell at the given index</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#exceptions","title":"Exceptions","text":"<p>ArgumentOutOfRangeException Thrown if index is out of range (0 &lt;= index &lt; CellCount)</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#getcellsequencesint32","title":"GetCellSequences(Int32)","text":"<p>Get all of the sequences assigned to the cell at the given index. This will contain LLamaKvCacheViewSafeHandle.MaxSequenceCount entries  sequences even if the cell actually has more than that many sequences, allocate a new view with a larger maxSequences parameter  if necessary. Invalid sequences will be negative values.</p> <pre><code>public Span&lt;LLamaSeqId&gt; GetCellSequences(int index)\n</code></pre>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#parameters_2","title":"Parameters","text":"<p><code>index</code> Int32 The index of the cell [0, CellCount)</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#returns_3","title":"Returns","text":"<p>Span&lt;LLamaSeqId&gt; A span containing the sequences assigned to this cell</p>"},{"location":"xmldocs/llama.native.llamakvcacheviewsafehandle/#exceptions_1","title":"Exceptions","text":"<p>ArgumentOutOfRangeException Thrown if index is out of range (0 &lt;= index &lt; CellCount)</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamalogitbias/","title":"Llama.native.llamalogitbias","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamalogitbias/#llamalogitbias","title":"LLamaLogitBias","text":"<p>Namespace: LLama.Native</p> <p>A bias to apply directly to a logit</p> <pre><code>public struct LLamaLogitBias\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaLogitBias Implements IEquatable&lt;LLamaLogitBias&gt;</p>"},{"location":"xmldocs/llama.native.llamalogitbias/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamalogitbias/#token","title":"Token","text":"<p>The token to apply the bias to</p> <pre><code>public LLamaToken Token;\n</code></pre>"},{"location":"xmldocs/llama.native.llamalogitbias/#bias","title":"Bias","text":"<p>The bias to add</p> <pre><code>public float Bias;\n</code></pre>"},{"location":"xmldocs/llama.native.llamalogitbias/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamalogitbias/#tostring","title":"ToString()","text":"<pre><code>string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.llamalogitbias/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.llamalogitbias/#gethashcode","title":"GetHashCode()","text":"<pre><code>int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.native.llamalogitbias/#returns_1","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamalogitbias/#equalsobject","title":"Equals(Object)","text":"<pre><code>bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.native.llamalogitbias/#parameters","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.native.llamalogitbias/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamalogitbias/#equalsllamalogitbias","title":"Equals(LLamaLogitBias)","text":"<pre><code>bool Equals(LLamaLogitBias other)\n</code></pre>"},{"location":"xmldocs/llama.native.llamalogitbias/#parameters_1","title":"Parameters","text":"<p><code>other</code> LLamaLogitBias</p>"},{"location":"xmldocs/llama.native.llamalogitbias/#returns_3","title":"Returns","text":"<p>Boolean</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaloglevel/","title":"llama.native.llamaloglevel","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaloglevel/#llamaloglevel","title":"LLamaLogLevel","text":"<p>Namespace: LLama.Native</p> <p>Severity level of a log message. This enum should always be aligned with  the one defined on llama.cpp side at  https://github.com/ggerganov/llama.cpp/blob/0eb4e12beebabae46d37b78742f4c5d4dbe52dc1/ggml/include/ggml.h#L559</p> <pre><code>public enum LLamaLogLevel\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaLogLevel Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.llamaloglevel/#fields","title":"Fields","text":"Name Value Description None 0 Logs are never written. Debug 1 Logs that are used for interactive investigation during development. Info 2 Logs that track the general flow of the application. Warning 3 Logs that highlight an abnormal or unexpected event in the application flow, but do not otherwise cause the application execution to stop. Error 4 Logs that highlight when the current flow of execution is stopped due to a failure. Continue 5 Continue log level is equivalent to None in the way it is used in llama.cpp. <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamamodelkvoverridetype/","title":"llama.native.llamamodelkvoverridetype","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamamodelkvoverridetype/#llamamodelkvoverridetype","title":"LLamaModelKvOverrideType","text":"<p>Namespace: LLama.Native</p> <p>Specifies what type of value is being overridden by LLamaModelKvOverride</p> <pre><code>public enum LLamaModelKvOverrideType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaModelKvOverrideType Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>llama_model_kv_override_type</p>"},{"location":"xmldocs/llama.native.llamamodelkvoverridetype/#fields","title":"Fields","text":"Name Value Description Int 0 Overriding an int value Float 1 Overriding a float value Bool 2 Overriding a bool value String 3 Overriding a string value <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/","title":"llama.native.llamamodelmetadataoverride","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#llamamodelmetadataoverride","title":"LLamaModelMetadataOverride","text":"<p>Namespace: LLama.Native</p> <p>Override a key/value pair in the llama model metadata (llama_model_kv_override)</p> <pre><code>public struct LLamaModelMetadataOverride\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaModelMetadataOverride</p>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#key","title":"key","text":"<p>Key to override</p> <pre><code>public &lt;key&gt;e__FixedBuffer key;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#tag","title":"Tag","text":"<p>Type of value</p> <pre><code>public LLamaModelKvOverrideType Tag;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#intvalue","title":"IntValue","text":"<p>Value, must only be used if Tag == LLAMA_KV_OVERRIDE_INT</p> <pre><code>public long IntValue;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#floatvalue","title":"FloatValue","text":"<p>Value, must only be used if Tag == LLAMA_KV_OVERRIDE_FLOAT</p> <pre><code>public double FloatValue;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#boolvalue","title":"BoolValue","text":"<p>Value, must only be used if Tag == LLAMA_KV_OVERRIDE_BOOL</p> <pre><code>public long BoolValue;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelmetadataoverride/#stringvalue","title":"StringValue","text":"<p>Value, must only be used if Tag == String</p> <pre><code>public &lt;StringValue&gt;e__FixedBuffer StringValue;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamamodelparams/","title":"llama.native.llamamodelparams","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamamodelparams/#llamamodelparams","title":"LLamaModelParams","text":"<p>Namespace: LLama.Native</p> <p>A C# representation of the llama.cpp <code>llama_model_params</code> struct</p> <pre><code>public struct LLamaModelParams\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaModelParams</p>"},{"location":"xmldocs/llama.native.llamamodelparams/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamamodelparams/#tensor_buft_overrides","title":"tensor_buft_overrides","text":"<p>NULL-terminated list of buffer types to use for tensors that match a pattern</p> <pre><code>public LLamaModelTensorBufferOverride* tensor_buft_overrides;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#n_gpu_layers","title":"n_gpu_layers","text":"<p>// number of layers to store in VRAM</p> <pre><code>public int n_gpu_layers;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#split_mode","title":"split_mode","text":"<p>how to split the model across multiple GPUs</p> <pre><code>public GPUSplitMode split_mode;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#main_gpu","title":"main_gpu","text":"<p>the GPU that is used for the entire model when split_mode is LLAMA_SPLIT_MODE_NONE</p> <pre><code>public int main_gpu;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#tensor_split","title":"tensor_split","text":"<p>how to split layers across multiple GPUs (size: NativeApi.llama_max_devices())</p> <pre><code>public Single* tensor_split;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#progress_callback","title":"progress_callback","text":"<p>called with a progress value between 0 and 1, pass NULL to disable. If the provided progress_callback  returns true, model loading continues. If it returns false, model loading is immediately aborted.</p> <pre><code>public LlamaProgressCallback progress_callback;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#progress_callback_user_data","title":"progress_callback_user_data","text":"<p>context pointer passed to the progress callback</p> <pre><code>public Void* progress_callback_user_data;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#kv_overrides","title":"kv_overrides","text":"<p>override key-value pairs of the model meta data</p> <pre><code>public LLamaModelMetadataOverride* kv_overrides;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamamodelparams/#vocab_only","title":"vocab_only","text":"<p>only load the vocabulary, no weights</p> <pre><code>public bool vocab_only { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelparams/#use_mmap","title":"use_mmap","text":"<p>use mmap if possible</p> <pre><code>public bool use_mmap { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelparams/#use_mlock","title":"use_mlock","text":"<p>force system to keep model in RAM</p> <pre><code>public bool use_mlock { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#property-value_2","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelparams/#check_tensors","title":"check_tensors","text":"<p>validate model tensor data</p> <pre><code>public bool check_tensors { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelparams/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamamodelparams/#default","title":"Default()","text":"<p>Create a LLamaModelParams with default values</p> <pre><code>LLamaModelParams Default()\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelparams/#returns","title":"Returns","text":"<p>LLamaModelParams</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/","title":"llama.native.llamamodelquantizeparams","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#llamamodelquantizeparams","title":"LLamaModelQuantizeParams","text":"<p>Namespace: LLama.Native</p> <p>Quantizer parameters used in the native API</p> <pre><code>public struct LLamaModelQuantizeParams\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaModelQuantizeParams</p> <p>Remarks:</p> <p>llama_model_quantize_params</p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#nthread","title":"nthread","text":"<p>number of threads to use for quantizing, if &lt;=0 will use std::thread::hardware_concurrency()</p> <pre><code>public int nthread;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#ftype","title":"ftype","text":"<p>quantize to this llama_ftype</p> <pre><code>public LLamaFtype ftype;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#output_tensor_type","title":"output_tensor_type","text":"<p>output tensor type</p> <pre><code>public GGMLType output_tensor_type;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#token_embedding_type","title":"token_embedding_type","text":"<p>token embeddings tensor type</p> <pre><code>public GGMLType token_embedding_type;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#imatrix","title":"imatrix","text":"<p>pointer to importance matrix data</p> <pre><code>public IntPtr imatrix;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#kv_overrides","title":"kv_overrides","text":"<p>pointer to vector containing overrides</p> <pre><code>public IntPtr kv_overrides;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#tensor_types","title":"tensor_types","text":"<p>pointer to vector containing tensor types</p> <pre><code>public IntPtr tensor_types;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#allow_requantize","title":"allow_requantize","text":"<p>allow quantizing non-f32/f16 tensors</p> <pre><code>public bool allow_requantize { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#quantize_output_tensor","title":"quantize_output_tensor","text":"<p>quantize output.weight</p> <pre><code>public bool quantize_output_tensor { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#only_copy","title":"only_copy","text":"<p>only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored</p> <pre><code>public bool only_copy { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#property-value_2","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#pure","title":"pure","text":"<p>quantize all tensors to the default type</p> <pre><code>public bool pure { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#keep_split","title":"keep_split","text":"<p>quantize to the same number of shards</p> <pre><code>public bool keep_split { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#property-value_4","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#default","title":"Default()","text":"<p>Create a LLamaModelQuantizeParams with default values</p> <pre><code>LLamaModelQuantizeParams Default()\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodelquantizeparams/#returns","title":"Returns","text":"<p>LLamaModelQuantizeParams</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamamodeltensorbufferoverride/","title":"Llama.native.llamamodeltensorbufferoverride","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamamodeltensorbufferoverride/#llamamodeltensorbufferoverride","title":"LLamaModelTensorBufferOverride","text":"<p>Namespace: LLama.Native</p> <p>Represents a mapping between a tensor name pattern and a backend buffer type  Original type: llama_model_tensor_buft_override</p> <pre><code>public struct LLamaModelTensorBufferOverride\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaModelTensorBufferOverride</p>"},{"location":"xmldocs/llama.native.llamamodeltensorbufferoverride/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamamodeltensorbufferoverride/#pattern","title":"Pattern","text":"<p>Tensor name pattern to match</p> <pre><code>public Byte* Pattern;\n</code></pre>"},{"location":"xmldocs/llama.native.llamamodeltensorbufferoverride/#buffertype","title":"BufferType","text":"<p>Backend buffer type to use for matching tensors, as obtained via ggml_backend_dev_buffer_type</p> <pre><code>public IntPtr BufferType;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamanativebatch/","title":"llama.native.llamanativebatch","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamanativebatch/#llamanativebatch","title":"LLamaNativeBatch","text":"<p>Namespace: LLama.Native</p> <p>Input data for llama_decode  A llama_batch object can contain input about one or many sequences  The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens</p> <pre><code>public struct LLamaNativeBatch\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaNativeBatch</p>"},{"location":"xmldocs/llama.native.llamanativebatch/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamanativebatch/#n_tokens","title":"n_tokens","text":"<p>The number of items pointed at by pos, seq_id and logits.</p> <pre><code>public int n_tokens;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#tokens","title":"tokens","text":"<p>Either <code>n_tokens</code> of <code>llama_token</code>, or <code>NULL</code>, depending on how this batch was created</p> <pre><code>public LLamaToken* tokens;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#embd","title":"embd","text":"<p>Either <code>n_tokens * embd * sizeof(float)</code> or <code>NULL</code>, depending on how this batch was created</p> <pre><code>public Single* embd;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#pos","title":"pos","text":"<p>the positions of the respective token in the sequence  (if set to NULL, the token position will be tracked automatically by llama_decode)</p> <pre><code>public LLamaPos* pos;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#n_seq_id","title":"n_seq_id","text":"<p>https://github.com/ggerganov/llama.cpp/blob/master/llama.h#L139 ???</p> <pre><code>public Int32* n_seq_id;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#seq_id","title":"seq_id","text":"<p>the sequence to which the respective token belongs  (if set to NULL, the sequence ID will be assumed to be 0)</p> <pre><code>public LLamaSeqId** seq_id;\n</code></pre>"},{"location":"xmldocs/llama.native.llamanativebatch/#logits","title":"logits","text":"<p>if zero, the logits for the respective token will not be output  (if set to NULL, only the logits for last token will be returned)</p> <pre><code>public Byte* logits;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/","title":"Llama.native.llamaperfcontexttimings","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#llamaperfcontexttimings","title":"LLamaPerfContextTimings","text":"<p>Namespace: LLama.Native</p> <p>LLama performance information</p> <pre><code>public struct LLamaPerfContextTimings\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaPerfContextTimings</p> <p>Remarks:</p> <p>llama_perf_context_data</p>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#resettimestamp","title":"ResetTimestamp","text":"<p>Timestamp when reset was last called</p> <pre><code>public TimeSpan ResetTimestamp { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#property-value","title":"Property Value","text":"<p>TimeSpan</p>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#loading","title":"Loading","text":"<p>Time spent loading</p> <pre><code>public TimeSpan Loading { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#property-value_1","title":"Property Value","text":"<p>TimeSpan</p>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#prompteval","title":"PromptEval","text":"<p>total milliseconds spent prompt processing</p> <pre><code>public TimeSpan PromptEval { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#property-value_2","title":"Property Value","text":"<p>TimeSpan</p>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#eval","title":"Eval","text":"<p>Total milliseconds in eval/decode calls</p> <pre><code>public TimeSpan Eval { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#property-value_3","title":"Property Value","text":"<p>TimeSpan</p>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#promptokensevaluated","title":"PrompTokensEvaluated","text":"<p>number of tokens in eval calls for the prompt (with batch size &gt; 1)</p> <pre><code>public int PrompTokensEvaluated { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#property-value_4","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#tokensevaluated","title":"TokensEvaluated","text":"<p>number of eval calls</p> <pre><code>public int TokensEvaluated { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamaperfcontexttimings/#property-value_5","title":"Property Value","text":"<p>Int32</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamapoolingtype/","title":"llama.native.llamapoolingtype","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamapoolingtype/#llamapoolingtype","title":"LLamaPoolingType","text":"<p>Namespace: LLama.Native</p> <pre><code>public enum LLamaPoolingType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaPoolingType Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>llama_pooling_type</p>"},{"location":"xmldocs/llama.native.llamapoolingtype/#fields","title":"Fields","text":"Name Value Description Unspecified -1 No specific pooling type. Use the model default if this is specific in IContextParams.PoolingType None 0 Do not pool embeddings (per-token embeddings) Mean 1 Take the mean of every token embedding CLS 2 Return the embedding for the special \"CLS\" token Last 3 Return the embeddings of the last token Rank 4 Used by reranking models to attach the classification head to the graph <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamapos/","title":"llama.native.llamapos","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamapos/#llamapos","title":"LLamaPos","text":"<p>Namespace: LLama.Native</p> <p>Indicates position in a sequence</p> <pre><code>public struct LLamaPos\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaPos Implements IEquatable&lt;LLamaPos&gt;</p>"},{"location":"xmldocs/llama.native.llamapos/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamapos/#value","title":"Value","text":"<p>The raw value</p> <pre><code>public int Value;\n</code></pre>"},{"location":"xmldocs/llama.native.llamapos/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamapos/#tostring","title":"ToString()","text":"<pre><code>string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.llamapos/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.llamapos/#gethashcode","title":"GetHashCode()","text":"<pre><code>int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.native.llamapos/#returns_1","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamapos/#equalsobject","title":"Equals(Object)","text":"<pre><code>bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.native.llamapos/#parameters","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.native.llamapos/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamapos/#equalsllamapos","title":"Equals(LLamaPos)","text":"<pre><code>bool Equals(LLamaPos other)\n</code></pre>"},{"location":"xmldocs/llama.native.llamapos/#parameters_1","title":"Parameters","text":"<p><code>other</code> LLamaPos</p>"},{"location":"xmldocs/llama.native.llamapos/#returns_3","title":"Returns","text":"<p>Boolean</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaropetype/","title":"llama.native.llamaropetype","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaropetype/#llamaropetype","title":"LLamaRopeType","text":"<p>Namespace: LLama.Native</p> <pre><code>public enum LLamaRopeType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaRopeType Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>llama_rope_type</p>"},{"location":"xmldocs/llama.native.llamaropetype/#fields","title":"Fields","text":"Name Value Description None -1 Norm 0 <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamasamplerchainparams/","title":"Llama.native.llamasamplerchainparams","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamasamplerchainparams/#llamasamplerchainparams","title":"LLamaSamplerChainParams","text":"<p>Namespace: LLama.Native</p> <pre><code>public struct LLamaSamplerChainParams\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaSamplerChainParams</p> <p>Remarks:</p> <p>llama_sampler_chain_params</p>"},{"location":"xmldocs/llama.native.llamasamplerchainparams/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamasamplerchainparams/#noperf","title":"NoPerf","text":"<p>whether to measure performance timings</p> <pre><code>public bool NoPerf { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamasamplerchainparams/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamasamplerchainparams/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamasamplerchainparams/#default","title":"Default()","text":"<p>Get the default LLamaSamplerChainParams</p> <pre><code>LLamaSamplerChainParams Default()\n</code></pre>"},{"location":"xmldocs/llama.native.llamasamplerchainparams/#returns","title":"Returns","text":"<p>LLamaSamplerChainParams</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamasamplingtimings/","title":"Llama.native.llamasamplingtimings","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamasamplingtimings/#llamasamplingtimings","title":"LLamaSamplingTimings","text":"<p>Namespace: LLama.Native</p> <p>LLama performance information</p> <pre><code>public struct LLamaSamplingTimings\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaSamplingTimings</p> <p>Remarks:</p> <p>llama_perf_sampler_data</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaseqid/","title":"llama.native.llamaseqid","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamaseqid/#llamaseqid","title":"LLamaSeqId","text":"<p>Namespace: LLama.Native</p> <p>ID for a sequence in a batch</p> <pre><code>public struct LLamaSeqId\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaSeqId Implements IEquatable&lt;LLamaSeqId&gt;</p>"},{"location":"xmldocs/llama.native.llamaseqid/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamaseqid/#value","title":"Value","text":"<p>The raw value</p> <pre><code>public int Value;\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#zero","title":"Zero","text":"<p>LLamaSeqId with value 0</p> <pre><code>public static LLamaSeqId Zero;\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamaseqid/#tostring","title":"ToString()","text":"<pre><code>string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.llamaseqid/#gethashcode","title":"GetHashCode()","text":"<pre><code>int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#returns_1","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamaseqid/#equalsobject","title":"Equals(Object)","text":"<pre><code>bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#parameters","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.native.llamaseqid/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamaseqid/#equalsllamaseqid","title":"Equals(LLamaSeqId)","text":"<pre><code>bool Equals(LLamaSeqId other)\n</code></pre>"},{"location":"xmldocs/llama.native.llamaseqid/#parameters_1","title":"Parameters","text":"<p><code>other</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.native.llamaseqid/#returns_3","title":"Returns","text":"<p>Boolean</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamatoken/","title":"llama.native.llamatoken","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamatoken/#llamatoken","title":"LLamaToken","text":"<p>Namespace: LLama.Native</p> <p>A single token</p> <pre><code>public struct LLamaToken\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaToken Implements IEquatable&lt;LLamaToken&gt; Attributes NullableContextAttribute, NullableAttribute, IsReadOnlyAttribute, DebuggerDisplayAttribute</p>"},{"location":"xmldocs/llama.native.llamatoken/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamatoken/#invalidtoken","title":"InvalidToken","text":"<p>Token Value used when token is inherently null</p> <pre><code>public static LLamaToken InvalidToken;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamatoken/#getattributessafellamamodelhandle","title":"GetAttributes(SafeLlamaModelHandle)","text":"<p>Get attributes for this token</p> <pre><code>LLamaTokenAttr GetAttributes(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#parameters","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.llamatoken/#returns","title":"Returns","text":"<p>LLamaTokenAttr</p>"},{"location":"xmldocs/llama.native.llamatoken/#getattributesvocabulary","title":"GetAttributes(Vocabulary)","text":"<p>Get attributes for this token</p> <pre><code>LLamaTokenAttr GetAttributes(Vocabulary vocab)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#parameters_1","title":"Parameters","text":"<p><code>vocab</code> Vocabulary</p>"},{"location":"xmldocs/llama.native.llamatoken/#returns_1","title":"Returns","text":"<p>LLamaTokenAttr</p>"},{"location":"xmldocs/llama.native.llamatoken/#getscorevocabulary","title":"GetScore(Vocabulary)","text":"<p>Get score for this token</p> <pre><code>float GetScore(Vocabulary vocab)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#parameters_2","title":"Parameters","text":"<p><code>vocab</code> Vocabulary</p>"},{"location":"xmldocs/llama.native.llamatoken/#returns_2","title":"Returns","text":"<p>Single</p>"},{"location":"xmldocs/llama.native.llamatoken/#iscontrolsafellamamodelhandle","title":"IsControl(SafeLlamaModelHandle)","text":"<p>Check if this is a control token</p> <pre><code>bool IsControl(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#parameters_3","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.llamatoken/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamatoken/#iscontrolvocabulary","title":"IsControl(Vocabulary)","text":"<p>Check if this is a control token</p> <pre><code>bool IsControl(Vocabulary vocab)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#parameters_4","title":"Parameters","text":"<p><code>vocab</code> Vocabulary</p>"},{"location":"xmldocs/llama.native.llamatoken/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamatoken/#isendofgenerationsafellamamodelhandle","title":"IsEndOfGeneration(SafeLlamaModelHandle)","text":"<p>Check if this token should end generation</p> <pre><code>bool IsEndOfGeneration(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#parameters_5","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.llamatoken/#returns_5","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamatoken/#isendofgenerationvocabulary","title":"IsEndOfGeneration(Vocabulary)","text":"<p>Check if this token should end generation</p> <pre><code>bool IsEndOfGeneration(Vocabulary vocab)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#parameters_6","title":"Parameters","text":"<p><code>vocab</code> Vocabulary</p>"},{"location":"xmldocs/llama.native.llamatoken/#returns_6","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamatoken/#tostring","title":"ToString()","text":"<pre><code>string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#returns_7","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.llamatoken/#gethashcode","title":"GetHashCode()","text":"<pre><code>int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#returns_8","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.llamatoken/#equalsobject","title":"Equals(Object)","text":"<pre><code>bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#parameters_7","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.native.llamatoken/#returns_9","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamatoken/#equalsllamatoken","title":"Equals(LLamaToken)","text":"<pre><code>bool Equals(LLamaToken other)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatoken/#parameters_8","title":"Parameters","text":"<p><code>other</code> LLamaToken</p>"},{"location":"xmldocs/llama.native.llamatoken/#returns_10","title":"Returns","text":"<p>Boolean</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamatokenattr/","title":"Llama.native.llamatokenattr","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamatokenattr/#llamatokenattr","title":"LLamaTokenAttr","text":"<p>Namespace: LLama.Native</p> <p>Token attributes</p> <pre><code>public enum LLamaTokenAttr\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaTokenAttr Implements IComparable, ISpanFormattable, IFormattable, IConvertible Attributes FlagsAttribute</p> <p>Remarks:</p> <p>C# equivalent of llama_token_attr</p>"},{"location":"xmldocs/llama.native.llamatokenattr/#fields","title":"Fields","text":"Name Value Description Undefined 0 Unknown 1 Unused 2 Normal 4 Control 8 UserDefined 16 Byte 32 Normalized 64 LStrip 128 RStrip 256 SingleWord 512 <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamatokendata/","title":"llama.native.llamatokendata","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamatokendata/#llamatokendata","title":"LLamaTokenData","text":"<p>Namespace: LLama.Native</p> <p>A single token along with probability of this token being selected</p> <pre><code>public struct LLamaTokenData\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaTokenData</p>"},{"location":"xmldocs/llama.native.llamatokendata/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamatokendata/#id","title":"ID","text":"<p>token id</p> <pre><code>public LLamaToken ID;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendata/#logit","title":"Logit","text":"<p>log-odds of the token</p> <pre><code>public float Logit;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendata/#probability","title":"Probability","text":"<p>probability of the token</p> <pre><code>public float Probability;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendata/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.llamatokendata/#llamatokendatallamatoken-single-single","title":"LLamaTokenData(LLamaToken, Single, Single)","text":"<p>Create a new LLamaTokenData</p> <pre><code>LLamaTokenData(LLamaToken id, float logit, float probability)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendata/#parameters","title":"Parameters","text":"<p><code>id</code> LLamaToken</p> <p><code>logit</code> Single</p> <p><code>probability</code> Single</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamatokendataarray/","title":"llama.native.llamatokendataarray","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#llamatokendataarray","title":"LLamaTokenDataArray","text":"<p>Namespace: LLama.Native</p> <p>Contains an array of LLamaTokenData, potentially sorted.</p> <pre><code>public struct LLamaTokenDataArray\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaTokenDataArray</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llamatokendataarray/#data","title":"Data","text":"<p>The LLamaTokenData</p> <pre><code>public Memory&lt;LLamaTokenData&gt; Data;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#sorted","title":"Sorted","text":"<p>Indicates if <code>data</code> is sorted by logits in descending order. If this is false the token data is in no particular order.</p> <pre><code>public bool Sorted;\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.llamatokendataarray/#llamatokendataarraymemoryllamatokendata-boolean","title":"LLamaTokenDataArray(Memory&lt;LLamaTokenData&gt;, Boolean)","text":"<p>Create a new LLamaTokenDataArray</p> <pre><code>LLamaTokenDataArray(Memory&lt;LLamaTokenData&gt; tokens, bool isSorted)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters","title":"Parameters","text":"<p><code>tokens</code> Memory&lt;LLamaTokenData&gt;</p> <p><code>isSorted</code> Boolean</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamatokendataarray/#createreadonlyspansingle","title":"Create(ReadOnlySpan&lt;Single&gt;)","text":"<p>Create a new LLamaTokenDataArray, copying the data from the given logits</p> <pre><code>LLamaTokenDataArray Create(ReadOnlySpan&lt;float&gt; logits)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_1","title":"Parameters","text":"<p><code>logits</code> ReadOnlySpan&lt;Single&gt;</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#returns","title":"Returns","text":"<p>LLamaTokenDataArray</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#createreadonlyspansingle-memoryllamatokendata","title":"Create(ReadOnlySpan&lt;Single&gt;, Memory&lt;LLamaTokenData&gt;)","text":"<p>Create a new LLamaTokenDataArray, copying the data from the given logits into temporary memory.</p> <pre><code>LLamaTokenDataArray Create(ReadOnlySpan&lt;float&gt; logits, Memory&lt;LLamaTokenData&gt; buffer)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_2","title":"Parameters","text":"<p><code>logits</code> ReadOnlySpan&lt;Single&gt;</p> <p><code>buffer</code> Memory&lt;LLamaTokenData&gt; Temporary memory which will be used to work on these logits. Must be at least as large as logits array</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#returns_1","title":"Returns","text":"<p>LLamaTokenDataArray</p> <p>Remarks:</p> <p>The memory must not be modified while this LLamaTokenDataArray is in use.</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#overwritelogitsreadonlyspanvaluetuplellamatoken-single","title":"OverwriteLogits(ReadOnlySpan&lt;ValueTuple&lt;LLamaToken, Single&gt;&gt;)","text":"<p>Overwrite the logit values for all given tokens</p> <pre><code>void OverwriteLogits(ReadOnlySpan&lt;ValueTuple&lt;LLamaToken, float&gt;&gt; values)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarray/#parameters_3","title":"Parameters","text":"<p><code>values</code> ReadOnlySpan&lt;ValueTuple&lt;LLamaToken, Single&gt;&gt; tuples of token and logit value to overwrite</p>"},{"location":"xmldocs/llama.native.llamatokendataarray/#softmax","title":"Softmax()","text":"<p>Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.</p> <pre><code>void Softmax()\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/","title":"llama.native.llamatokendataarraynative","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#llamatokendataarraynative","title":"LLamaTokenDataArrayNative","text":"<p>Namespace: LLama.Native</p> <p>Contains a pointer to an array of LLamaTokenData which is pinned in memory.</p> <pre><code>public struct LLamaTokenDataArrayNative\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLamaTokenDataArrayNative</p> <p>Remarks:</p> <p>C# equivalent of llama_token_data_array</p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.llamatokendataarraynative/#data","title":"Data","text":"<p>A pointer to an array of LlamaTokenData</p> <pre><code>public Span&lt;LLamaTokenData&gt; Data { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#property-value","title":"Property Value","text":"<p>Span&lt;LLamaTokenData&gt;</p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#sorted","title":"Sorted","text":"<p>Indicates if the items in the array are sorted, so the most likely token is first</p> <pre><code>public bool Sorted { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#selected","title":"Selected","text":"<p>The index of the selected token (i.e. not the token id)</p> <pre><code>public long Selected { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#property-value_2","title":"Property Value","text":"<p>Int64</p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#size","title":"Size","text":"<p>Number of LLamaTokenData in the array. Set this to shrink the array</p> <pre><code>public ulong Size { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#property-value_3","title":"Property Value","text":"<p>UInt64</p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.llamatokendataarraynative/#createllamatokendataarray-llamatokendataarraynative","title":"Create(LLamaTokenDataArray, LLamaTokenDataArrayNative&amp;)","text":"<p>Create a new LLamaTokenDataArrayNative around the data in the LLamaTokenDataArray</p> <pre><code>MemoryHandle Create(LLamaTokenDataArray array, LLamaTokenDataArrayNative&amp; native)\n</code></pre>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#parameters","title":"Parameters","text":"<p><code>array</code> LLamaTokenDataArray Data source</p> <p><code>native</code> LLamaTokenDataArrayNative&amp; Created native array</p>"},{"location":"xmldocs/llama.native.llamatokendataarraynative/#returns","title":"Returns","text":"<p>MemoryHandle A memory handle, pinning the data in place until disposed</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamavocabtype/","title":"llama.native.llamavocabtype","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llamavocabtype/#llamavocabtype","title":"LLamaVocabType","text":"<p>Namespace: LLama.Native</p> <pre><code>public enum LLamaVocabType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 LLamaVocabType Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>llama_vocab_type</p>"},{"location":"xmldocs/llama.native.llamavocabtype/#fields","title":"Fields","text":"Name Value Description None 0 For models without vocab SentencePiece 1 LLaMA tokenizer based on byte-level BPE with byte fallback BytePairEncoding 2 GPT-2 tokenizer based on byte-level BPE WordPiece 3 BERT tokenizer based on WordPiece Unigram 4 T5 tokenizer based on Unigram RWKV 5 RWKV tokenizer based on greedy tokenization <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llavaimageembed/","title":"llama.native.llavaimageembed","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.llavaimageembed/#llavaimageembed","title":"LLavaImageEmbed","text":"<p>Namespace: LLama.Native</p> <p>LLaVa Image embeddings</p> <pre><code>public struct LLavaImageEmbed\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 LLavaImageEmbed</p> <p>Remarks:</p> <p>llava_image_embed</p>"},{"location":"xmldocs/llama.native.llavaimageembed/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.llavaimageembed/#embed","title":"embed","text":"<p>The embeddings of the embedded image.</p> <pre><code>public Single* embed;\n</code></pre>"},{"location":"xmldocs/llama.native.llavaimageembed/#n_image_pos","title":"n_image_pos","text":"<p>The position of the image's tokens.</p> <pre><code>public int n_image_pos;\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.loraadapter/","title":"Llama.native.loraadapter","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.loraadapter/#loraadapter","title":"LoraAdapter","text":"<p>Namespace: LLama.Native</p> <p>A LoRA adapter which can be applied to a context for a specific model</p> <pre><code>public class LoraAdapter\n</code></pre> <p>Inheritance Object \u2192 LoraAdapter Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.loraadapter/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.loraadapter/#model","title":"Model","text":"<p>The model which this LoRA adapter was loaded with.</p> <pre><code>public SafeLlamaModelHandle Model { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.loraadapter/#property-value","title":"Property Value","text":"<p>SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.loraadapter/#path","title":"Path","text":"<p>The full path of the file this adapter was loaded from</p> <pre><code>public string Path { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.loraadapter/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.native.loraadapter/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.loraadapter/#unload","title":"Unload()","text":"<p>Unload this adapter</p> <pre><code>public void Unload()\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativeapi/","title":"llama.native.nativeapi","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativeapi/#nativeapi","title":"NativeApi","text":"<p>Namespace: LLama.Native</p> <p>Direct translation of the llama.cpp API</p> <pre><code>public static class NativeApi\n</code></pre> <p>Inheritance Object \u2192 NativeApi Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.nativeapi/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativeapi/#llama_empty_call","title":"llama_empty_call()","text":"<p>A method that does nothing. This is a native method, calling it will force the llama native dependencies to be loaded.</p> <pre><code>public static void llama_empty_call()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#llama_backend_free","title":"llama_backend_free()","text":"<p>Call once at the end of the program - currently only used for MPI</p> <pre><code>public static void llama_backend_free()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#llama_max_devices","title":"llama_max_devices()","text":"<p>Get the maximum number of devices supported by llama.cpp</p> <pre><code>public static long llama_max_devices()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns","title":"Returns","text":"<p>Int64</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_supports_mmap","title":"llama_supports_mmap()","text":"<p>Check if memory mapping is supported</p> <pre><code>public static bool llama_supports_mmap()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_supports_mlock","title":"llama_supports_mlock()","text":"<p>Check if memory locking is supported</p> <pre><code>public static bool llama_supports_mlock()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_supports_gpu_offload","title":"llama_supports_gpu_offload()","text":"<p>Check if GPU offload is supported</p> <pre><code>public static bool llama_supports_gpu_offload()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_supports_rpc","title":"llama_supports_rpc()","text":"<p>Check if RPC offload is supported</p> <pre><code>public static bool llama_supports_rpc()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_state_load_filesafellamacontexthandle-string-llamatoken-uint64-uint64","title":"llama_state_load_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64, UInt64&amp;)","text":"<p>Load session file</p> <pre><code>public static bool llama_state_load_file(SafeLLamaContextHandle ctx, string path_session, LLamaToken[] tokens_out, ulong n_token_capacity, UInt64&amp; n_token_count_out)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>path_session</code> String</p> <p><code>tokens_out</code> LLamaToken[]</p> <p><code>n_token_capacity</code> UInt64</p> <p><code>n_token_count_out</code> UInt64&amp;</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_5","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_state_save_filesafellamacontexthandle-string-llamatoken-uint64","title":"llama_state_save_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64)","text":"<p>Save session file</p> <pre><code>public static bool llama_state_save_file(SafeLLamaContextHandle ctx, string path_session, LLamaToken[] tokens, ulong n_token_count)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_1","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>path_session</code> String</p> <p><code>tokens</code> LLamaToken[]</p> <p><code>n_token_count</code> UInt64</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_6","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_state_seq_save_filesafellamacontexthandle-string-llamaseqid-llamatoken-uintptr","title":"llama_state_seq_save_file(SafeLLamaContextHandle, String, LLamaSeqId, LLamaToken*, UIntPtr)","text":"<p>Saves the specified sequence as a file on specified filepath. Can later be loaded via NativeApi.llama_state_load_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64, UInt64&amp;)</p> <pre><code>public static UIntPtr llama_state_seq_save_file(SafeLLamaContextHandle ctx, string filepath, LLamaSeqId seq_id, LLamaToken* tokens, UIntPtr n_token_count)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_2","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>filepath</code> String</p> <p><code>seq_id</code> LLamaSeqId</p> <p><code>tokens</code> LLamaToken*</p> <p><code>n_token_count</code> UIntPtr</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_7","title":"Returns","text":"<p>UIntPtr</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_state_seq_load_filesafellamacontexthandle-string-llamaseqid-llamatoken-uintptr-uintptr","title":"llama_state_seq_load_file(SafeLLamaContextHandle, String, LLamaSeqId, LLamaToken*, UIntPtr, UIntPtr&amp;)","text":"<p>Loads a sequence saved as a file via NativeApi.llama_state_save_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64) into the specified sequence</p> <pre><code>public static UIntPtr llama_state_seq_load_file(SafeLLamaContextHandle ctx, string filepath, LLamaSeqId dest_seq_id, LLamaToken* tokens_out, UIntPtr n_token_capacity, UIntPtr&amp; n_token_count_out)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_3","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>filepath</code> String</p> <p><code>dest_seq_id</code> LLamaSeqId</p> <p><code>tokens_out</code> LLamaToken*</p> <p><code>n_token_capacity</code> UIntPtr</p> <p><code>n_token_count_out</code> UIntPtr&amp;</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_8","title":"Returns","text":"<p>UIntPtr</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_set_causal_attnsafellamacontexthandle-boolean","title":"llama_set_causal_attn(SafeLLamaContextHandle, Boolean)","text":"<p>Set whether to use causal attention or not. If set to true, the model will only attend to the past tokens</p> <pre><code>public static void llama_set_causal_attn(SafeLLamaContextHandle ctx, bool causalAttn)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_4","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>causalAttn</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_set_embeddingssafellamacontexthandle-boolean","title":"llama_set_embeddings(SafeLLamaContextHandle, Boolean)","text":"<p>Set whether the model is in embeddings mode or not.</p> <pre><code>public static void llama_set_embeddings(SafeLLamaContextHandle ctx, bool embeddings)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_5","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>embeddings</code> Boolean If true, embeddings will be returned but logits will not</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_set_abort_callbacksafellamamodelhandle-intptr-intptr","title":"llama_set_abort_callback(SafeLlamaModelHandle, IntPtr, IntPtr)","text":"<p>Set abort callback</p> <pre><code>public static void llama_set_abort_callback(SafeLlamaModelHandle ctx, IntPtr abortCallback, IntPtr abortCallbackData)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_6","title":"Parameters","text":"<p><code>ctx</code> SafeLlamaModelHandle</p> <p><code>abortCallback</code> IntPtr</p> <p><code>abortCallbackData</code> IntPtr</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_n_seq_maxsafellamacontexthandle","title":"llama_n_seq_max(SafeLLamaContextHandle)","text":"<p>Get the n_seq_max for this context</p> <pre><code>public static uint llama_n_seq_max(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_7","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_9","title":"Returns","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_get_embeddingssafellamacontexthandle","title":"llama_get_embeddings(SafeLLamaContextHandle)","text":"<p>Get all output token embeddings.  When pooling_type == LLAMA_POOLING_TYPE_NONE or when using a generative model, the embeddings for which  llama_batch.logits[i] != 0 are stored contiguously in the order they have appeared in the batch.  shape: [n_outputs*n_embd]  Otherwise, returns an empty span.</p> <pre><code>public static Single* llama_get_embeddings(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_8","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_10","title":"Returns","text":"<p>Single*</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_chat_apply_templatebyte-llamachatmessage-uintptr-boolean-byte-int32","title":"llama_chat_apply_template(Byte, LLamaChatMessage, UIntPtr, Boolean, Byte*, Int32)","text":"<p>Apply chat template. Inspired by hf apply_chat_template() on python.</p> <pre><code>public static int llama_chat_apply_template(Byte* tmpl, LLamaChatMessage* chat, UIntPtr n_msg, bool add_ass, Byte* buf, int length)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_9","title":"Parameters","text":"<p><code>tmpl</code> Byte* A Jinja template to use for this chat. If this is nullptr, the model\u2019s default chat template will be used instead.</p> <p><code>chat</code> LLamaChatMessage* Pointer to a list of multiple llama_chat_message</p> <p><code>n_msg</code> UIntPtr Number of llama_chat_message in this chat</p> <p><code>add_ass</code> Boolean Whether to end the prompt with the token(s) that indicate the start of an assistant message.</p> <p><code>buf</code> Byte* A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)</p> <p><code>length</code> Int32 The size of the allocated buffer</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_11","title":"Returns","text":"<p>Int32 The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template.</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_chat_builtin_templateschar-uintptr","title":"llama_chat_builtin_templates(Char, UIntPtr)**","text":"<p>Get list of built-in chat templates</p> <pre><code>public static int llama_chat_builtin_templates(Char** output, UIntPtr len)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_10","title":"Parameters","text":"<p><code>output</code> Char**</p> <p><code>len</code> UIntPtr</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_12","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_print_timingssafellamacontexthandle","title":"llama_print_timings(SafeLLamaContextHandle)","text":"<p>Print out timing information for this context</p> <pre><code>public static void llama_print_timings(SafeLLamaContextHandle ctx)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_11","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_print_system_info","title":"llama_print_system_info()","text":"<p>Print system information</p> <pre><code>public static IntPtr llama_print_system_info()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_13","title":"Returns","text":"<p>IntPtr</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_token_to_piecevocabulary-llamatoken-spanbyte-int32-boolean","title":"llama_token_to_piece(Vocabulary, LLamaToken, Span&lt;Byte&gt;, Int32, Boolean)","text":"<p>Convert a single token into text</p> <pre><code>public static int llama_token_to_piece(Vocabulary vocab, LLamaToken llamaToken, Span&lt;byte&gt; buffer, int lstrip, bool special)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_12","title":"Parameters","text":"<p><code>vocab</code> Vocabulary</p> <p><code>llamaToken</code> LLamaToken</p> <p><code>buffer</code> Span&lt;Byte&gt; buffer to write string into</p> <p><code>lstrip</code> Int32 User can skip up to 'lstrip' leading spaces before copying (useful when encoding/decoding multiple tokens with 'add_space_prefix')</p> <p><code>special</code> Boolean If true, special tokens are rendered in the output</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_14","title":"Returns","text":"<p>Int32 The length written, or if the buffer is too small a negative that indicates the length required</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_log_setllamalogcallback","title":"llama_log_set(LLamaLogCallback)","text":""},{"location":"xmldocs/llama.native.nativeapi/#caution","title":"Caution","text":"<p>Use <code>NativeLogConfig.llama_log_set</code> instead</p> <p>Register a callback to receive llama log messages</p> <pre><code>public static void llama_log_set(LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_13","title":"Parameters","text":"<p><code>logCallback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_kv_self_seq_rmsafellamacontexthandle-llamaseqid-llamapos-llamapos","title":"llama_kv_self_seq_rm(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos)","text":"<p>Removes all tokens that belong to the specified sequence and have positions in [p0, p1)</p> <pre><code>public static bool llama_kv_self_seq_rm(SafeLLamaContextHandle ctx, LLamaSeqId seq, LLamaPos p0, LLamaPos p1)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_14","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>seq</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_15","title":"Returns","text":"<p>Boolean Returns false if a partial sequence cannot be removed. Removing a whole sequence never fails</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_batch_initint32-int32-int32","title":"llama_batch_init(Int32, Int32, Int32)","text":"<p>Allocates a batch of tokens on the heap  Each token can be assigned up to n_seq_max sequence ids  The batch has to be freed with llama_batch_free()  If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)  Otherwise, llama_batch.token will be allocated to store n_tokens llama_token  The rest of the llama_batch members are allocated with size n_tokens  All members are left uninitialized</p> <pre><code>public static LLamaNativeBatch llama_batch_init(int n_tokens, int embd, int n_seq_max)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_15","title":"Parameters","text":"<p><code>n_tokens</code> Int32</p> <p><code>embd</code> Int32</p> <p><code>n_seq_max</code> Int32 Each token can be assigned up to n_seq_max sequence ids</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_16","title":"Returns","text":"<p>LLamaNativeBatch</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_batch_freellamanativebatch","title":"llama_batch_free(LLamaNativeBatch)","text":"<p>Frees a batch of tokens allocated with llama_batch_init()</p> <pre><code>public static void llama_batch_free(LLamaNativeBatch batch)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_16","title":"Parameters","text":"<p><code>batch</code> LLamaNativeBatch</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_apply_adapter_cvecsafellamacontexthandle-single-uintptr-int32-int32-int32","title":"llama_apply_adapter_cvec(SafeLLamaContextHandle, Single*, UIntPtr, Int32, Int32, Int32)","text":"<p>Apply a loaded control vector to a llama_context, or if data is NULL, clear  the currently loaded vector.  n_embd should be the size of a single layer's control, and data should point  to an n_embd x n_layers buffer starting from layer 1.  il_start and il_end are the layer range the vector should apply to (both inclusive)  See llama_control_vector_load in common to load a control vector.</p> <pre><code>public static int llama_apply_adapter_cvec(SafeLLamaContextHandle ctx, Single* data, UIntPtr len, int n_embd, int il_start, int il_end)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_17","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>data</code> Single*</p> <p><code>len</code> UIntPtr</p> <p><code>n_embd</code> Int32</p> <p><code>il_start</code> Int32</p> <p><code>il_end</code> Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_17","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_split_pathstring-uintptr-string-int32-int32","title":"llama_split_path(String, UIntPtr, String, Int32, Int32)","text":"<p>Build a split GGUF final path for this chunk.  llama_split_path(split_path, sizeof(split_path), \"/models/ggml-model-q4_0\", 2, 4) =&gt; split_path = \"/models/ggml-model-q4_0-00002-of-00004.gguf\"</p> <pre><code>public static int llama_split_path(string split_path, UIntPtr maxlen, string path_prefix, int split_no, int split_count)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_18","title":"Parameters","text":"<p><code>split_path</code> String</p> <p><code>maxlen</code> UIntPtr</p> <p><code>path_prefix</code> String</p> <p><code>split_no</code> Int32</p> <p><code>split_count</code> Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_18","title":"Returns","text":"<p>Int32 Returns the split_path length.</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_split_prefixstring-uintptr-string-int32-int32","title":"llama_split_prefix(String, UIntPtr, String, Int32, Int32)","text":"<p>Extract the path prefix from the split_path if and only if the split_no and split_count match.  llama_split_prefix(split_prefix, 64, \"/models/ggml-model-q4_0-00002-of-00004.gguf\", 2, 4) =&gt; split_prefix = \"/models/ggml-model-q4_0\"</p> <pre><code>public static int llama_split_prefix(string split_prefix, UIntPtr maxlen, string split_path, int split_no, int split_count)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_19","title":"Parameters","text":"<p><code>split_prefix</code> String</p> <p><code>maxlen</code> UIntPtr</p> <p><code>split_path</code> String</p> <p><code>split_no</code> Int32</p> <p><code>split_count</code> Int32</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_19","title":"Returns","text":"<p>Int32 Returns the split_prefix length.</p>"},{"location":"xmldocs/llama.native.nativeapi/#ggml_backend_dev_count","title":"ggml_backend_dev_count()","text":"<p>Get the number of available backend devices</p> <pre><code>public static UIntPtr ggml_backend_dev_count()\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#returns_20","title":"Returns","text":"<p>UIntPtr Count of available backend devices</p>"},{"location":"xmldocs/llama.native.nativeapi/#ggml_backend_dev_getuintptr","title":"ggml_backend_dev_get(UIntPtr)","text":"<p>Get a backend device by index</p> <pre><code>public static IntPtr ggml_backend_dev_get(UIntPtr i)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_20","title":"Parameters","text":"<p><code>i</code> UIntPtr Device index</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_21","title":"Returns","text":"<p>IntPtr Pointer to the backend device</p>"},{"location":"xmldocs/llama.native.nativeapi/#ggml_backend_dev_buffer_typeintptr","title":"ggml_backend_dev_buffer_type(IntPtr)","text":"<p>Get the buffer type for a backend device</p> <pre><code>public static IntPtr ggml_backend_dev_buffer_type(IntPtr dev)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_21","title":"Parameters","text":"<p><code>dev</code> IntPtr Backend device pointer</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_22","title":"Returns","text":"<p>IntPtr Pointer to the buffer type</p>"},{"location":"xmldocs/llama.native.nativeapi/#ggml_backend_buft_nameintptr","title":"ggml_backend_buft_name(IntPtr)","text":"<p>Get the name of a buffer type</p> <pre><code>public static IntPtr ggml_backend_buft_name(IntPtr buft)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_22","title":"Parameters","text":"<p><code>buft</code> IntPtr Buffer type pointer</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_23","title":"Returns","text":"<p>IntPtr Name of the buffer type</p>"},{"location":"xmldocs/llama.native.nativeapi/#llava_validate_embed_sizesafellamacontexthandle-safellavamodelhandle","title":"llava_validate_embed_size(SafeLLamaContextHandle, SafeLlavaModelHandle)","text":"<p>Sanity check for clip &lt;-&gt; llava embed size match</p> <pre><code>public static bool llava_validate_embed_size(SafeLLamaContextHandle ctxLlama, SafeLlavaModelHandle ctxClip)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_23","title":"Parameters","text":"<p><code>ctxLlama</code> SafeLLamaContextHandle LLama Context</p> <p><code>ctxClip</code> SafeLlavaModelHandle Llava Model</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_24","title":"Returns","text":"<p>Boolean True if validate successfully</p>"},{"location":"xmldocs/llama.native.nativeapi/#llava_image_embed_make_with_bytessafellavamodelhandle-int32-byte-int32","title":"llava_image_embed_make_with_bytes(SafeLlavaModelHandle, Int32, Byte[], Int32)","text":"<p>Build an image embed from image file bytes</p> <pre><code>public static SafeLlavaImageEmbedHandle llava_image_embed_make_with_bytes(SafeLlavaModelHandle ctx_clip, int n_threads, Byte[] image_bytes, int image_bytes_length)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_24","title":"Parameters","text":"<p><code>ctx_clip</code> SafeLlavaModelHandle SafeHandle to the Clip Model</p> <p><code>n_threads</code> Int32 Number of threads</p> <p><code>image_bytes</code> Byte[] Binary image in jpeg format</p> <p><code>image_bytes_length</code> Int32 Bytes length of the image</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_25","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle SafeHandle to the Embeddings</p>"},{"location":"xmldocs/llama.native.nativeapi/#llava_image_embed_make_with_filenamesafellavamodelhandle-int32-string","title":"llava_image_embed_make_with_filename(SafeLlavaModelHandle, Int32, String)","text":"<p>Build an image embed from a path to an image filename</p> <pre><code>public static SafeLlavaImageEmbedHandle llava_image_embed_make_with_filename(SafeLlavaModelHandle ctx_clip, int n_threads, string image_path)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_25","title":"Parameters","text":"<p><code>ctx_clip</code> SafeLlavaModelHandle SafeHandle to the Clip Model</p> <p><code>n_threads</code> Int32 Number of threads</p> <p><code>image_path</code> String Image filename (jpeg) to generate embeddings</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_26","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle SafeHandle to the embeddings</p>"},{"location":"xmldocs/llama.native.nativeapi/#llava_image_embed_freeintptr","title":"llava_image_embed_free(IntPtr)","text":"<p>Free an embedding made with llava_image_embed_make_*</p> <pre><code>public static void llava_image_embed_free(IntPtr embed)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_26","title":"Parameters","text":"<p><code>embed</code> IntPtr Embeddings to release</p>"},{"location":"xmldocs/llama.native.nativeapi/#llava_eval_image_embedsafellamacontexthandle-safellavaimageembedhandle-int32-int32","title":"llava_eval_image_embed(SafeLLamaContextHandle, SafeLlavaImageEmbedHandle, Int32, Int32&amp;)","text":"<p>Write the image represented by embed into the llama context with batch size n_batch, starting at context  pos n_past. on completion, n_past points to the next position in the context after the image embed.</p> <pre><code>public static bool llava_eval_image_embed(SafeLLamaContextHandle ctx_llama, SafeLlavaImageEmbedHandle embed, int n_batch, Int32&amp; n_past)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_27","title":"Parameters","text":"<p><code>ctx_llama</code> SafeLLamaContextHandle Llama Context</p> <p><code>embed</code> SafeLlavaImageEmbedHandle Embedding handle</p> <p><code>n_batch</code> Int32</p> <p><code>n_past</code> Int32&amp;</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_27","title":"Returns","text":"<p>Boolean True on success</p>"},{"location":"xmldocs/llama.native.nativeapi/#getloadednativelibrarynativelibraryname","title":"GetLoadedNativeLibrary(NativeLibraryName)","text":"<p>Get the loaded native library. If you are using netstandard2.0, it will always return null.</p> <pre><code>public static INativeLibrary GetLoadedNativeLibrary(NativeLibraryName name)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_28","title":"Parameters","text":"<p><code>name</code> NativeLibraryName</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_28","title":"Returns","text":"<p>INativeLibrary</p>"},{"location":"xmldocs/llama.native.nativeapi/#exceptions","title":"Exceptions","text":"<p>ArgumentException</p>"},{"location":"xmldocs/llama.native.nativeapi/#llama_model_quantizestring-string-llamamodelquantizeparams","title":"llama_model_quantize(String, String, LLamaModelQuantizeParams&amp;)","text":"<p>Returns 0 on success</p> <pre><code>public static uint llama_model_quantize(string fname_inp, string fname_out, LLamaModelQuantizeParams&amp; param)\n</code></pre>"},{"location":"xmldocs/llama.native.nativeapi/#parameters_29","title":"Parameters","text":"<p><code>fname_inp</code> String</p> <p><code>fname_out</code> String</p> <p><code>param</code> LLamaModelQuantizeParams&amp;</p>"},{"location":"xmldocs/llama.native.nativeapi/#returns_29","title":"Returns","text":"<p>UInt32 Returns 0 on success</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/","title":"llama.native.nativelibraryconfig","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#nativelibraryconfig","title":"NativeLibraryConfig","text":"<p>Namespace: LLama.Native</p> <p>Allows configuration of the native llama.cpp libraries to load and use.  All configuration must be done before using any other LLamaSharp methods!</p> <pre><code>public sealed class NativeLibraryConfig\n</code></pre> <p>Inheritance Object \u2192 NativeLibraryConfig Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.nativelibraryconfig/#instance","title":"Instance","text":""},{"location":"xmldocs/llama.native.nativelibraryconfig/#caution","title":"Caution","text":"<p>Please use NativeLibraryConfig.All instead, or set configurations for NativeLibraryConfig.LLama and NativeLibraryConfig.LLavaShared respectively.</p> <p>Set configurations for all the native libraries, including LLama and LLava</p> <pre><code>public static NativeLibraryConfigContainer Instance { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#property-value","title":"Property Value","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#all","title":"All","text":"<p>Set configurations for all the native libraries, including LLama and LLava</p> <pre><code>public static NativeLibraryConfigContainer All { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#property-value_1","title":"Property Value","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#llama","title":"LLama","text":"<p>Configuration for LLama native library</p> <pre><code>public static NativeLibraryConfig LLama { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#property-value_2","title":"Property Value","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#llava","title":"LLava","text":"<p>Configuration for LLava native library</p> <pre><code>public static NativeLibraryConfig LLava { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#property-value_3","title":"Property Value","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#libraryhasloaded","title":"LibraryHasLoaded","text":"<p>Check if the native library has already been loaded. Configuration cannot be modified if this is true.</p> <pre><code>public bool LibraryHasLoaded { get; internal set; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#property-value_4","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativelibraryconfig/#withlibrarystring","title":"WithLibrary(String)","text":"<p>Load a specified native library as backend for LLamaSharp.  When this method is called, all the other configurations will be ignored.</p> <pre><code>public NativeLibraryConfig WithLibrary(string libraryPath)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters","title":"Parameters","text":"<p><code>libraryPath</code> String The full path to the native library to load.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withcudaboolean","title":"WithCuda(Boolean)","text":"<p>Configure whether to use cuda backend if possible. Default is true.</p> <pre><code>public NativeLibraryConfig WithCuda(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_1","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_1","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_1","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withvulkanboolean","title":"WithVulkan(Boolean)","text":"<p>Configure whether to use vulkan backend if possible. Default is true.</p> <pre><code>public NativeLibraryConfig WithVulkan(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_2","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_2","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_2","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withavxavxlevel","title":"WithAvx(AvxLevel)","text":"<p>Configure the prefferred avx support level of the backend.   Default value is detected automatically due to your operating system.</p> <pre><code>public NativeLibraryConfig WithAvx(AvxLevel level)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_3","title":"Parameters","text":"<p><code>level</code> AvxLevel</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_3","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_3","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withautofallbackboolean","title":"WithAutoFallback(Boolean)","text":"<p>Configure whether to allow fallback when there's no match for preferred settings. Default is true.</p> <pre><code>public NativeLibraryConfig WithAutoFallback(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_4","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_4","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_4","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#skipcheckboolean","title":"SkipCheck(Boolean)","text":"<p>Whether to skip the check when you don't allow fallback. This option   may be useful under some complex conditions. For example, you're sure   you have your cublas configured but LLamaSharp take it as invalid by mistake. Default is false;</p> <pre><code>public NativeLibraryConfig SkipCheck(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_5","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_5","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_5","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withsearchdirectoriesienumerablestring","title":"WithSearchDirectories(IEnumerable&lt;String&gt;)","text":"<p>Add self-defined search directories. Note that the file structure of the added   directories must be the same as the default directory. Besides, the directory   won't be used recursively.</p> <pre><code>public NativeLibraryConfig WithSearchDirectories(IEnumerable&lt;string&gt; directories)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_6","title":"Parameters","text":"<p><code>directories</code> IEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_6","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withsearchdirectorystring","title":"WithSearchDirectory(String)","text":"<p>Add self-defined search directories. Note that the file structure of the added   directories must be the same as the default directory. Besides, the directory   won't be used recursively.</p> <pre><code>public NativeLibraryConfig WithSearchDirectory(string directory)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_7","title":"Parameters","text":"<p><code>directory</code> String</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_7","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withselectingpolicyinativelibraryselectingpolicy","title":"WithSelectingPolicy(INativeLibrarySelectingPolicy)","text":"<p>Set the policy which decides how to select the desired native libraries and order them by priority.   By default we use DefaultNativeLibrarySelectingPolicy.</p> <pre><code>public NativeLibraryConfig WithSelectingPolicy(INativeLibrarySelectingPolicy policy)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_8","title":"Parameters","text":"<p><code>policy</code> INativeLibrarySelectingPolicy</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_8","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withlogcallbackllamalogcallback","title":"WithLogCallback(LLamaLogCallback)","text":"<p>Set the log callback that will be used for all llama.cpp log messages</p> <pre><code>public NativeLibraryConfig WithLogCallback(LLamaLogCallback callback)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_9","title":"Parameters","text":"<p><code>callback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_9","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_6","title":"Exceptions","text":"<p>NotImplementedException</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#withlogcallbackilogger","title":"WithLogCallback(ILogger)","text":"<p>Set the log callback that will be used for all llama.cpp log messages</p> <pre><code>public NativeLibraryConfig WithLogCallback(ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_10","title":"Parameters","text":"<p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_10","title":"Returns","text":"<p>NativeLibraryConfig</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#exceptions_7","title":"Exceptions","text":"<p>NotImplementedException</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#dryruninativelibrary","title":"DryRun(INativeLibrary&amp;)","text":"<p>Try to load the native library with the current configurations,   but do not actually set it to NativeApi.</p> <p>You can still modify the configuration after this calling but only before any call from NativeApi.</p> <pre><code>public bool DryRun(INativeLibrary&amp; loadedLibrary)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#parameters_11","title":"Parameters","text":"<p><code>loadedLibrary</code> INativeLibrary&amp; The loaded livrary. When the loading failed, this will be null.   However if you are using .NET standard2.0, this will never return null.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfig/#returns_11","title":"Returns","text":"<p>Boolean Whether the running is successful.</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/","title":"Llama.native.nativelibraryconfigcontainer","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#nativelibraryconfigcontainer","title":"NativeLibraryConfigContainer","text":"<p>Namespace: LLama.Native</p> <p>A class to set same configurations to multiple libraries at the same time.</p> <pre><code>public sealed class NativeLibraryConfigContainer\n</code></pre> <p>Inheritance Object \u2192 NativeLibraryConfigContainer Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#foreachactionnativelibraryconfig","title":"ForEach(Action&lt;NativeLibraryConfig&gt;)","text":"<p>Do an action for all the configs in this container.</p> <pre><code>public void ForEach(Action&lt;NativeLibraryConfig&gt; action)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters","title":"Parameters","text":"<p><code>action</code> Action&lt;NativeLibraryConfig&gt;</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#withlibrarystring-string","title":"WithLibrary(String, String)","text":"<p>Load a specified native library as backend for LLamaSharp.  When this method is called, all the other configurations will be ignored.</p> <pre><code>public NativeLibraryConfigContainer WithLibrary(string llamaPath, string llavaPath)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_1","title":"Parameters","text":"<p><code>llamaPath</code> String The full path to the llama library to load.</p> <p><code>llavaPath</code> String The full path to the llava library to load.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns","title":"Returns","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#exceptions","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#withcudaboolean","title":"WithCuda(Boolean)","text":"<p>Configure whether to use cuda backend if possible.</p> <pre><code>public NativeLibraryConfigContainer WithCuda(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_2","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns_1","title":"Returns","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#exceptions_1","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#withvulkanboolean","title":"WithVulkan(Boolean)","text":"<p>Configure whether to use vulkan backend if possible.</p> <pre><code>public NativeLibraryConfigContainer WithVulkan(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_3","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns_2","title":"Returns","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#exceptions_2","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#withavxavxlevel","title":"WithAvx(AvxLevel)","text":"<p>Configure the prefferred avx support level of the backend.</p> <pre><code>public NativeLibraryConfigContainer WithAvx(AvxLevel level)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_4","title":"Parameters","text":"<p><code>level</code> AvxLevel</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns_3","title":"Returns","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#exceptions_3","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#withautofallbackboolean","title":"WithAutoFallback(Boolean)","text":"<p>Configure whether to allow fallback when there's no match for preferred settings.</p> <pre><code>public NativeLibraryConfigContainer WithAutoFallback(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_5","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns_4","title":"Returns","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#exceptions_4","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#skipcheckboolean","title":"SkipCheck(Boolean)","text":"<p>Whether to skip the check when you don't allow fallback. This option   may be useful under some complex conditions. For example, you're sure   you have your cublas configured but LLamaSharp take it as invalid by mistake.</p> <pre><code>public NativeLibraryConfigContainer SkipCheck(bool enable)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_6","title":"Parameters","text":"<p><code>enable</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns_5","title":"Returns","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#exceptions_5","title":"Exceptions","text":"<p>InvalidOperationException Thrown if <code>LibraryHasLoaded</code> is true.</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#withsearchdirectoriesienumerablestring","title":"WithSearchDirectories(IEnumerable&lt;String&gt;)","text":"<p>Add self-defined search directories. Note that the file structure of the added   directories must be the same as the default directory. Besides, the directory   won't be used recursively.</p> <pre><code>public NativeLibraryConfigContainer WithSearchDirectories(IEnumerable&lt;string&gt; directories)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_7","title":"Parameters","text":"<p><code>directories</code> IEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns_6","title":"Returns","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#withsearchdirectorystring","title":"WithSearchDirectory(String)","text":"<p>Add self-defined search directories. Note that the file structure of the added   directories must be the same as the default directory. Besides, the directory   won't be used recursively.</p> <pre><code>public NativeLibraryConfigContainer WithSearchDirectory(string directory)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_8","title":"Parameters","text":"<p><code>directory</code> String</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns_7","title":"Returns","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#withselectingpolicyinativelibraryselectingpolicy","title":"WithSelectingPolicy(INativeLibrarySelectingPolicy)","text":"<p>Set the policy which decides how to select the desired native libraries and order them by priority.   By default we use DefaultNativeLibrarySelectingPolicy.</p> <pre><code>public NativeLibraryConfigContainer WithSelectingPolicy(INativeLibrarySelectingPolicy policy)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_9","title":"Parameters","text":"<p><code>policy</code> INativeLibrarySelectingPolicy</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns_8","title":"Returns","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#withlogcallbackllamalogcallback","title":"WithLogCallback(LLamaLogCallback)","text":"<p>Set the log callback that will be used for all llama.cpp log messages</p> <pre><code>public NativeLibraryConfigContainer WithLogCallback(LLamaLogCallback callback)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_10","title":"Parameters","text":"<p><code>callback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns_9","title":"Returns","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#exceptions_6","title":"Exceptions","text":"<p>NotImplementedException</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#withlogcallbackilogger","title":"WithLogCallback(ILogger)","text":"<p>Set the log callback that will be used for all llama.cpp log messages</p> <pre><code>public NativeLibraryConfigContainer WithLogCallback(ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_11","title":"Parameters","text":"<p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns_10","title":"Returns","text":"<p>NativeLibraryConfigContainer</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#exceptions_7","title":"Exceptions","text":"<p>NotImplementedException</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#dryruninativelibrary-inativelibrary","title":"DryRun(INativeLibrary&amp;, INativeLibrary&amp;)","text":"<p>Try to load the native library with the current configurations,   but do not actually set it to NativeApi.</p> <p>You can still modify the configuration after this calling but only before any call from NativeApi.</p> <pre><code>public bool DryRun(INativeLibrary&amp; loadedLLamaNativeLibrary, INativeLibrary&amp; loadedLLavaNativeLibrary)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#parameters_12","title":"Parameters","text":"<p><code>loadedLLamaNativeLibrary</code> INativeLibrary&amp;</p> <p><code>loadedLLavaNativeLibrary</code> INativeLibrary&amp;</p>"},{"location":"xmldocs/llama.native.nativelibraryconfigcontainer/#returns_11","title":"Returns","text":"<p>Boolean Whether the running is successful.</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibraryfrompath/","title":"Llama.native.nativelibraryfrompath","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibraryfrompath/#nativelibraryfrompath","title":"NativeLibraryFromPath","text":"<p>Namespace: LLama.Native</p> <p>A native library specified with a local file path.</p> <pre><code>public class NativeLibraryFromPath : LLama.Abstractions.INativeLibrary\n</code></pre> <p>Inheritance Object \u2192 NativeLibraryFromPath Implements INativeLibrary Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.nativelibraryfrompath/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.nativelibraryfrompath/#metadata","title":"Metadata","text":"<pre><code>public NativeLibraryMetadata Metadata { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryfrompath/#property-value","title":"Property Value","text":"<p>NativeLibraryMetadata</p>"},{"location":"xmldocs/llama.native.nativelibraryfrompath/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.nativelibraryfrompath/#nativelibraryfrompathstring","title":"NativeLibraryFromPath(String)","text":"<pre><code>public NativeLibraryFromPath(string path)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryfrompath/#parameters","title":"Parameters","text":"<p><code>path</code> String</p>"},{"location":"xmldocs/llama.native.nativelibraryfrompath/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativelibraryfrompath/#preparesysteminfo-llamalogcallback","title":"Prepare(SystemInfo, LLamaLogCallback)","text":"<pre><code>public IEnumerable&lt;string&gt; Prepare(SystemInfo systemInfo, LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibraryfrompath/#parameters_1","title":"Parameters","text":"<p><code>systemInfo</code> SystemInfo</p> <p><code>logCallback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.nativelibraryfrompath/#returns","title":"Returns","text":"<p>IEnumerable&lt;String&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/","title":"Llama.native.nativelibrarymetadata","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#nativelibrarymetadata","title":"NativeLibraryMetadata","text":"<p>Namespace: LLama.Native</p> <p>Information of a native library file.</p> <pre><code>public class NativeLibraryMetadata : System.IEquatable`1[[LLama.Native.NativeLibraryMetadata, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 NativeLibraryMetadata Implements IEquatable&lt;NativeLibraryMetadata&gt; Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.nativelibrarymetadata/#equalitycontract","title":"EqualityContract","text":"<pre><code>protected Type EqualityContract { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#property-value","title":"Property Value","text":"<p>Type</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#nativelibraryname","title":"NativeLibraryName","text":"<p>Which kind of library it is.</p> <pre><code>public NativeLibraryName NativeLibraryName { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#property-value_1","title":"Property Value","text":"<p>NativeLibraryName</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#usecuda","title":"UseCuda","text":"<p>Whether it's compiled with cublas.</p> <pre><code>public bool UseCuda { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#property-value_2","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#usevulkan","title":"UseVulkan","text":"<p>Whether it's compiled with vulkan.</p> <pre><code>public bool UseVulkan { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#avxlevel","title":"AvxLevel","text":"<p>Which AvxLevel it's compiled with.</p> <pre><code>public AvxLevel AvxLevel { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#property-value_4","title":"Property Value","text":"<p>AvxLevel</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.nativelibrarymetadata/#nativelibrarymetadatanativelibraryname-boolean-boolean-avxlevel","title":"NativeLibraryMetadata(NativeLibraryName, Boolean, Boolean, AvxLevel)","text":"<p>Information of a native library file.</p> <pre><code>public NativeLibraryMetadata(NativeLibraryName NativeLibraryName, bool UseCuda, bool UseVulkan, AvxLevel AvxLevel)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#parameters","title":"Parameters","text":"<p><code>NativeLibraryName</code> NativeLibraryName Which kind of library it is.</p> <p><code>UseCuda</code> Boolean Whether it's compiled with cublas.</p> <p><code>UseVulkan</code> Boolean Whether it's compiled with vulkan.</p> <p><code>AvxLevel</code> AvxLevel Which AvxLevel it's compiled with.</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#nativelibrarymetadatanativelibrarymetadata","title":"NativeLibraryMetadata(NativeLibraryMetadata)","text":"<pre><code>protected NativeLibraryMetadata(NativeLibraryMetadata original)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#parameters_1","title":"Parameters","text":"<p><code>original</code> NativeLibraryMetadata</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativelibrarymetadata/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#parameters_2","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#parameters_3","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#equalsnativelibrarymetadata","title":"Equals(NativeLibraryMetadata)","text":"<pre><code>public bool Equals(NativeLibraryMetadata other)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#parameters_4","title":"Parameters","text":"<p><code>other</code> NativeLibraryMetadata</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public NativeLibraryMetadata &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#returns_5","title":"Returns","text":"<p>NativeLibraryMetadata</p>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#deconstructnativelibraryname-boolean-boolean-avxlevel","title":"Deconstruct(NativeLibraryName&amp;, Boolean&amp;, Boolean&amp;, AvxLevel&amp;)","text":"<pre><code>public void Deconstruct(NativeLibraryName&amp; NativeLibraryName, Boolean&amp; UseCuda, Boolean&amp; UseVulkan, AvxLevel&amp; AvxLevel)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarymetadata/#parameters_5","title":"Parameters","text":"<p><code>NativeLibraryName</code> NativeLibraryName&amp;</p> <p><code>UseCuda</code> Boolean&amp;</p> <p><code>UseVulkan</code> Boolean&amp;</p> <p><code>AvxLevel</code> AvxLevel&amp;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibraryname/","title":"Llama.native.nativelibraryname","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibraryname/#nativelibraryname","title":"NativeLibraryName","text":"<p>Namespace: LLama.Native</p> <p>The name of the native library</p> <pre><code>public enum NativeLibraryName\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 NativeLibraryName Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p>"},{"location":"xmldocs/llama.native.nativelibraryname/#fields","title":"Fields","text":"Name Value Description LLama 0 The native library compiled from llama.cpp. LLava 1 The native library compiled from the LLaVA example of llama.cpp. <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibrarywithavx/","title":"Llama.native.nativelibrarywithavx","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibrarywithavx/#nativelibrarywithavx","title":"NativeLibraryWithAvx","text":"<p>Namespace: LLama.Native</p> <p>A native library compiled with avx support but without cuda/cublas.</p> <pre><code>public class NativeLibraryWithAvx : LLama.Abstractions.INativeLibrary\n</code></pre> <p>Inheritance Object \u2192 NativeLibraryWithAvx Implements INativeLibrary Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.nativelibrarywithavx/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.nativelibrarywithavx/#metadata","title":"Metadata","text":"<pre><code>public NativeLibraryMetadata Metadata { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithavx/#property-value","title":"Property Value","text":"<p>NativeLibraryMetadata</p>"},{"location":"xmldocs/llama.native.nativelibrarywithavx/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.nativelibrarywithavx/#nativelibrarywithavxnativelibraryname-avxlevel-boolean","title":"NativeLibraryWithAvx(NativeLibraryName, AvxLevel, Boolean)","text":"<pre><code>public NativeLibraryWithAvx(NativeLibraryName libraryName, AvxLevel avxLevel, bool skipCheck)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithavx/#parameters","title":"Parameters","text":"<p><code>libraryName</code> NativeLibraryName</p> <p><code>avxLevel</code> AvxLevel</p> <p><code>skipCheck</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibrarywithavx/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativelibrarywithavx/#preparesysteminfo-llamalogcallback","title":"Prepare(SystemInfo, LLamaLogCallback)","text":"<pre><code>public IEnumerable&lt;string&gt; Prepare(SystemInfo systemInfo, LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithavx/#parameters_1","title":"Parameters","text":"<p><code>systemInfo</code> SystemInfo</p> <p><code>logCallback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.nativelibrarywithavx/#returns","title":"Returns","text":"<p>IEnumerable&lt;String&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibrarywithcuda/","title":"Llama.native.nativelibrarywithcuda","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibrarywithcuda/#nativelibrarywithcuda","title":"NativeLibraryWithCuda","text":"<p>Namespace: LLama.Native</p> <p>A native library compiled with cublas/cuda.</p> <pre><code>public class NativeLibraryWithCuda : LLama.Abstractions.INativeLibrary\n</code></pre> <p>Inheritance Object \u2192 NativeLibraryWithCuda Implements INativeLibrary Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.nativelibrarywithcuda/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.nativelibrarywithcuda/#metadata","title":"Metadata","text":"<pre><code>public NativeLibraryMetadata Metadata { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithcuda/#property-value","title":"Property Value","text":"<p>NativeLibraryMetadata</p>"},{"location":"xmldocs/llama.native.nativelibrarywithcuda/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.nativelibrarywithcuda/#nativelibrarywithcudaint32-nativelibraryname-avxlevel-boolean","title":"NativeLibraryWithCuda(Int32, NativeLibraryName, AvxLevel, Boolean)","text":"<pre><code>public NativeLibraryWithCuda(int majorCudaVersion, NativeLibraryName libraryName, AvxLevel avxLevel, bool skipCheck)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithcuda/#parameters","title":"Parameters","text":"<p><code>majorCudaVersion</code> Int32</p> <p><code>libraryName</code> NativeLibraryName</p> <p><code>avxLevel</code> AvxLevel</p> <p><code>skipCheck</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibrarywithcuda/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativelibrarywithcuda/#preparesysteminfo-llamalogcallback","title":"Prepare(SystemInfo, LLamaLogCallback)","text":"<pre><code>public IEnumerable&lt;string&gt; Prepare(SystemInfo systemInfo, LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithcuda/#parameters_1","title":"Parameters","text":"<p><code>systemInfo</code> SystemInfo</p> <p><code>logCallback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.nativelibrarywithcuda/#returns","title":"Returns","text":"<p>IEnumerable&lt;String&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/","title":"Llama.native.nativelibrarywithmacorfallback","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/#nativelibrarywithmacorfallback","title":"NativeLibraryWithMacOrFallback","text":"<p>Namespace: LLama.Native</p> <p>A native library compiled on Mac, or fallbacks from all other libraries in the selection.</p> <pre><code>public class NativeLibraryWithMacOrFallback : LLama.Abstractions.INativeLibrary\n</code></pre> <p>Inheritance Object \u2192 NativeLibraryWithMacOrFallback Implements INativeLibrary Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/#metadata","title":"Metadata","text":"<pre><code>public NativeLibraryMetadata Metadata { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/#property-value","title":"Property Value","text":"<p>NativeLibraryMetadata</p>"},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/#nativelibrarywithmacorfallbacknativelibraryname","title":"NativeLibraryWithMacOrFallback(NativeLibraryName)","text":"<pre><code>public NativeLibraryWithMacOrFallback(NativeLibraryName libraryName)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/#parameters","title":"Parameters","text":"<p><code>libraryName</code> NativeLibraryName</p>"},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/#preparesysteminfo-llamalogcallback","title":"Prepare(SystemInfo, LLamaLogCallback)","text":"<pre><code>public IEnumerable&lt;string&gt; Prepare(SystemInfo systemInfo, LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/#parameters_1","title":"Parameters","text":"<p><code>systemInfo</code> SystemInfo</p> <p><code>logCallback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.nativelibrarywithmacorfallback/#returns","title":"Returns","text":"<p>IEnumerable&lt;String&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/","title":"Llama.native.nativelibrarywithvulkan","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/#nativelibrarywithvulkan","title":"NativeLibraryWithVulkan","text":"<p>Namespace: LLama.Native</p> <p>A native library compiled with vulkan.</p> <pre><code>public class NativeLibraryWithVulkan : LLama.Abstractions.INativeLibrary\n</code></pre> <p>Inheritance Object \u2192 NativeLibraryWithVulkan Implements INativeLibrary Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/#metadata","title":"Metadata","text":"<pre><code>public NativeLibraryMetadata Metadata { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/#property-value","title":"Property Value","text":"<p>NativeLibraryMetadata</p>"},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/#nativelibrarywithvulkanstring-nativelibraryname-avxlevel-boolean","title":"NativeLibraryWithVulkan(String, NativeLibraryName, AvxLevel, Boolean)","text":"<pre><code>public NativeLibraryWithVulkan(string vulkanVersion, NativeLibraryName libraryName, AvxLevel avxLevel, bool skipCheck)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/#parameters","title":"Parameters","text":"<p><code>vulkanVersion</code> String</p> <p><code>libraryName</code> NativeLibraryName</p> <p><code>avxLevel</code> AvxLevel</p> <p><code>skipCheck</code> Boolean</p>"},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/#preparesysteminfo-llamalogcallback","title":"Prepare(SystemInfo, LLamaLogCallback)","text":"<pre><code>public IEnumerable&lt;string&gt; Prepare(SystemInfo systemInfo, LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/#parameters_1","title":"Parameters","text":"<p><code>systemInfo</code> SystemInfo</p> <p><code>logCallback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.nativelibrarywithvulkan/#returns","title":"Returns","text":"<p>IEnumerable&lt;String&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelogconfig/","title":"Llama.native.nativelogconfig","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.nativelogconfig/#nativelogconfig","title":"NativeLogConfig","text":"<p>Namespace: LLama.Native</p> <p>Configure llama.cpp logging</p> <pre><code>public static class NativeLogConfig\n</code></pre> <p>Inheritance Object \u2192 NativeLogConfig Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.nativelogconfig/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.nativelogconfig/#llama_log_setllamalogcallback","title":"llama_log_set(LLamaLogCallback)","text":"<p>Register a callback to receive llama log messages</p> <pre><code>public static void llama_log_set(LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelogconfig/#parameters","title":"Parameters","text":"<p><code>logCallback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.nativelogconfig/#llama_log_setilogger","title":"llama_log_set(ILogger)","text":"<p>Register a callback to receive llama log messages</p> <pre><code>public static void llama_log_set(ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.native.nativelogconfig/#parameters_1","title":"Parameters","text":"<p><code>logger</code> ILogger</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.ropescalingtype/","title":"llama.native.ropescalingtype","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.ropescalingtype/#ropescalingtype","title":"RopeScalingType","text":"<p>Namespace: LLama.Native</p> <p>RoPE scaling type.</p> <pre><code>public enum RopeScalingType\n</code></pre> <p>Inheritance Object \u2192 ValueType \u2192 Enum \u2192 RopeScalingType Implements IComparable, ISpanFormattable, IFormattable, IConvertible</p> <p>Remarks:</p> <p>C# equivalent of llama_rope_scaling_type</p>"},{"location":"xmldocs/llama.native.ropescalingtype/#fields","title":"Fields","text":"Name Value Description Unspecified -1 No particular scaling type has been specified None 0 Do not apply any RoPE scaling Linear 1 Positional linear interpolation, as described by kaikendev: https://kaiokendev.github.io/til#extending-context-to-8k Yarn 2 YaRN scaling: https://arxiv.org/pdf/2309.00071.pdf LongRope 3 LongRope scaling <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/","title":"llama.native.safellamacontexthandle","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#safellamacontexthandle","title":"SafeLLamaContextHandle","text":"<p>Namespace: LLama.Native</p> <p>A safe wrapper around a llama_context</p> <pre><code>public sealed class SafeLLamaContextHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 SafeLLamaContextHandle Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.safellamacontexthandle/#handle","title":"handle","text":"<pre><code>protected IntPtr handle;\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellamacontexthandle/#contextsize","title":"ContextSize","text":"<p>Total number of tokens in the context</p> <pre><code>public uint ContextSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#embeddingsize","title":"EmbeddingSize","text":"<p>Dimension of embedding vectors</p> <pre><code>public int EmbeddingSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#batchsize","title":"BatchSize","text":"<p>Get the maximum batch size for this context</p> <pre><code>public uint BatchSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_2","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#ubatchsize","title":"UBatchSize","text":"<p>Get the physical maximum batch size for this context</p> <pre><code>public uint UBatchSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_3","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#generationthreads","title":"GenerationThreads","text":"<p>Get or set the number of threads used for generation of a single token.</p> <pre><code>public int GenerationThreads { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_4","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#batchthreads","title":"BatchThreads","text":"<p>Get or set the number of threads used for prompt and batch processing (multiple token).</p> <pre><code>public int BatchThreads { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_5","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#poolingtype","title":"PoolingType","text":"<p>Get the pooling type for this context</p> <pre><code>public LLamaPoolingType PoolingType { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_6","title":"Property Value","text":"<p>LLamaPoolingType</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#modelhandle","title":"ModelHandle","text":"<p>Get the model which this context is using</p> <pre><code>public SafeLlamaModelHandle ModelHandle { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_7","title":"Property Value","text":"<p>SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#vocab","title":"Vocab","text":"<p>Get the vocabulary for the model this context is using</p> <pre><code>public Vocabulary Vocab { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_8","title":"Property Value","text":"<p>Vocabulary</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachecanshift","title":"KvCacheCanShift","text":"<p>Check if the context supports KV cache shifting</p> <pre><code>public bool KvCacheCanShift { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_9","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_10","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#property-value_11","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.safellamacontexthandle/#safellamacontexthandle_1","title":"SafeLLamaContextHandle()","text":"<pre><code>public SafeLLamaContextHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellamacontexthandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#createsafellamamodelhandle-llamacontextparams","title":"Create(SafeLlamaModelHandle, LLamaContextParams)","text":"<p>Create a new llama_state for the given model</p> <pre><code>public static SafeLLamaContextHandle Create(SafeLlamaModelHandle model, LLamaContextParams lparams)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>lparams</code> LLamaContextParams</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_1","title":"Returns","text":"<p>SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#exceptions","title":"Exceptions","text":"<p>RuntimeError</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#addloraadapterloraadapter-single","title":"AddLoraAdapter(LoraAdapter, Single)","text":"<p>Add a LoRA adapter to this context</p> <pre><code>public void AddLoraAdapter(LoraAdapter lora, float scale)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_1","title":"Parameters","text":"<p><code>lora</code> LoraAdapter</p> <p><code>scale</code> Single</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#exceptions_1","title":"Exceptions","text":"<p>ArgumentException</p> <p>RuntimeError</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#removeloraadapterloraadapter","title":"RemoveLoraAdapter(LoraAdapter)","text":"<p>Remove a LoRA adapter from this context</p> <pre><code>public bool RemoveLoraAdapter(LoraAdapter lora)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_2","title":"Parameters","text":"<p><code>lora</code> LoraAdapter</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_2","title":"Returns","text":"<p>Boolean Indicates if the lora was in this context and was remove</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#clearloraadapters","title":"ClearLoraAdapters()","text":"<p>Remove all LoRA adapters from this context</p> <pre><code>public void ClearLoraAdapters()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getlogitsint32","title":"GetLogits(Int32)","text":"<p>Token logits obtained from the last call to llama_decode.  The logits for the last token are stored in the last row.  Only tokens with <code>logits = true</code> requested are present.  Can be mutated in order to change the probabilities of the next token.  Rows: n_tokens  Cols: n_vocab</p> <pre><code>public Span&lt;float&gt; GetLogits(int numTokens)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_3","title":"Parameters","text":"<p><code>numTokens</code> Int32 The amount of tokens whose logits should be retrieved, in [numTokens X n_vocab] format.  Tokens' order is based on their order in the LlamaBatch (so, first tokens are first, etc).  This is helpful when requesting logits for many tokens in a sequence, or want to decode multiple sequences in one go.</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_3","title":"Returns","text":"<p>Span&lt;Single&gt;</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getlogitsithint32","title":"GetLogitsIth(Int32)","text":"<p>Logits for the ith token. Equivalent to: llama_get_logits(ctx) + i*n_vocab</p> <pre><code>public Span&lt;float&gt; GetLogitsIth(int i)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_4","title":"Parameters","text":"<p><code>i</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_4","title":"Returns","text":"<p>Span&lt;Single&gt;</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getembeddingsithllamapos","title":"GetEmbeddingsIth(LLamaPos)","text":"<p>Get the embeddings for the ith sequence.  Equivalent to: llama_get_embeddings(ctx) + ctx-&gt;output_ids[i]*n_embd</p> <pre><code>public Span&lt;float&gt; GetEmbeddingsIth(LLamaPos pos)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_5","title":"Parameters","text":"<p><code>pos</code> LLamaPos</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_5","title":"Returns","text":"<p>Span&lt;Single&gt; A pointer to the first float in an embedding, length = ctx.EmbeddingSize</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getembeddingsseqllamaseqid","title":"GetEmbeddingsSeq(LLamaSeqId)","text":"<p>Get the embeddings for the a specific sequence.  Equivalent to: llama_get_embeddings(ctx) + ctx-&gt;output_ids[i]*n_embd</p> <pre><code>public Span&lt;float&gt; GetEmbeddingsSeq(LLamaSeqId seq)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_6","title":"Parameters","text":"<p><code>seq</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_6","title":"Returns","text":"<p>Span&lt;Single&gt; A pointer to the first float in an embedding, length = ctx.EmbeddingSize</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#tokenizestring-boolean-boolean-encoding","title":"Tokenize(String, Boolean, Boolean, Encoding)","text":"<p>Convert the given text into tokens</p> <pre><code>public LLamaToken[] Tokenize(string text, bool add_bos, bool special, Encoding encoding)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_7","title":"Parameters","text":"<p><code>text</code> String The text to tokenize</p> <p><code>add_bos</code> Boolean Whether the \"BOS\" token should be added</p> <p><code>special</code> Boolean Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.</p> <p><code>encoding</code> Encoding Encoding to use for the text</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_7","title":"Returns","text":"<p>LLamaToken[]</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#exceptions_2","title":"Exceptions","text":"<p>RuntimeError</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#tokentospanllamatoken-spanbyte","title":"TokenToSpan(LLamaToken, Span&lt;Byte&gt;)","text":"<p>Convert a single llama token into bytes</p> <pre><code>public uint TokenToSpan(LLamaToken token, Span&lt;byte&gt; dest)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_8","title":"Parameters","text":"<p><code>token</code> LLamaToken Token to decode</p> <p><code>dest</code> Span&lt;Byte&gt; A span to attempt to write into. If this is too small nothing will be written</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_8","title":"Returns","text":"<p>UInt32 The size of this token. nothing will be written if this is larger than <code>dest</code></p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#synchronize","title":"Synchronize()","text":"<p>Wait until all computations are finished. This is automatically done when using any of the functions to obtain computation results  and is not necessary to call it explicitly in most cases.</p> <pre><code>public void Synchronize()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#encodellamabatch","title":"Encode(LLamaBatch)","text":"<p>Processes a batch of tokens with the encoder part of the encoder-decoder model. Stores the encoder output  internally for later use by the decoder cross-attention layers.</p> <pre><code>public DecodeResult Encode(LLamaBatch batch)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_9","title":"Parameters","text":"<p><code>batch</code> LLamaBatch</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_9","title":"Returns","text":"<p>DecodeResult 0 = success &lt; 0 = error (the KV cache state is restored to the state before this call)</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#decodellamabatch","title":"Decode(LLamaBatch)","text":"<pre><code>public DecodeResult Decode(LLamaBatch batch)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_10","title":"Parameters","text":"<p><code>batch</code> LLamaBatch</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_10","title":"Returns","text":"<p>DecodeResult Positive return values does not mean a fatal error, but rather a warning:  - 0: success  - 1: could not find a KV slot for the batch (try reducing the size of the batch or increase the context)  - &lt; 0: error (the KV cache state is restored to the state before this call)</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#decodellamabatchembeddings","title":"Decode(LLamaBatchEmbeddings)","text":"<pre><code>public DecodeResult Decode(LLamaBatchEmbeddings batch)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_11","title":"Parameters","text":"<p><code>batch</code> LLamaBatchEmbeddings</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_11","title":"Returns","text":"<p>DecodeResult Positive return values does not mean a fatal error, but rather a warning:  - 0: success  - 1: could not find a KV slot for the batch (try reducing the size of the batch or increase the context)  - &lt; 0: error</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getstatesize","title":"GetStateSize()","text":"<p>Get the size of the state, when saved as bytes</p> <pre><code>public UIntPtr GetStateSize()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_12","title":"Returns","text":"<p>UIntPtr</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getstatesizellamaseqid","title":"GetStateSize(LLamaSeqId)","text":"<p>Get the size of the KV cache for a single sequence ID, when saved as bytes</p> <pre><code>public UIntPtr GetStateSize(LLamaSeqId sequence)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_12","title":"Parameters","text":"<p><code>sequence</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_13","title":"Returns","text":"<p>UIntPtr</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getstatebyte-uintptr","title":"GetState(Byte*, UIntPtr)","text":"<p>Get the raw state of this context, encoded as bytes. Data is written into the <code>dest</code> pointer.</p> <pre><code>public UIntPtr GetState(Byte* dest, UIntPtr size)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_13","title":"Parameters","text":"<p><code>dest</code> Byte* Destination to write to</p> <p><code>size</code> UIntPtr Number of bytes available to write to in dest (check required size with <code>GetStateSize()</code>)</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_14","title":"Returns","text":"<p>UIntPtr The number of bytes written to dest</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#exceptions_3","title":"Exceptions","text":"<p>ArgumentOutOfRangeException Thrown if dest is too small</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#getstatebyte-uintptr-llamaseqid","title":"GetState(Byte*, UIntPtr, LLamaSeqId)","text":"<p>Get the raw state of a single sequence from this context, encoded as bytes. Data is written into the <code>dest</code> pointer.</p> <pre><code>public UIntPtr GetState(Byte* dest, UIntPtr size, LLamaSeqId sequence)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_14","title":"Parameters","text":"<p><code>dest</code> Byte* Destination to write to</p> <p><code>size</code> UIntPtr Number of bytes available to write to in dest (check required size with <code>GetStateSize()</code>)</p> <p><code>sequence</code> LLamaSeqId The sequence to get state data for</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_15","title":"Returns","text":"<p>UIntPtr The number of bytes written to dest</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#setstatebyte-uintptr","title":"SetState(Byte*, UIntPtr)","text":"<p>Set the raw state of this context</p> <pre><code>public UIntPtr SetState(Byte* src, UIntPtr size)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_15","title":"Parameters","text":"<p><code>src</code> Byte* The pointer to read the state from</p> <p><code>size</code> UIntPtr Number of bytes that can be safely read from the pointer</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_16","title":"Returns","text":"<p>UIntPtr Number of bytes read from the src pointer</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#setstatebyte-uintptr-llamaseqid","title":"SetState(Byte*, UIntPtr, LLamaSeqId)","text":"<p>Set the raw state of a single sequence</p> <pre><code>public UIntPtr SetState(Byte* src, UIntPtr size, LLamaSeqId sequence)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_16","title":"Parameters","text":"<p><code>src</code> Byte* The pointer to read the state from</p> <p><code>size</code> UIntPtr Number of bytes that can be safely read from the pointer</p> <p><code>sequence</code> LLamaSeqId Sequence ID to set</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_17","title":"Returns","text":"<p>UIntPtr Number of bytes read from the src pointer</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#gettimings","title":"GetTimings()","text":"<p>Get performance information</p> <pre><code>public LLamaPerfContextTimings GetTimings()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_18","title":"Returns","text":"<p>LLamaPerfContextTimings</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#resettimings","title":"ResetTimings()","text":"<p>Reset all performance information for this context</p> <pre><code>public void ResetTimings()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcacheupdate","title":"KvCacheUpdate()","text":"<p>Apply KV cache updates (such as K-shifts, defragmentation, etc.)</p> <pre><code>public void KvCacheUpdate()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachedefrag","title":"KvCacheDefrag()","text":"<p>Defragment the KV cache. This will be applied:  - lazily on next llama_decode()  - explicitly with llama_kv_self_update()</p> <pre><code>public void KvCacheDefrag()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachegetdebugviewint32","title":"KvCacheGetDebugView(Int32)","text":"<p>Get a new KV cache view that can be used to debug the KV cache</p> <pre><code>public LLamaKvCacheViewSafeHandle KvCacheGetDebugView(int maxSequences)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_17","title":"Parameters","text":"<p><code>maxSequences</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_19","title":"Returns","text":"<p>LLamaKvCacheViewSafeHandle</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachecountcells","title":"KvCacheCountCells()","text":"<p>Count the number of used cells in the KV cache (i.e. have at least one sequence assigned to them)</p> <pre><code>public int KvCacheCountCells()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_20","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachecounttokens","title":"KvCacheCountTokens()","text":"<p>Returns the number of tokens in the KV cache (slow, use only for debug)  If a KV cell has multiple sequences assigned to it, it will be counted multiple times</p> <pre><code>public int KvCacheCountTokens()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_21","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcacheclear","title":"KvCacheClear()","text":"<p>Clear the KV cache - both cell info is erased and KV data is zeroed</p> <pre><code>public void KvCacheClear()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcacheremovellamaseqid-llamapos-llamapos","title":"KvCacheRemove(LLamaSeqId, LLamaPos, LLamaPos)","text":"<p>Removes all tokens that belong to the specified sequence and have positions in [p0, p1)</p> <pre><code>public void KvCacheRemove(LLamaSeqId seq, LLamaPos p0, LLamaPos p1)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_18","title":"Parameters","text":"<p><code>seq</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachesequencecopyllamaseqid-llamaseqid-llamapos-llamapos","title":"KvCacheSequenceCopy(LLamaSeqId, LLamaSeqId, LLamaPos, LLamaPos)","text":"<p>Copy all tokens that belong to the specified sequence to another sequence. Note that  this does not allocate extra KV cache memory - it simply assigns the tokens to the  new sequence</p> <pre><code>public void KvCacheSequenceCopy(LLamaSeqId src, LLamaSeqId dest, LLamaPos p0, LLamaPos p1)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_19","title":"Parameters","text":"<p><code>src</code> LLamaSeqId</p> <p><code>dest</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachesequencekeepllamaseqid","title":"KvCacheSequenceKeep(LLamaSeqId)","text":"<p>Removes all tokens that do not belong to the specified sequence</p> <pre><code>public void KvCacheSequenceKeep(LLamaSeqId seq)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_20","title":"Parameters","text":"<p><code>seq</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachesequenceaddllamaseqid-llamapos-llamapos-int32","title":"KvCacheSequenceAdd(LLamaSeqId, LLamaPos, LLamaPos, Int32)","text":"<p>Adds relative position \"delta\" to all tokens that belong to the specified sequence  and have positions in [p0, p1. If the KV cache is RoPEd, the KV data is updated  accordingly</p> <pre><code>public void KvCacheSequenceAdd(LLamaSeqId seq, LLamaPos p0, LLamaPos p1, int delta)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_21","title":"Parameters","text":"<p><code>seq</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p> <p><code>delta</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachesequencedividellamaseqid-llamapos-llamapos-int32","title":"KvCacheSequenceDivide(LLamaSeqId, LLamaPos, LLamaPos, Int32)","text":"<p>Integer division of the positions by factor of <code>d &amp;gt; 1</code>.  If the KV cache is RoPEd, the KV data is updated accordingly.  p0 &lt; 0 : [0, p1]  p1 &lt; 0 : [p0, inf)</p> <pre><code>public void KvCacheSequenceDivide(LLamaSeqId seq, LLamaPos p0, LLamaPos p1, int divisor)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_22","title":"Parameters","text":"<p><code>seq</code> LLamaSeqId</p> <p><code>p0</code> LLamaPos</p> <p><code>p1</code> LLamaPos</p> <p><code>divisor</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#kvcachemaxpositionllamaseqid","title":"KvCacheMaxPosition(LLamaSeqId)","text":"<p>Returns the largest position present in the KV cache for the specified sequence</p> <pre><code>public LLamaPos KvCacheMaxPosition(LLamaSeqId seq)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#parameters_23","title":"Parameters","text":"<p><code>seq</code> LLamaSeqId</p>"},{"location":"xmldocs/llama.native.safellamacontexthandle/#returns_22","title":"Returns","text":"<p>LLamaPos</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellamahandlebase/","title":"llama.native.safellamahandlebase","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellamahandlebase/#safellamahandlebase","title":"SafeLLamaHandleBase","text":"<p>Namespace: LLama.Native</p> <p>Base class for all llama handles to native resources</p> <pre><code>public abstract class SafeLLamaHandleBase : System.Runtime.InteropServices.SafeHandle, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase Implements IDisposable</p>"},{"location":"xmldocs/llama.native.safellamahandlebase/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.safellamahandlebase/#handle","title":"handle","text":"<pre><code>protected IntPtr handle;\n</code></pre>"},{"location":"xmldocs/llama.native.safellamahandlebase/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellamahandlebase/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamahandlebase/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamahandlebase/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamahandlebase/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamahandlebase/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellamahandlebase/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamahandlebase/#returns","title":"Returns","text":"<p>String</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/","title":"llama.native.safellamamodelhandle","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#safellamamodelhandle","title":"SafeLlamaModelHandle","text":"<p>Namespace: LLama.Native</p> <p>A reference to a set of llama model weights</p> <pre><code>public sealed class SafeLlamaModelHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 SafeLlamaModelHandle Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.safellamamodelhandle/#handle","title":"handle","text":"<pre><code>protected IntPtr handle;\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellamamodelhandle/#ropetype","title":"RopeType","text":"<p>Get the rope (positional embedding) type for this model</p> <pre><code>public LLamaRopeType RopeType { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value","title":"Property Value","text":"<p>LLamaRopeType</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#contextsize","title":"ContextSize","text":"<p>The number of tokens in the context that this model was trained for</p> <pre><code>public int ContextSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#ropefrequency","title":"RopeFrequency","text":"<p>Get the rope frequency this model was trained with</p> <pre><code>public float RopeFrequency { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_2","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#embeddingsize","title":"EmbeddingSize","text":"<p>Dimension of embedding vectors</p> <pre><code>public int EmbeddingSize { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_3","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#sizeinbytes","title":"SizeInBytes","text":"<p>Get the size of this model in bytes</p> <pre><code>public ulong SizeInBytes { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_4","title":"Property Value","text":"<p>UInt64</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parametercount","title":"ParameterCount","text":"<p>Get the number of parameters in this model</p> <pre><code>public ulong ParameterCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_5","title":"Property Value","text":"<p>UInt64</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#layercount","title":"LayerCount","text":"<p>Get the number of layers in this model</p> <pre><code>public int LayerCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_6","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#headcount","title":"HeadCount","text":"<p>Get the number of heads in this model</p> <pre><code>public int HeadCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_7","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#kvheadcount","title":"KVHeadCount","text":"<p>Get the number of KV heads in this model</p> <pre><code>public int KVHeadCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_8","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#hasencoder","title":"HasEncoder","text":"<p>Returns true if the model contains an encoder that requires llama_encode() call</p> <pre><code>public bool HasEncoder { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_9","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#hasdecoder","title":"HasDecoder","text":"<p>Returns true if the model contains a decoder that requires llama_decode() call</p> <pre><code>public bool HasDecoder { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_10","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#isrecurrent","title":"IsRecurrent","text":"<p>Returns true if the model is recurrent (like Mamba, RWKV, etc.)</p> <pre><code>public bool IsRecurrent { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_11","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#description","title":"Description","text":"<p>Get a description of this model</p> <pre><code>public string Description { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_12","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#metadatacount","title":"MetadataCount","text":"<p>Get the number of metadata key/value pairs</p> <pre><code>public int MetadataCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_13","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#vocab","title":"Vocab","text":"<p>Get the vocabulary of this model</p> <pre><code>public Vocabulary Vocab { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_14","title":"Property Value","text":"<p>Vocabulary</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_15","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#property-value_16","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.safellamamodelhandle/#safellamamodelhandle_1","title":"SafeLlamaModelHandle()","text":"<pre><code>public SafeLlamaModelHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellamamodelhandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#loadfromfilestring-llamamodelparams","title":"LoadFromFile(String, LLamaModelParams)","text":"<p>Load a model from the given file path into memory</p> <pre><code>public static SafeLlamaModelHandle LoadFromFile(string modelPath, LLamaModelParams lparams)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters","title":"Parameters","text":"<p><code>modelPath</code> String</p> <p><code>lparams</code> LLamaModelParams</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_1","title":"Returns","text":"<p>SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#exceptions","title":"Exceptions","text":"<p>RuntimeError</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#loadlorafromfilestring","title":"LoadLoraFromFile(String)","text":"<p>Load a LoRA adapter from file. The adapter will be associated with this model but will not be applied</p> <pre><code>public LoraAdapter LoadLoraFromFile(string path)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_1","title":"Parameters","text":"<p><code>path</code> String</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_2","title":"Returns","text":"<p>LoraAdapter</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#exceptions_1","title":"Exceptions","text":"<p>InvalidOperationException</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#tokentospanllamatoken-spanbyte-int32-boolean","title":"TokenToSpan(LLamaToken, Span&lt;Byte&gt;, Int32, Boolean)","text":"<p>Convert a single llama token into bytes</p> <pre><code>public uint TokenToSpan(LLamaToken token, Span&lt;byte&gt; dest, int lstrip, bool special)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_2","title":"Parameters","text":"<p><code>token</code> LLamaToken Token to decode</p> <p><code>dest</code> Span&lt;Byte&gt; A span to attempt to write into. If this is too small nothing will be written</p> <p><code>lstrip</code> Int32 User can skip up to 'lstrip' leading spaces before copying (useful when encoding/decoding multiple tokens with 'add_space_prefix')</p> <p><code>special</code> Boolean If true, special characters will be converted to text. If false they will be invisible.</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_3","title":"Returns","text":"<p>UInt32 The size of this token. nothing will be written if this is larger than <code>dest</code></p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#tokenizestring-boolean-boolean-encoding","title":"Tokenize(String, Boolean, Boolean, Encoding)","text":"<p>Convert a string of text into tokens</p> <pre><code>public LLamaToken[] Tokenize(string text, bool addBos, bool special, Encoding encoding)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_3","title":"Parameters","text":"<p><code>text</code> String</p> <p><code>addBos</code> Boolean</p> <p><code>special</code> Boolean Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.</p> <p><code>encoding</code> Encoding</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_4","title":"Returns","text":"<p>LLamaToken[]</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#createcontextllamacontextparams","title":"CreateContext(LLamaContextParams)","text":"<p>Create a new context for this model</p> <pre><code>public SafeLLamaContextHandle CreateContext(LLamaContextParams params)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_4","title":"Parameters","text":"<p><code>params</code> LLamaContextParams</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_5","title":"Returns","text":"<p>SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#metadatavaluebykeystring","title":"MetadataValueByKey(String)","text":"<p>Get the metadata value for the given key</p> <pre><code>public Nullable&lt;Memory&lt;byte&gt;&gt; MetadataValueByKey(string key)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_5","title":"Parameters","text":"<p><code>key</code> String The key to fetch</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_6","title":"Returns","text":"<p>Nullable&lt;Memory&lt;Byte&gt;&gt; The value, null if there is no such key</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#metadatakeybyindexint32","title":"MetadataKeyByIndex(Int32)","text":"<p>Get the metadata key for the given index</p> <pre><code>public Nullable&lt;Memory&lt;byte&gt;&gt; MetadataKeyByIndex(int index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_6","title":"Parameters","text":"<p><code>index</code> Int32 The index to get</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_7","title":"Returns","text":"<p>Nullable&lt;Memory&lt;Byte&gt;&gt; The key, null if there is no such key or if the buffer was too small</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#metadatavaluebyindexint32","title":"MetadataValueByIndex(Int32)","text":"<p>Get the metadata value for the given index</p> <pre><code>public Nullable&lt;Memory&lt;byte&gt;&gt; MetadataValueByIndex(int index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_7","title":"Parameters","text":"<p><code>index</code> Int32 The index to get</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_8","title":"Returns","text":"<p>Nullable&lt;Memory&lt;Byte&gt;&gt; The value, null if there is no such value or if the buffer was too small</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#gettemplatestring-boolean","title":"GetTemplate(String, Boolean)","text":"<p>Get the default chat template. Returns nullptr if not available  If name is NULL, returns the default chat template</p> <pre><code>public string GetTemplate(string name, bool strict)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#parameters_8","title":"Parameters","text":"<p><code>name</code> String The name of the template, in case there are many or differently named. Set to 'null' for the default behaviour of finding an appropriate match.</p> <p><code>strict</code> Boolean Setting this to true will cause the call to throw if no valid templates are found.</p>"},{"location":"xmldocs/llama.native.safellamamodelhandle/#returns_9","title":"Returns","text":"<p>String</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/","title":"Llama.native.safellamasamplerchainhandle","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#safellamasamplerchainhandle","title":"SafeLLamaSamplerChainHandle","text":"<p>Namespace: LLama.Native</p> <p>A chain of sampler stages that can be used to select tokens from logits.</p> <pre><code>public sealed class SafeLLamaSamplerChainHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 SafeLLamaSamplerChainHandle Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p> <p>Remarks:</p> <p>Wraps a handle returned from <code>llama_sampler_chain_init</code>. Other samplers are owned by this chain and are never directly exposed.</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#handle","title":"handle","text":"<pre><code>protected IntPtr handle;\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#count","title":"Count","text":"<p>Get the number of samplers in this chain</p> <pre><code>public int Count { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#property-value_2","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#safellamasamplerchainhandle_1","title":"SafeLLamaSamplerChainHandle()","text":"<pre><code>public SafeLLamaSamplerChainHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#applyllamatokendataarraynative","title":"Apply(LLamaTokenDataArrayNative&amp;)","text":"<p>Apply this sampler to a set of candidates</p> <pre><code>public void Apply(LLamaTokenDataArrayNative&amp; candidates)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters","title":"Parameters","text":"<p><code>candidates</code> LLamaTokenDataArrayNative&amp;</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#samplesafellamacontexthandle-int32","title":"Sample(SafeLLamaContextHandle, Int32)","text":"<p>Sample and accept a token from the idx-th output of the last evaluation. Shorthand for:</p> <pre><code>var logits = ctx.GetLogitsIth(idx);\nvar token_data_array = LLamaTokenDataArray.Create(logits);\nusing LLamaTokenDataArrayNative.Create(token_data_array, out var native_token_data);\nsampler_chain.Apply(native_token_data);\nvar token = native_token_data.Data.Span[native_token_data.Selected];\nsampler_chain.Accept(token);\nreturn token;\n</code></pre> <pre><code>public LLamaToken Sample(SafeLLamaContextHandle context, int index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_1","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p> <p><code>index</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#returns_1","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#reset","title":"Reset()","text":"<p>Reset the state of this sampler</p> <pre><code>public void Reset()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#acceptllamatoken","title":"Accept(LLamaToken)","text":"<p>Accept a token and update the internal state of this sampler</p> <pre><code>public void Accept(LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_2","title":"Parameters","text":"<p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#getnameint32","title":"GetName(Int32)","text":"<p>Get the name of the sampler at the given index</p> <pre><code>public string GetName(int index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_3","title":"Parameters","text":"<p><code>index</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#returns_2","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#getseedint32","title":"GetSeed(Int32)","text":"<p>Get the seed of the sampler at the given index if applicable. returns LLAMA_DEFAULT_SEED otherwise</p> <pre><code>public uint GetSeed(int index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_4","title":"Parameters","text":"<p><code>index</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#returns_3","title":"Returns","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#createllamasamplerchainparams","title":"Create(LLamaSamplerChainParams)","text":"<p>Create a new sampler chain</p> <pre><code>public static SafeLLamaSamplerChainHandle Create(LLamaSamplerChainParams params)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_5","title":"Parameters","text":"<p><code>params</code> LLamaSamplerChainParams</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#returns_4","title":"Returns","text":"<p>SafeLLamaSamplerChainHandle</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addclonesafellamasamplerchainhandle-int32","title":"AddClone(SafeLLamaSamplerChainHandle, Int32)","text":"<p>Clone a sampler stage from another chain and add it to this chain</p> <pre><code>public void AddClone(SafeLLamaSamplerChainHandle src, int index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_6","title":"Parameters","text":"<p><code>src</code> SafeLLamaSamplerChainHandle The chain to clone a stage from</p> <p><code>index</code> Int32 The index of the stage to clone</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#removeint32","title":"Remove(Int32)","text":"<p>Remove a sampler stage from this chain</p> <pre><code>public void Remove(int index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_7","title":"Parameters","text":"<p><code>index</code> Int32</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#exceptions","title":"Exceptions","text":"<p>ArgumentOutOfRangeException</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addcustomtsamplertsampler","title":"AddCustom&lt;TSampler&gt;(TSampler)","text":"<p>Add a custom sampler stage</p> <pre><code>public void AddCustom&lt;TSampler&gt;(TSampler sampler)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#type-parameters","title":"Type Parameters","text":"<p><code>TSampler</code></p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_8","title":"Parameters","text":"<p><code>sampler</code> TSampler</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addgreedysampler","title":"AddGreedySampler()","text":"<p>Add a sampler which picks the most likely token.</p> <pre><code>public void AddGreedySampler()\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#adddistributionsampleruint32","title":"AddDistributionSampler(UInt32)","text":"<p>Add a sampler which picks from the probability distribution of all tokens</p> <pre><code>public void AddDistributionSampler(uint seed)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_9","title":"Parameters","text":"<p><code>seed</code> UInt32</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addmirostat1samplerint32-uint32-single-single-int32","title":"AddMirostat1Sampler(Int32, UInt32, Single, Single, Int32)","text":"<p>Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.</p> <pre><code>public void AddMirostat1Sampler(int vocabCount, uint seed, float tau, float eta, int m)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_10","title":"Parameters","text":"<p><code>vocabCount</code> Int32</p> <p><code>seed</code> UInt32</p> <p><code>tau</code> Single The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</p> <p><code>eta</code> Single The learning rate used to update <code>mu</code> based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause <code>mu</code> to be updated more quickly, while a smaller learning rate will result in slower updates.</p> <p><code>m</code> Int32 The number of tokens considered in the estimation of <code>s_hat</code>. This is an arbitrary value that is used to calculate <code>s_hat</code>, which in turn helps to calculate the value of <code>k</code>. In the paper, they use <code>m = 100</code>, but you can experiment with different values to see how it affects the performance of the algorithm.</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addmirostat2sampleruint32-single-single","title":"AddMirostat2Sampler(UInt32, Single, Single)","text":"<p>Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.</p> <pre><code>public void AddMirostat2Sampler(uint seed, float tau, float eta)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_11","title":"Parameters","text":"<p><code>seed</code> UInt32</p> <p><code>tau</code> Single The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</p> <p><code>eta</code> Single The learning rate used to update <code>mu</code> based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause <code>mu</code> to be updated more quickly, while a smaller learning rate will result in slower updates.</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addtopkint32","title":"AddTopK(Int32)","text":"<p>Top-K sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751</p> <pre><code>public void AddTopK(int k)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_12","title":"Parameters","text":"<p><code>k</code> Int32</p> <p>Remarks:</p> <p>Setting k &lt;= 0 makes this a noop</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addtopnsigmasingle","title":"AddTopNSigma(Single)","text":"<p>Top n sigma sampling as described in academic paper \"Top-n\u03c3: Not All Logits Are You Need\" https://arxiv.org/pdf/2411.07641</p> <pre><code>public void AddTopNSigma(float n)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_13","title":"Parameters","text":"<p><code>n</code> Single</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addtoppsingle-intptr","title":"AddTopP(Single, IntPtr)","text":"<p>Nucleus sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751</p> <pre><code>public void AddTopP(float p, IntPtr minKeep)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_14","title":"Parameters","text":"<p><code>p</code> Single</p> <p><code>minKeep</code> IntPtr</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addminpsingle-intptr","title":"AddMinP(Single, IntPtr)","text":"<p>Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841</p> <pre><code>public void AddMinP(float p, IntPtr minKeep)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_15","title":"Parameters","text":"<p><code>p</code> Single</p> <p><code>minKeep</code> IntPtr</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addtypicalsingle-intptr","title":"AddTypical(Single, IntPtr)","text":"<p>Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.</p> <pre><code>public void AddTypical(float p, IntPtr minKeep)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_16","title":"Parameters","text":"<p><code>p</code> Single</p> <p><code>minKeep</code> IntPtr</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addtemperaturesingle","title":"AddTemperature(Single)","text":"<p>Apply temperature to the logits.  If temperature is less than zero the maximum logit is left unchanged and the rest are set to -infinity</p> <pre><code>public void AddTemperature(float t)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_17","title":"Parameters","text":"<p><code>t</code> Single</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#adddynamictemperaturesingle-single-single","title":"AddDynamicTemperature(Single, Single, Single)","text":"<p>Dynamic temperature implementation (a.k.a. entropy) described in the paper https://arxiv.org/abs/2309.02772.</p> <pre><code>public void AddDynamicTemperature(float t, float delta, float exponent)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_18","title":"Parameters","text":"<p><code>t</code> Single</p> <p><code>delta</code> Single</p> <p><code>exponent</code> Single</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addxtcsingle-single-int32-uint32","title":"AddXTC(Single, Single, Int32, UInt32)","text":"<p>XTC sampler as described in https://github.com/oobabooga/text-generation-webui/pull/6335</p> <pre><code>public void AddXTC(float p, float t, int minKeep, uint seed)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_19","title":"Parameters","text":"<p><code>p</code> Single</p> <p><code>t</code> Single</p> <p><code>minKeep</code> Int32</p> <p><code>seed</code> UInt32</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addfillinmiddleinfillsafellamamodelhandle","title":"AddFillInMiddleInfill(SafeLlamaModelHandle)","text":"<p>This sampler is meant to be used for fill-in-the-middle infilling, after top_k + top_p sampling    1. if the sum of the EOG probs times the number of candidates is higher than the sum of the other probs -&gt; pick EOG  2. combine probs of tokens that have the same prefix  example:  - before:  \"abc\": 0.5  \"abcd\": 0.2  \"abcde\": 0.1  \"dummy\": 0.1  - after:  \"abc\": 0.8  \"dummy\": 0.1  3. discard non-EOG tokens with low prob  4. if no tokens are left -&gt; pick EOT</p> <pre><code>public void AddFillInMiddleInfill(SafeLlamaModelHandle model)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_20","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addgrammarsafellamamodelhandle-string-string","title":"AddGrammar(SafeLlamaModelHandle, String, String)","text":"<p>Create a sampler which makes tokens impossible unless they match the grammar.</p> <pre><code>public void AddGrammar(SafeLlamaModelHandle model, string grammar, string root)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_21","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle The model that this grammar will be used with</p> <p><code>grammar</code> String</p> <p><code>root</code> String Root rule of the grammar</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addgrammarvocabulary-string-string","title":"AddGrammar(Vocabulary, String, String)","text":"<p>Create a sampler which makes tokens impossible unless they match the grammar.</p> <pre><code>public void AddGrammar(Vocabulary vocab, string grammar, string root)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_22","title":"Parameters","text":"<p><code>vocab</code> Vocabulary The vocabulary that this grammar will be used with</p> <p><code>grammar</code> String</p> <p><code>root</code> String Root rule of the grammar</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addlazygrammarsafellamamodelhandle-string-string-readonlyspanstring-readonlyspanllamatoken","title":"AddLazyGrammar(SafeLlamaModelHandle, String, String, ReadOnlySpan&lt;String&gt;, ReadOnlySpan&lt;LLamaToken&gt;)","text":"<p>Create a sampler using lazy grammar sampling: https://github.com/ggerganov/llama.cpp/pull/9639</p> <pre><code>public void AddLazyGrammar(SafeLlamaModelHandle model, string grammar, string root, ReadOnlySpan&lt;string&gt; patterns, ReadOnlySpan&lt;LLamaToken&gt; triggerTokens)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_23","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle</p> <p><code>grammar</code> String Grammar in GBNF form</p> <p><code>root</code> String Root rule of the grammar</p> <p><code>patterns</code> ReadOnlySpan&lt;String&gt; A list of patterns that will trigger the grammar sampler. Pattern will be matched from the start of the generation output, and grammar sampler will be fed content starting from its first match group.</p> <p><code>triggerTokens</code> ReadOnlySpan&lt;LLamaToken&gt; A list of tokens that will trigger the grammar sampler. Grammar sampler will be fed content starting from the trigger token included..</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addpenaltiesint32-single-single-single","title":"AddPenalties(Int32, Single, Single, Single)","text":"<p>Create a sampler that applies various repetition penalties.</p> <p>Avoid using on the full vocabulary as searching for repeated tokens can become slow. For example, apply top-k or top-p sampling first.</p> <pre><code>public void AddPenalties(int penaltyCount, float repeat, float freq, float presence)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_24","title":"Parameters","text":"<p><code>penaltyCount</code> Int32 How many tokens of history to consider when calculating penalties</p> <p><code>repeat</code> Single Repetition penalty</p> <p><code>freq</code> Single Frequency penalty</p> <p><code>presence</code> Single Presence penalty</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#adddrysafellamamodelhandle-readonlyspanstring-single-single-int32-int32","title":"AddDry(SafeLlamaModelHandle, ReadOnlySpan&lt;String&gt;, Single, Single, Int32, Int32)","text":"<p>DRY sampler, designed by p-e-w, as described in: https://github.com/oobabooga/text-generation-webui/pull/5677.  Porting Koboldcpp implementation authored by pi6am: https://github.com/LostRuins/koboldcpp/pull/982</p> <pre><code>public void AddDry(SafeLlamaModelHandle model, ReadOnlySpan&lt;string&gt; sequenceBreakers, float multiplier, float base, int allowedLength, int penaltyLastN)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_25","title":"Parameters","text":"<p><code>model</code> SafeLlamaModelHandle The model this sampler will be used with</p> <p><code>sequenceBreakers</code> ReadOnlySpan&lt;String&gt;</p> <p><code>multiplier</code> Single penalty multiplier, 0.0 = disabled</p> <p><code>base</code> Single exponential base</p> <p><code>allowedLength</code> Int32 repeated sequences longer than this are penalized</p> <p><code>penaltyLastN</code> Int32 how many tokens to scan for repetitions (0 = entire context)</p>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#addlogitbiasint32-spanllamalogitbias","title":"AddLogitBias(Int32, Span&lt;LLamaLogitBias&gt;)","text":"<p>Create a sampler that applies a bias directly to the logits</p> <pre><code>public void AddLogitBias(int vocabSize, Span&lt;LLamaLogitBias&gt; biases)\n</code></pre>"},{"location":"xmldocs/llama.native.safellamasamplerchainhandle/#parameters_26","title":"Parameters","text":"<p><code>vocabSize</code> Int32</p> <p><code>biases</code> Span&lt;LLamaLogitBias&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/","title":"llama.native.safellavaimageembedhandle","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#safellavaimageembedhandle","title":"SafeLlavaImageEmbedHandle","text":"<p>Namespace: LLama.Native</p> <p>A Reference to a llava Image Embed handle</p> <pre><code>public sealed class SafeLlavaImageEmbedHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 SafeLlavaImageEmbedHandle Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#handle","title":"handle","text":"<pre><code>protected IntPtr handle;\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#model","title":"Model","text":"<p>Get the model used to create this image embedding</p> <pre><code>public SafeLlavaModelHandle Model { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#property-value","title":"Property Value","text":"<p>SafeLlavaModelHandle</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#embeddingdimensions","title":"EmbeddingDimensions","text":"<p>Get the number of dimensions in an embedding</p> <pre><code>public int EmbeddingDimensions { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#patchcount","title":"PatchCount","text":"<p>Get the number of \"patches\" in an image embedding</p> <pre><code>public int PatchCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#property-value_4","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#safellavaimageembedhandle_1","title":"SafeLlavaImageEmbedHandle()","text":"<pre><code>public SafeLlavaImageEmbedHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#createfromfilenamesafellavamodelhandle-llamacontext-string","title":"CreateFromFileName(SafeLlavaModelHandle, LLamaContext, String)","text":"<p>Create an image embed from an image file</p> <pre><code>public static SafeLlavaImageEmbedHandle CreateFromFileName(SafeLlavaModelHandle clip, LLamaContext ctx, string image)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#parameters","title":"Parameters","text":"<p><code>clip</code> SafeLlavaModelHandle</p> <p><code>ctx</code> LLamaContext</p> <p><code>image</code> String Path to the image file. Supported formats:</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#-","title":"-","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#-_1","title":"-","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#returns","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#exceptions","title":"Exceptions","text":"<p>InvalidOperationException</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#createfromfilenamesafellavamodelhandle-string-int32","title":"CreateFromFileName(SafeLlavaModelHandle, String, Int32)","text":"<p>Create an image embed from an image file</p> <pre><code>public static SafeLlavaImageEmbedHandle CreateFromFileName(SafeLlavaModelHandle clip, string image, int threads)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#parameters_1","title":"Parameters","text":"<p><code>clip</code> SafeLlavaModelHandle</p> <p><code>image</code> String Path to the image file. Supported formats:</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#-_2","title":"-","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#-_3","title":"-","text":"<p><code>threads</code> Int32</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#returns_1","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#exceptions_1","title":"Exceptions","text":"<p>InvalidOperationException</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#createfrommemorysafellavamodelhandle-llamacontext-byte","title":"CreateFromMemory(SafeLlavaModelHandle, LLamaContext, Byte[])","text":"<p>Create an image embed from the bytes of an image.</p> <pre><code>public static SafeLlavaImageEmbedHandle CreateFromMemory(SafeLlavaModelHandle clip, LLamaContext ctx, Byte[] image)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#parameters_2","title":"Parameters","text":"<p><code>clip</code> SafeLlavaModelHandle</p> <p><code>ctx</code> LLamaContext</p> <p><code>image</code> Byte[] Image bytes. Supported formats:</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#-_4","title":"-","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#-_5","title":"-","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#returns_2","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#createfrommemorysafellavamodelhandle-byte-int32","title":"CreateFromMemory(SafeLlavaModelHandle, Byte[], Int32)","text":"<p>Create an image embed from the bytes of an image.</p> <pre><code>public static SafeLlavaImageEmbedHandle CreateFromMemory(SafeLlavaModelHandle clip, Byte[] image, int threads)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#parameters_3","title":"Parameters","text":"<p><code>clip</code> SafeLlavaModelHandle</p> <p><code>image</code> Byte[] Image bytes. Supported formats:</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#-_6","title":"-","text":""},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#-_7","title":"-","text":"<p><code>threads</code> Int32</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#returns_3","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#getembeddingspansingle-int32","title":"GetEmbedding(Span&lt;Single&gt;, Int32)","text":"<p>Copy the embeddings data to the destination span</p> <pre><code>public void GetEmbedding(Span&lt;float&gt; dest, int index)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavaimageembedhandle/#parameters_4","title":"Parameters","text":"<p><code>dest</code> Span&lt;Single&gt;</p> <p><code>index</code> Int32</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/","title":"llama.native.safellavamodelhandle","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#safellavamodelhandle","title":"SafeLlavaModelHandle","text":"<p>Namespace: LLama.Native</p> <p>A reference to a set of llava model weights.</p> <pre><code>public sealed class SafeLlavaModelHandle : SafeLLamaHandleBase, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 CriticalFinalizerObject \u2192 SafeHandle \u2192 SafeLLamaHandleBase \u2192 SafeLlavaModelHandle Implements IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.native.safellavamodelhandle/#handle","title":"handle","text":"<pre><code>protected IntPtr handle;\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.safellavamodelhandle/#embeddingdimensions","title":"EmbeddingDimensions","text":"<p>Get the number of dimensions in an embedding</p> <pre><code>public int EmbeddingDimensions { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#patchcount","title":"PatchCount","text":"<p>Get the number of \"patches\" in an image embedding</p> <pre><code>public int PatchCount { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#property-value_1","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#isinvalid","title":"IsInvalid","text":"<pre><code>public bool IsInvalid { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#property-value_2","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#isclosed","title":"IsClosed","text":"<pre><code>public bool IsClosed { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#property-value_3","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.safellavamodelhandle/#safellavamodelhandle_1","title":"SafeLlavaModelHandle()","text":"<pre><code>public SafeLlavaModelHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.safellavamodelhandle/#releasehandle","title":"ReleaseHandle()","text":"<pre><code>protected bool ReleaseHandle()\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#loadfromfilestring-int32","title":"LoadFromFile(String, Int32)","text":"<p>Load a model from the given file path into memory</p> <pre><code>public static SafeLlavaModelHandle LoadFromFile(string modelPath, int verbosity)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#parameters","title":"Parameters","text":"<p><code>modelPath</code> String MMP File (Multi-Modal Projections)</p> <p><code>verbosity</code> Int32 Verbosity level</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns_1","title":"Returns","text":"<p>SafeLlavaModelHandle SafeHandle of the Clip Model</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#exceptions","title":"Exceptions","text":"<p>InvalidOperationException</p> <p>LoadWeightsFailedException</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#createimageembeddingsllamacontext-string","title":"CreateImageEmbeddings(LLamaContext, String)","text":"<p>Create the Image Embeddings.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(LLamaContext ctxLlama, string image)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#parameters_1","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext LLama Context</p> <p><code>image</code> String Image filename (it supports jpeg format only)</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns_2","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle return the SafeHandle of these embeddings</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#createimageembeddingsstring-int32","title":"CreateImageEmbeddings(String, Int32)","text":"<p>Create the Image Embeddings.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(string image, int threads)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#parameters_2","title":"Parameters","text":"<p><code>image</code> String Image in binary format (it supports jpeg format only)</p> <p><code>threads</code> Int32 Number of threads to use</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns_3","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle return the SafeHandle of these embeddings</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#createimageembeddingsllamacontext-byte","title":"CreateImageEmbeddings(LLamaContext, Byte[])","text":"<p>Create the Image Embeddings.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(LLamaContext ctxLlama, Byte[] image)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#parameters_3","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext LLama Context</p> <p><code>image</code> Byte[] Image in binary format (it supports jpeg format only)</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns_4","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle return the SafeHandle of these embeddings</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#createimageembeddingsbyte-int32","title":"CreateImageEmbeddings(Byte[], Int32)","text":"<p>Create the Image Embeddings.</p> <pre><code>public SafeLlavaImageEmbedHandle CreateImageEmbeddings(Byte[] image, int threads)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#parameters_4","title":"Parameters","text":"<p><code>image</code> Byte[] Image in binary format (it supports jpeg format only)</p> <p><code>threads</code> Int32 Number of threads to use</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns_5","title":"Returns","text":"<p>SafeLlavaImageEmbedHandle return the SafeHandle of these embeddings</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#evalimageembedllamacontext-safellavaimageembedhandle-int32","title":"EvalImageEmbed(LLamaContext, SafeLlavaImageEmbedHandle, Int32&amp;)","text":"<p>Evaluates the image embeddings.</p> <pre><code>public bool EvalImageEmbed(LLamaContext ctxLlama, SafeLlavaImageEmbedHandle imageEmbed, Int32&amp; n_past)\n</code></pre>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#parameters_5","title":"Parameters","text":"<p><code>ctxLlama</code> LLamaContext Llama Context</p> <p><code>imageEmbed</code> SafeLlavaImageEmbedHandle The current embeddings to evaluate</p> <p><code>n_past</code> Int32&amp;</p>"},{"location":"xmldocs/llama.native.safellavamodelhandle/#returns_6","title":"Returns","text":"<p>Boolean True on success</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.systeminfo/","title":"Llama.native.systeminfo","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.systeminfo/#systeminfo","title":"SystemInfo","text":"<p>Namespace: LLama.Native</p> <p>Operating system information.</p> <pre><code>public class SystemInfo : System.IEquatable`1[[LLama.Native.SystemInfo, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 SystemInfo Implements IEquatable&lt;SystemInfo&gt; Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.native.systeminfo/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.systeminfo/#equalitycontract","title":"EqualityContract","text":"<pre><code>protected Type EqualityContract { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#property-value","title":"Property Value","text":"<p>Type</p>"},{"location":"xmldocs/llama.native.systeminfo/#osplatform","title":"OSPlatform","text":"<pre><code>public OSPlatform OSPlatform { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#property-value_1","title":"Property Value","text":"<p>OSPlatform</p>"},{"location":"xmldocs/llama.native.systeminfo/#cudamajorversion","title":"CudaMajorVersion","text":"<pre><code>public int CudaMajorVersion { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#property-value_2","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.systeminfo/#vulkanversion","title":"VulkanVersion","text":"<pre><code>public string VulkanVersion { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#property-value_3","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.native.systeminfo/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.systeminfo/#systeminfoosplatform-int32-string","title":"SystemInfo(OSPlatform, Int32, String)","text":"<p>Operating system information.</p> <pre><code>public SystemInfo(OSPlatform OSPlatform, int CudaMajorVersion, string VulkanVersion)\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#parameters","title":"Parameters","text":"<p><code>OSPlatform</code> OSPlatform</p> <p><code>CudaMajorVersion</code> Int32</p> <p><code>VulkanVersion</code> String</p>"},{"location":"xmldocs/llama.native.systeminfo/#systeminfosysteminfo","title":"SystemInfo(SystemInfo)","text":"<pre><code>protected SystemInfo(SystemInfo original)\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#parameters_1","title":"Parameters","text":"<p><code>original</code> SystemInfo</p>"},{"location":"xmldocs/llama.native.systeminfo/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.systeminfo/#get","title":"Get()","text":"<p>Get the system information of the current machine.</p> <pre><code>public static SystemInfo Get()\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#returns","title":"Returns","text":"<p>SystemInfo</p>"},{"location":"xmldocs/llama.native.systeminfo/#exceptions","title":"Exceptions","text":"<p>PlatformNotSupportedException</p>"},{"location":"xmldocs/llama.native.systeminfo/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#returns_1","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.native.systeminfo/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#parameters_2","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.native.systeminfo/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.systeminfo/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#returns_3","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.native.systeminfo/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#parameters_3","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.native.systeminfo/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.systeminfo/#equalssysteminfo","title":"Equals(SystemInfo)","text":"<pre><code>public bool Equals(SystemInfo other)\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#parameters_4","title":"Parameters","text":"<p><code>other</code> SystemInfo</p>"},{"location":"xmldocs/llama.native.systeminfo/#returns_5","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.native.systeminfo/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public SystemInfo &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#returns_6","title":"Returns","text":"<p>SystemInfo</p>"},{"location":"xmldocs/llama.native.systeminfo/#deconstructosplatform-int32-string","title":"Deconstruct(OSPlatform&amp;, Int32&amp;, String&amp;)","text":"<pre><code>public void Deconstruct(OSPlatform&amp; OSPlatform, Int32&amp; CudaMajorVersion, String&amp; VulkanVersion)\n</code></pre>"},{"location":"xmldocs/llama.native.systeminfo/#parameters_5","title":"Parameters","text":"<p><code>OSPlatform</code> OSPlatform&amp;</p> <p><code>CudaMajorVersion</code> Int32&amp;</p> <p><code>VulkanVersion</code> String&amp;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.unknownnativelibrary/","title":"Llama.native.unknownnativelibrary","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.native.unknownnativelibrary/#unknownnativelibrary","title":"UnknownNativeLibrary","text":"<p>Namespace: LLama.Native</p> <p>When you are using .NET standard2.0, dynamic native library loading is not supported.  This class will be returned in NativeLibraryConfig.DryRun(INativeLibrary&amp;).</p> <pre><code>public class UnknownNativeLibrary : LLama.Abstractions.INativeLibrary\n</code></pre> <p>Inheritance Object \u2192 UnknownNativeLibrary Implements INativeLibrary</p>"},{"location":"xmldocs/llama.native.unknownnativelibrary/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.native.unknownnativelibrary/#metadata","title":"Metadata","text":"<pre><code>public NativeLibraryMetadata Metadata { get; }\n</code></pre>"},{"location":"xmldocs/llama.native.unknownnativelibrary/#property-value","title":"Property Value","text":"<p>NativeLibraryMetadata</p>"},{"location":"xmldocs/llama.native.unknownnativelibrary/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.native.unknownnativelibrary/#unknownnativelibrary_1","title":"UnknownNativeLibrary()","text":"<pre><code>public UnknownNativeLibrary()\n</code></pre>"},{"location":"xmldocs/llama.native.unknownnativelibrary/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.native.unknownnativelibrary/#preparesysteminfo-llamalogcallback","title":"Prepare(SystemInfo, LLamaLogCallback)","text":"<pre><code>public IEnumerable&lt;string&gt; Prepare(SystemInfo systemInfo, LLamaLogCallback logCallback)\n</code></pre>"},{"location":"xmldocs/llama.native.unknownnativelibrary/#parameters","title":"Parameters","text":"<p><code>systemInfo</code> SystemInfo</p> <p><code>logCallback</code> LLamaLogCallback</p>"},{"location":"xmldocs/llama.native.unknownnativelibrary/#returns","title":"Returns","text":"<p>IEnumerable&lt;String&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/","title":"llama.sampling.basesamplingpipeline","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#basesamplingpipeline","title":"BaseSamplingPipeline","text":"<p>Namespace: LLama.Sampling</p> <pre><code>public abstract class BaseSamplingPipeline : ISamplingPipeline, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 BaseSamplingPipeline Implements ISamplingPipeline, IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#basesamplingpipeline_1","title":"BaseSamplingPipeline()","text":"<p>Create a new sampler wrapping a llama.cpp sampler chain</p> <pre><code>public BaseSamplingPipeline()\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#createchainsafellamacontexthandle","title":"CreateChain(SafeLLamaContextHandle)","text":"<p>Create a sampling chain. This will be called once, the base class will automatically dispose the chain.</p> <pre><code>protected abstract SafeLLamaSamplerChainHandle CreateChain(SafeLLamaContextHandle context)\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#parameters","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#returns","title":"Returns","text":"<p>SafeLLamaSamplerChainHandle</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#dispose","title":"Dispose()","text":"<pre><code>public void Dispose()\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#samplesafellamacontexthandle-int32","title":"Sample(SafeLLamaContextHandle, Int32)","text":"<pre><code>public LLamaToken Sample(SafeLLamaContextHandle ctx, int index)\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#parameters_1","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>index</code> Int32</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#returns_1","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#applysafellamacontexthandle-llamatokendataarray","title":"Apply(SafeLLamaContextHandle, LLamaTokenDataArray)","text":"<pre><code>public void Apply(SafeLLamaContextHandle ctx, LLamaTokenDataArray data)\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#parameters_2","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>data</code> LLamaTokenDataArray</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#applysafellamacontexthandle-llamatokendataarraynative","title":"Apply(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)","text":"<p>Apply this sampling chain to a LLamaTokenDataArrayNative</p> <pre><code>public void Apply(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; data)\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#parameters_3","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>data</code> LLamaTokenDataArrayNative&amp;</p>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#reset","title":"Reset()","text":"<pre><code>public void Reset()\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#acceptllamatoken","title":"Accept(LLamaToken)","text":"<pre><code>public void Accept(LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.sampling.basesamplingpipeline/#parameters_4","title":"Parameters","text":"<p><code>token</code> LLamaToken</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/","title":"llama.sampling.defaultsamplingpipeline","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#defaultsamplingpipeline","title":"DefaultSamplingPipeline","text":"<p>Namespace: LLama.Sampling</p> <p>An implementation of ISamplePipeline which mimics the default llama.cpp sampling</p> <pre><code>public class DefaultSamplingPipeline : BaseSamplingPipeline, ISamplingPipeline, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 BaseSamplingPipeline \u2192 DefaultSamplingPipeline Implements ISamplingPipeline, IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#logitbias","title":"LogitBias","text":"<p>Bias values to add to certain logits</p> <pre><code>public IReadOnlyDictionary&lt;LLamaToken, float&gt; LogitBias { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value","title":"Property Value","text":"<p>IReadOnlyDictionary&lt;LLamaToken, Single&gt;</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#repeatpenalty","title":"RepeatPenalty","text":"<p>Repetition penalty, as described in https://arxiv.org/abs/1909.05858</p> <pre><code>public float RepeatPenalty { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_1","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#frequencypenalty","title":"FrequencyPenalty","text":"<p>Frequency penalty as described by OpenAI: https://platform.openai.com/docs/api-reference/chat/create  Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text  so far, decreasing the model's likelihood to repeat the same line verbatim.</p> <pre><code>public float FrequencyPenalty { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_2","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#presencepenalty","title":"PresencePenalty","text":"<p>Presence penalty as described by OpenAI: https://platform.openai.com/docs/api-reference/chat/create  Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the  text so far, increasing the model's likelihood to talk about new topics.</p> <pre><code>public float PresencePenalty { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_3","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#penaltycount","title":"PenaltyCount","text":"<p>How many tokens should be considered for penalties</p> <pre><code>public int PenaltyCount { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_4","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#penalizenewline","title":"PenalizeNewline","text":"<p>Whether the newline token should be protected from being modified by penalty</p> <pre><code>public bool PenalizeNewline { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_5","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#preventeos","title":"PreventEOS","text":"<p>Whether the EOS token should be suppressed. Setting this to 'true' prevents EOS from being sampled</p> <pre><code>public bool PreventEOS { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_6","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#temperature","title":"Temperature","text":"<p>Temperature to apply (higher temperature is more \"creative\")</p> <pre><code>public float Temperature { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_7","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#topk","title":"TopK","text":"<p>Number of tokens to keep in TopK sampling</p> <pre><code>public int TopK { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_8","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#typicalp","title":"TypicalP","text":"<p>P value for locally typical sampling</p> <pre><code>public float TypicalP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_9","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#topp","title":"TopP","text":"<p>P value for TopP sampling</p> <pre><code>public float TopP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_10","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#minp","title":"MinP","text":"<p>P value for MinP sampling</p> <pre><code>public float MinP { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_11","title":"Property Value","text":"<p>Single</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#grammar","title":"Grammar","text":"<p>Grammar to apply to constrain possible tokens</p> <pre><code>public Grammar Grammar { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_12","title":"Property Value","text":"<p>Grammar</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#minkeep","title":"MinKeep","text":"<p>The minimum number of tokens to keep for samplers which remove tokens</p> <pre><code>public int MinKeep { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_13","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#seed","title":"Seed","text":"<p>Seed to use for random sampling</p> <pre><code>public uint Seed { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_14","title":"Property Value","text":"<p>UInt32</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#grammaroptimization","title":"GrammarOptimization","text":"<p>Selected grammar optimization mode</p> <pre><code>public GrammarOptimizationMode GrammarOptimization { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#property-value_15","title":"Property Value","text":"<p>GrammarOptimizationMode</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#defaultsamplingpipeline_1","title":"DefaultSamplingPipeline()","text":"<pre><code>public DefaultSamplingPipeline()\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#dispose","title":"Dispose()","text":"<pre><code>public void Dispose()\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#reset","title":"Reset()","text":"<pre><code>public void Reset()\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#acceptllamatoken","title":"Accept(LLamaToken)","text":"<pre><code>public void Accept(LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#parameters","title":"Parameters","text":"<p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#createchainsafellamacontexthandle","title":"CreateChain(SafeLLamaContextHandle)","text":"<pre><code>protected SafeLLamaSamplerChainHandle CreateChain(SafeLLamaContextHandle context)\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#parameters_1","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#returns","title":"Returns","text":"<p>SafeLLamaSamplerChainHandle</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#samplesafellamacontexthandle-int32","title":"Sample(SafeLLamaContextHandle, Int32)","text":"<pre><code>public LLamaToken Sample(SafeLLamaContextHandle ctx, int index)\n</code></pre>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#parameters_2","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>index</code> Int32</p>"},{"location":"xmldocs/llama.sampling.defaultsamplingpipeline/#returns_1","title":"Returns","text":"<p>LLamaToken</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.grammar/","title":"Llama.sampling.grammar","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.grammar/#grammar","title":"Grammar","text":"<p>Namespace: LLama.Sampling</p> <p>A grammar in GBNF form</p> <pre><code>public class Grammar : System.IEquatable`1[[LLama.Sampling.Grammar, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 Grammar Implements IEquatable&lt;Grammar&gt; Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.sampling.grammar/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.sampling.grammar/#equalitycontract","title":"EqualityContract","text":"<pre><code>protected Type EqualityContract { get; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#property-value","title":"Property Value","text":"<p>Type</p>"},{"location":"xmldocs/llama.sampling.grammar/#gbnf","title":"Gbnf","text":"<pre><code>public string Gbnf { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#property-value_1","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.sampling.grammar/#root","title":"Root","text":"<pre><code>public string Root { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#property-value_2","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.sampling.grammar/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.sampling.grammar/#grammarstring-string","title":"Grammar(String, String)","text":"<p>A grammar in GBNF form</p> <pre><code>public Grammar(string Gbnf, string Root)\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#parameters","title":"Parameters","text":"<p><code>Gbnf</code> String</p> <p><code>Root</code> String</p>"},{"location":"xmldocs/llama.sampling.grammar/#grammargrammar","title":"Grammar(Grammar)","text":"<pre><code>protected Grammar(Grammar original)\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#parameters_1","title":"Parameters","text":"<p><code>original</code> Grammar</p>"},{"location":"xmldocs/llama.sampling.grammar/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.grammar/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.sampling.grammar/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#parameters_2","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.sampling.grammar/#returns_1","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sampling.grammar/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#returns_2","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.sampling.grammar/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#parameters_3","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.sampling.grammar/#returns_3","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sampling.grammar/#equalsgrammar","title":"Equals(Grammar)","text":"<pre><code>public bool Equals(Grammar other)\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#parameters_4","title":"Parameters","text":"<p><code>other</code> Grammar</p>"},{"location":"xmldocs/llama.sampling.grammar/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sampling.grammar/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public Grammar &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#returns_5","title":"Returns","text":"<p>Grammar</p>"},{"location":"xmldocs/llama.sampling.grammar/#deconstructstring-string","title":"Deconstruct(String&amp;, String&amp;)","text":"<pre><code>public void Deconstruct(String&amp; Gbnf, String&amp; Root)\n</code></pre>"},{"location":"xmldocs/llama.sampling.grammar/#parameters_5","title":"Parameters","text":"<p><code>Gbnf</code> String&amp;</p> <p><code>Root</code> String&amp;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/","title":"llama.sampling.greedysamplingpipeline","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#greedysamplingpipeline","title":"GreedySamplingPipeline","text":"<p>Namespace: LLama.Sampling</p> <p>A sampling pipeline which always selects the most likely token</p> <pre><code>public class GreedySamplingPipeline : BaseSamplingPipeline, ISamplingPipeline, System.IDisposable\n</code></pre> <p>Inheritance Object \u2192 BaseSamplingPipeline \u2192 GreedySamplingPipeline Implements ISamplingPipeline, IDisposable Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#grammar","title":"Grammar","text":"<p>Grammar to apply to constrain possible tokens</p> <pre><code>public Grammar Grammar { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#property-value","title":"Property Value","text":"<p>Grammar</p>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#greedysamplingpipeline_1","title":"GreedySamplingPipeline()","text":"<pre><code>public GreedySamplingPipeline()\n</code></pre>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#createchainsafellamacontexthandle","title":"CreateChain(SafeLLamaContextHandle)","text":"<pre><code>protected SafeLLamaSamplerChainHandle CreateChain(SafeLLamaContextHandle context)\n</code></pre>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#parameters","title":"Parameters","text":"<p><code>context</code> SafeLLamaContextHandle</p>"},{"location":"xmldocs/llama.sampling.greedysamplingpipeline/#returns","title":"Returns","text":"<p>SafeLLamaSamplerChainHandle</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/","title":"llama.sampling.isamplingpipeline","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#isamplingpipeline","title":"ISamplingPipeline","text":"<p>Namespace: LLama.Sampling</p> <p>Convert a span of logits into a single sampled token. This interface can be implemented to completely customise the sampling process.</p> <pre><code>public interface ISamplingPipeline : System.IDisposable\n</code></pre> <p>Implements IDisposable Attributes NullableContextAttribute</p>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.isamplingpipeline/#samplesafellamacontexthandle-int32","title":"Sample(SafeLLamaContextHandle, Int32)","text":"<p>Sample a single token from the given context at the given position</p> <pre><code>LLamaToken Sample(SafeLLamaContextHandle ctx, int index)\n</code></pre>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#parameters","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle The context being sampled from</p> <p><code>index</code> Int32 Position to sample logits from</p>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#returns","title":"Returns","text":"<p>LLamaToken</p>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#applysafellamacontexthandle-llamatokendataarray","title":"Apply(SafeLLamaContextHandle, LLamaTokenDataArray)","text":"<p>Apply this pipeline to a set of token data</p> <pre><code>void Apply(SafeLLamaContextHandle ctx, LLamaTokenDataArray data)\n</code></pre>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#parameters_1","title":"Parameters","text":"<p><code>ctx</code> SafeLLamaContextHandle</p> <p><code>data</code> LLamaTokenDataArray</p>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#reset","title":"Reset()","text":"<p>Reset all internal state of the sampling pipeline</p> <pre><code>void Reset()\n</code></pre>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#acceptllamatoken","title":"Accept(LLamaToken)","text":"<p>Update the pipeline, with knowledge that a particular token was just accepted</p> <pre><code>void Accept(LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.sampling.isamplingpipeline/#parameters_2","title":"Parameters","text":"<p><code>token</code> LLamaToken</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.isamplingpipelineextensions/","title":"llama.sampling.isamplingpipelineextensions","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sampling.isamplingpipelineextensions/#isamplingpipelineextensions","title":"ISamplingPipelineExtensions","text":"<p>Namespace: LLama.Sampling</p> <p>Extension methods for ISamplingPipeline</p> <pre><code>public static class ISamplingPipelineExtensions\n</code></pre> <p>Inheritance Object \u2192 ISamplingPipelineExtensions Attributes ExtensionAttribute</p>"},{"location":"xmldocs/llama.sampling.isamplingpipelineextensions/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sampling.isamplingpipelineextensions/#sampleisamplingpipeline-llamacontext-int32","title":"Sample(ISamplingPipeline, LLamaContext, Int32)","text":"<p>Sample a single token from the given context at the given position</p> <pre><code>public static LLamaToken Sample(ISamplingPipeline pipe, LLamaContext ctx, int index)\n</code></pre>"},{"location":"xmldocs/llama.sampling.isamplingpipelineextensions/#parameters","title":"Parameters","text":"<p><code>pipe</code> ISamplingPipeline</p> <p><code>ctx</code> LLamaContext The context being sampled from</p> <p><code>index</code> Int32 Position to sample logits from</p>"},{"location":"xmldocs/llama.sampling.isamplingpipelineextensions/#returns","title":"Returns","text":"<p>LLamaToken</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sessionstate/","title":"llama.sessionstate","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.sessionstate/#sessionstate","title":"SessionState","text":"<p>Namespace: LLama</p> <p>The state of a chat session in-memory.</p> <pre><code>public class SessionState : System.IEquatable`1[[LLama.SessionState, LLamaSharp, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null]]\n</code></pre> <p>Inheritance Object \u2192 SessionState Implements IEquatable&lt;SessionState&gt; Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.sessionstate/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.sessionstate/#equalitycontract","title":"EqualityContract","text":"<pre><code>protected Type EqualityContract { get; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value","title":"Property Value","text":"<p>Type</p>"},{"location":"xmldocs/llama.sessionstate/#executorstate","title":"ExecutorState","text":"<p>Saved executor state for the session in JSON format.</p> <pre><code>public ExecutorBaseState ExecutorState { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value_1","title":"Property Value","text":"<p>ExecutorBaseState</p>"},{"location":"xmldocs/llama.sessionstate/#contextstate","title":"ContextState","text":"<p>Saved context state (KV cache) for the session.</p> <pre><code>public State ContextState { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value_2","title":"Property Value","text":"<p>State</p>"},{"location":"xmldocs/llama.sessionstate/#inputtransformpipeline","title":"InputTransformPipeline","text":"<p>The input transform pipeline used in this session.</p> <pre><code>public ITextTransform[] InputTransformPipeline { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value_3","title":"Property Value","text":"<p>ITextTransform[]</p>"},{"location":"xmldocs/llama.sessionstate/#outputtransform","title":"OutputTransform","text":"<p>The output transform used in this session.</p> <pre><code>public ITextStreamTransform OutputTransform { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value_4","title":"Property Value","text":"<p>ITextStreamTransform</p>"},{"location":"xmldocs/llama.sessionstate/#historytransform","title":"HistoryTransform","text":"<p>The history transform used in this session.</p> <pre><code>public IHistoryTransform HistoryTransform { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value_5","title":"Property Value","text":"<p>IHistoryTransform</p>"},{"location":"xmldocs/llama.sessionstate/#history","title":"History","text":"<p>The chat history messages for this session.</p> <pre><code>public Message[] History { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#property-value_6","title":"Property Value","text":"<p>Message[]</p>"},{"location":"xmldocs/llama.sessionstate/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.sessionstate/#sessionstatestate-executorbasestate-chathistory-listitexttransform-itextstreamtransform-ihistorytransform","title":"SessionState(State, ExecutorBaseState, ChatHistory, List&lt;ITextTransform&gt;, ITextStreamTransform, IHistoryTransform)","text":"<p>Create a new session state.</p> <pre><code>public SessionState(State contextState, ExecutorBaseState executorState, ChatHistory history, List&lt;ITextTransform&gt; inputTransformPipeline, ITextStreamTransform outputTransform, IHistoryTransform historyTransform)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters","title":"Parameters","text":"<p><code>contextState</code> State</p> <p><code>executorState</code> ExecutorBaseState</p> <p><code>history</code> ChatHistory</p> <p><code>inputTransformPipeline</code> List&lt;ITextTransform&gt;</p> <p><code>outputTransform</code> ITextStreamTransform</p> <p><code>historyTransform</code> IHistoryTransform</p>"},{"location":"xmldocs/llama.sessionstate/#sessionstatesessionstate","title":"SessionState(SessionState)","text":"<pre><code>protected SessionState(SessionState original)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters_1","title":"Parameters","text":"<p><code>original</code> SessionState</p>"},{"location":"xmldocs/llama.sessionstate/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.sessionstate/#savestring","title":"Save(String)","text":"<p>Save the session state to folder.</p> <pre><code>public void Save(string path)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters_2","title":"Parameters","text":"<p><code>path</code> String</p>"},{"location":"xmldocs/llama.sessionstate/#loadstring","title":"Load(String)","text":"<p>Load the session state from folder.</p> <pre><code>public static SessionState Load(string path)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters_3","title":"Parameters","text":"<p><code>path</code> String</p>"},{"location":"xmldocs/llama.sessionstate/#returns","title":"Returns","text":"<p>SessionState</p>"},{"location":"xmldocs/llama.sessionstate/#exceptions","title":"Exceptions","text":"<p>ArgumentException Throws when session state is incorrect</p>"},{"location":"xmldocs/llama.sessionstate/#tostring","title":"ToString()","text":"<pre><code>public string ToString()\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#returns_1","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.sessionstate/#printmembersstringbuilder","title":"PrintMembers(StringBuilder)","text":"<pre><code>protected bool PrintMembers(StringBuilder builder)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters_4","title":"Parameters","text":"<p><code>builder</code> StringBuilder</p>"},{"location":"xmldocs/llama.sessionstate/#returns_2","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sessionstate/#gethashcode","title":"GetHashCode()","text":"<pre><code>public int GetHashCode()\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#returns_3","title":"Returns","text":"<p>Int32</p>"},{"location":"xmldocs/llama.sessionstate/#equalsobject","title":"Equals(Object)","text":"<pre><code>public bool Equals(object obj)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters_5","title":"Parameters","text":"<p><code>obj</code> Object</p>"},{"location":"xmldocs/llama.sessionstate/#returns_4","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sessionstate/#equalssessionstate","title":"Equals(SessionState)","text":"<pre><code>public bool Equals(SessionState other)\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#parameters_6","title":"Parameters","text":"<p><code>other</code> SessionState</p>"},{"location":"xmldocs/llama.sessionstate/#returns_5","title":"Returns","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.sessionstate/#clone","title":"&lt;Clone&gt;$()","text":"<pre><code>public SessionState &lt;Clone&gt;$()\n</code></pre>"},{"location":"xmldocs/llama.sessionstate/#returns_6","title":"Returns","text":"<p>SessionState</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.statefulexecutorbase/","title":"Llama.statefulexecutorbase","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.statefulexecutorbase/#statefulexecutorbase","title":"StatefulExecutorBase","text":"<p>Namespace: LLama</p> <p>The base class for stateful LLama executors.</p> <pre><code>public abstract class StatefulExecutorBase : LLama.Abstractions.ILLamaExecutor\n</code></pre> <p>Inheritance Object \u2192 StatefulExecutorBase Implements ILLamaExecutor Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#fields","title":"Fields","text":""},{"location":"xmldocs/llama.statefulexecutorbase/#_logger","title":"_logger","text":"<p>The logger used by this executor.</p> <pre><code>protected ILogger _logger;\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#_pasttokenscount","title":"_pastTokensCount","text":"<p>The tokens that were already processed by the model.</p> <pre><code>protected int _pastTokensCount;\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#_consumedtokenscount","title":"_consumedTokensCount","text":"<p>The tokens that were consumed by the model during the current inference.</p> <pre><code>protected int _consumedTokensCount;\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#_n_session_consumed","title":"_n_session_consumed","text":"<pre><code>protected int _n_session_consumed;\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#_n_matching_session_tokens","title":"_n_matching_session_tokens","text":"<pre><code>protected int _n_matching_session_tokens;\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#_pathsession","title":"_pathSession","text":"<p>The path of the session file.</p> <pre><code>protected string _pathSession;\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#_embeds","title":"_embeds","text":"<p>A container of the tokens to be processed and after processed.</p> <pre><code>protected List&lt;LLamaToken&gt; _embeds;\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#_embed_inps","title":"_embed_inps","text":"<p>A container for the tokens of input.</p> <pre><code>protected List&lt;LLamaToken&gt; _embed_inps;\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#_session_tokens","title":"_session_tokens","text":"<pre><code>protected List&lt;LLamaToken&gt; _session_tokens;\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#_last_n_tokens","title":"_last_n_tokens","text":"<p>The last tokens generated by the model.</p> <pre><code>protected FixedSizeQueue&lt;LLamaToken&gt; _last_n_tokens;\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.statefulexecutorbase/#context","title":"Context","text":"<p>The context used by the executor.</p> <pre><code>public LLamaContext Context { get; }\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#property-value","title":"Property Value","text":"<p>LLamaContext</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#ismultimodal","title":"IsMultiModal","text":"<pre><code>public bool IsMultiModal { get; }\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#clipmodel","title":"ClipModel","text":"<pre><code>public LLavaWeights ClipModel { get; }\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#property-value_2","title":"Property Value","text":"<p>LLavaWeights</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#images","title":"Images","text":"<pre><code>public List&lt;Byte[]&gt; Images { get; }\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#property-value_3","title":"Property Value","text":"<p>List&lt;Byte[]&gt;</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.statefulexecutorbase/#statefulexecutorbasellamacontext-ilogger","title":"StatefulExecutorBase(LLamaContext, ILogger)","text":"<pre><code>protected StatefulExecutorBase(LLamaContext context, ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters","title":"Parameters","text":"<p><code>context</code> LLamaContext</p> <p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#statefulexecutorbasellamacontext-llavaweights-ilogger","title":"StatefulExecutorBase(LLamaContext, LLavaWeights, ILogger)","text":"<pre><code>public StatefulExecutorBase(LLamaContext context, LLavaWeights lLavaWeights, ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_1","title":"Parameters","text":"<p><code>context</code> LLamaContext</p> <p><code>lLavaWeights</code> LLavaWeights</p> <p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.statefulexecutorbase/#withsessionfilestring","title":"WithSessionFile(String)","text":"<p>This API is currently not verified.</p> <pre><code>public StatefulExecutorBase WithSessionFile(string filename)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_2","title":"Parameters","text":"<p><code>filename</code> String</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#returns","title":"Returns","text":"<p>StatefulExecutorBase</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#exceptions","title":"Exceptions","text":"<p>ArgumentNullException</p> <p>RuntimeError</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#savesessionfilestring","title":"SaveSessionFile(String)","text":"<p>This API has not been verified currently.</p> <pre><code>public void SaveSessionFile(string filename)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_3","title":"Parameters","text":"<p><code>filename</code> String</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#handlerunoutofcontextint32","title":"HandleRunOutOfContext(Int32)","text":"<p>After running out of the context, take some tokens from the original prompt and recompute the logits in batches.</p> <pre><code>protected void HandleRunOutOfContext(int tokensToKeep)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_4","title":"Parameters","text":"<p><code>tokensToKeep</code> Int32</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#tryreusematchingprefix","title":"TryReuseMatchingPrefix()","text":"<p>Try to reuse the matching prefix from the session file.</p> <pre><code>protected void TryReuseMatchingPrefix()\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#getloopconditioninferstateargs","title":"GetLoopCondition(InferStateArgs)","text":"<p>Decide whether to continue the loop.</p> <pre><code>protected abstract Task&lt;bool&gt; GetLoopCondition(InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_5","title":"Parameters","text":"<p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#returns_1","title":"Returns","text":"<p>Task&lt;Boolean&gt;</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#preprocessinputsstring-inferstateargs","title":"PreprocessInputs(String, InferStateArgs)","text":"<p>Preprocess the inputs before the inference.</p> <pre><code>protected abstract Task PreprocessInputs(string text, InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_6","title":"Parameters","text":"<p><code>text</code> String</p> <p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#returns_2","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#postprocessiinferenceparams-inferstateargs","title":"PostProcess(IInferenceParams, InferStateArgs)","text":"<p>Do some post processing after the inference.</p> <pre><code>protected abstract Task&lt;ValueTuple&lt;bool, IReadOnlyList&lt;string&gt;&gt;&gt; PostProcess(IInferenceParams inferenceParams, InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_7","title":"Parameters","text":"<p><code>inferenceParams</code> IInferenceParams</p> <p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#returns_3","title":"Returns","text":"<p>Task&lt;ValueTuple&lt;Boolean, IReadOnlyList&lt;String&gt;&gt;&gt;</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#inferinternaliinferenceparams-inferstateargs","title":"InferInternal(IInferenceParams, InferStateArgs)","text":"<p>The core inference logic.</p> <pre><code>protected abstract Task InferInternal(IInferenceParams inferenceParams, InferStateArgs args)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_8","title":"Parameters","text":"<p><code>inferenceParams</code> IInferenceParams</p> <p><code>args</code> InferStateArgs</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#returns_4","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#savestatestring","title":"SaveState(String)","text":"<p>Save the current state to a file.</p> <pre><code>public abstract Task SaveState(string filename)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_9","title":"Parameters","text":"<p><code>filename</code> String</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#returns_5","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#getstatedata","title":"GetStateData()","text":"<p>Get the current state data.</p> <pre><code>public abstract ExecutorBaseState GetStateData()\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#returns_6","title":"Returns","text":"<p>ExecutorBaseState</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#loadstateexecutorbasestate","title":"LoadState(ExecutorBaseState)","text":"<p>Load the state from data.</p> <pre><code>public abstract Task LoadState(ExecutorBaseState data)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_10","title":"Parameters","text":"<p><code>data</code> ExecutorBaseState</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#returns_7","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#loadstatestring","title":"LoadState(String)","text":"<p>Load the state from a file.</p> <pre><code>public abstract Task LoadState(string filename)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_11","title":"Parameters","text":"<p><code>filename</code> String</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#returns_8","title":"Returns","text":"<p>Task</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#inferasyncstring-iinferenceparams-cancellationtoken","title":"InferAsync(String, IInferenceParams, CancellationToken)","text":"<p>Execute the inference.</p> <pre><code>public IAsyncEnumerable&lt;string&gt; InferAsync(string text, IInferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_12","title":"Parameters","text":"<p><code>text</code> String The prompt. If null, generation will continue where it left off previously.</p> <p><code>inferenceParams</code> IInferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#returns_9","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#prefillpromptasyncstring","title":"PrefillPromptAsync(String)","text":"<p>Asynchronously runs a prompt through the model to compute KV cache without generating any new tokens.  It could reduce the latency of the first time response if the first input from the user is not immediate.</p> <pre><code>public Task PrefillPromptAsync(string prompt)\n</code></pre>"},{"location":"xmldocs/llama.statefulexecutorbase/#parameters_13","title":"Parameters","text":"<p><code>prompt</code> String Prompt to process</p>"},{"location":"xmldocs/llama.statefulexecutorbase/#returns_10","title":"Returns","text":"<p>Task</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.statelessexecutor/","title":"Llama.statelessexecutor","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.statelessexecutor/#statelessexecutor","title":"StatelessExecutor","text":"<p>Namespace: LLama</p> <p>This executor infer the input as one-time job. Previous inputs won't impact on the   response to current input.</p> <pre><code>public class StatelessExecutor : LLama.Abstractions.ILLamaExecutor\n</code></pre> <p>Inheritance Object \u2192 StatelessExecutor Implements ILLamaExecutor Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.statelessexecutor/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.statelessexecutor/#ismultimodal","title":"IsMultiModal","text":"<pre><code>public bool IsMultiModal { get; }\n</code></pre>"},{"location":"xmldocs/llama.statelessexecutor/#property-value","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.statelessexecutor/#clipmodel","title":"ClipModel","text":"<pre><code>public LLavaWeights ClipModel { get; }\n</code></pre>"},{"location":"xmldocs/llama.statelessexecutor/#property-value_1","title":"Property Value","text":"<p>LLavaWeights</p>"},{"location":"xmldocs/llama.statelessexecutor/#images","title":"Images","text":"<pre><code>public List&lt;Byte[]&gt; Images { get; }\n</code></pre>"},{"location":"xmldocs/llama.statelessexecutor/#property-value_2","title":"Property Value","text":"<p>List&lt;Byte[]&gt;</p>"},{"location":"xmldocs/llama.statelessexecutor/#context","title":"Context","text":"<p>The context used by the executor when running the inference.</p> <pre><code>public LLamaContext Context { get; private set; }\n</code></pre>"},{"location":"xmldocs/llama.statelessexecutor/#property-value_3","title":"Property Value","text":"<p>LLamaContext</p>"},{"location":"xmldocs/llama.statelessexecutor/#applytemplate","title":"ApplyTemplate","text":"<p>If true, applies the default template to the prompt as defined in the rules for llama_chat_apply_template template.</p> <pre><code>public bool ApplyTemplate { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.statelessexecutor/#property-value_4","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.statelessexecutor/#systemmessage","title":"SystemMessage","text":"<p>The system message to use with the prompt. Only used when StatelessExecutor.ApplyTemplate is true.</p> <pre><code>public string SystemMessage { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.statelessexecutor/#property-value_5","title":"Property Value","text":"<p>String</p>"},{"location":"xmldocs/llama.statelessexecutor/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.statelessexecutor/#statelessexecutorllamaweights-icontextparams-ilogger","title":"StatelessExecutor(LLamaWeights, IContextParams, ILogger)","text":"<p>Create a new stateless executor which will use the given model</p> <pre><code>public StatelessExecutor(LLamaWeights weights, IContextParams params, ILogger logger)\n</code></pre>"},{"location":"xmldocs/llama.statelessexecutor/#parameters","title":"Parameters","text":"<p><code>weights</code> LLamaWeights</p> <p><code>params</code> IContextParams</p> <p><code>logger</code> ILogger</p>"},{"location":"xmldocs/llama.statelessexecutor/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.statelessexecutor/#inferasyncstring-iinferenceparams-cancellationtoken","title":"InferAsync(String, IInferenceParams, CancellationToken)","text":"<pre><code>public IAsyncEnumerable&lt;string&gt; InferAsync(string prompt, IInferenceParams inferenceParams, CancellationToken cancellationToken)\n</code></pre>"},{"location":"xmldocs/llama.statelessexecutor/#parameters_1","title":"Parameters","text":"<p><code>prompt</code> String</p> <p><code>inferenceParams</code> IInferenceParams</p> <p><code>cancellationToken</code> CancellationToken</p>"},{"location":"xmldocs/llama.statelessexecutor/#returns","title":"Returns","text":"<p>IAsyncEnumerable&lt;String&gt;</p> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.streamingtokendecoder/","title":"llama.streamingtokendecoder","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.streamingtokendecoder/#streamingtokendecoder","title":"StreamingTokenDecoder","text":"<p>Namespace: LLama</p> <p>Decodes a stream of tokens into a stream of characters</p> <pre><code>public sealed class StreamingTokenDecoder\n</code></pre> <p>Inheritance Object \u2192 StreamingTokenDecoder Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#properties","title":"Properties","text":""},{"location":"xmldocs/llama.streamingtokendecoder/#availablecharacters","title":"AvailableCharacters","text":"<p>The number of decoded characters waiting to be read</p> <pre><code>public int AvailableCharacters { get; }\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#property-value","title":"Property Value","text":"<p>Int32</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#decodespecialtokens","title":"DecodeSpecialTokens","text":"<p>If true, special characters will be converted to text. If false they will be invisible.</p> <pre><code>public bool DecodeSpecialTokens { get; set; }\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#property-value_1","title":"Property Value","text":"<p>Boolean</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.streamingtokendecoder/#streamingtokendecoderencoding-llamaweights","title":"StreamingTokenDecoder(Encoding, LLamaWeights)","text":"<p>Create a new decoder</p> <pre><code>public StreamingTokenDecoder(Encoding encoding, LLamaWeights weights)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters","title":"Parameters","text":"<p><code>encoding</code> Encoding Text encoding to use</p> <p><code>weights</code> LLamaWeights Model weights</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#streamingtokendecoderllamacontext","title":"StreamingTokenDecoder(LLamaContext)","text":"<p>Create a new decoder</p> <pre><code>public StreamingTokenDecoder(LLamaContext context)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_1","title":"Parameters","text":"<p><code>context</code> LLamaContext Context to retrieve encoding and model weights from</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#streamingtokendecoderencoding-safellamacontexthandle","title":"StreamingTokenDecoder(Encoding, SafeLLamaContextHandle)","text":"<p>Create a new decoder</p> <pre><code>public StreamingTokenDecoder(Encoding encoding, SafeLLamaContextHandle context)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_2","title":"Parameters","text":"<p><code>encoding</code> Encoding Text encoding to use</p> <p><code>context</code> SafeLLamaContextHandle Context to retrieve model weights from</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#streamingtokendecoderencoding-safellamamodelhandle","title":"StreamingTokenDecoder(Encoding, SafeLlamaModelHandle)","text":"<p>Create a new decoder</p> <pre><code>public StreamingTokenDecoder(Encoding encoding, SafeLlamaModelHandle weights)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_3","title":"Parameters","text":"<p><code>encoding</code> Encoding Text encoding to use</p> <p><code>weights</code> SafeLlamaModelHandle Models weights to use</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.streamingtokendecoder/#addllamatoken","title":"Add(LLamaToken)","text":"<p>Add a single token to the decoder</p> <pre><code>public void Add(LLamaToken token)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_4","title":"Parameters","text":"<p><code>token</code> LLamaToken</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#addint32","title":"Add(Int32)","text":"<p>Add a single token to the decoder</p> <pre><code>public void Add(int token)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_5","title":"Parameters","text":"<p><code>token</code> Int32</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#addrangett","title":"AddRange&lt;T&gt;(T)","text":"<p>Add all tokens in the given enumerable</p> <pre><code>public void AddRange&lt;T&gt;(T tokens)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#type-parameters","title":"Type Parameters","text":"<p><code>T</code></p>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_6","title":"Parameters","text":"<p><code>tokens</code> T</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#addrangereadonlyspanllamatoken","title":"AddRange(ReadOnlySpan&lt;LLamaToken&gt;)","text":"<p>Add all tokens in the given span</p> <pre><code>public void AddRange(ReadOnlySpan&lt;LLamaToken&gt; tokens)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_7","title":"Parameters","text":"<p><code>tokens</code> ReadOnlySpan&lt;LLamaToken&gt;</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#readlistchar","title":"Read(List&lt;Char&gt;)","text":"<p>Read all decoded characters and clear the buffer</p> <pre><code>public void Read(List&lt;char&gt; dest)\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#parameters_8","title":"Parameters","text":"<p><code>dest</code> List&lt;Char&gt;</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#read","title":"Read()","text":"<p>Read all decoded characters as a string and clear the buffer</p> <pre><code>public string Read()\n</code></pre>"},{"location":"xmldocs/llama.streamingtokendecoder/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.streamingtokendecoder/#reset","title":"Reset()","text":"<p>Set the decoder back to its initial state</p> <pre><code>public void Reset()\n</code></pre> <p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/","title":"Llama.transformers.prompttemplatetransformer","text":"<p><code>&lt; Back</code></p>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#prompttemplatetransformer","title":"PromptTemplateTransformer","text":"<p>Namespace: LLama.Transformers</p> <p>A prompt formatter that will use llama.cpp's template formatter  If your model is not supported, you will need to define your own formatter according the cchat prompt specification for your model</p> <pre><code>public class PromptTemplateTransformer : LLama.Abstractions.IHistoryTransform\n</code></pre> <p>Inheritance Object \u2192 PromptTemplateTransformer Implements IHistoryTransform Attributes NullableContextAttribute, NullableAttribute</p>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#constructors","title":"Constructors","text":""},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#prompttemplatetransformerllamaweights-boolean","title":"PromptTemplateTransformer(LLamaWeights, Boolean)","text":"<p>A prompt formatter that will use llama.cpp's template formatter  If your model is not supported, you will need to define your own formatter according the cchat prompt specification for your model</p> <pre><code>public PromptTemplateTransformer(LLamaWeights model, bool withAssistant)\n</code></pre>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#parameters","title":"Parameters","text":"<p><code>model</code> LLamaWeights</p> <p><code>withAssistant</code> Boolean</p>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#methods","title":"Methods","text":""},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#historytotextchathistory","title":"HistoryToText(ChatHistory)","text":"<pre><code>public string HistoryToText(ChatHistory history)\n</code></pre>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#parameters_1","title":"Parameters","text":"<p><code>history</code> ChatHistory</p>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#returns","title":"Returns","text":"<p>String</p>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#texttohistoryauthorrole-string","title":"TextToHistory(AuthorRole, String)","text":"<pre><code>public ChatHistory TextToHistory(AuthorRole role, string text)\n</code></pre>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#parameters_2","title":"Parameters","text":"<p><code>role</code> AuthorRole</p> <p><code>text</code> String</p>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#returns_1","title":"Returns","text":"<p>ChatHistory</p>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#clone","title":"Clone()","text":"<pre><code>public IHistoryTransform Clone()\n</code></pre>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#returns_2","title":"Returns","text":"<p>IHistoryTransform</p>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#tomodelpromptllamatemplate","title":"ToModelPrompt(LLamaTemplate)","text":"<p>Apply the template to the messages and return the resulting prompt as a string</p> <pre><code>public static string ToModelPrompt(LLamaTemplate template)\n</code></pre>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#parameters_3","title":"Parameters","text":"<p><code>template</code> LLamaTemplate</p>"},{"location":"xmldocs/llama.transformers.prompttemplatetransformer/#returns_3","title":"Returns","text":"<p>String The formatted template string as defined by the model</p> <p><code>&lt; Back</code></p>"}]}