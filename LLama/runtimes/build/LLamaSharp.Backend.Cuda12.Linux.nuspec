<?xml version="1.0" encoding="utf-8"?>
<package >
    <metadata>
        <id>LLamaSharp.Backend.Cuda12.Linux</id>
        <version>$version$</version>
        <title>LLamaSharp.Backend.Cuda12.Linux</title>
        <authors>llama.cpp Authors</authors>
        <requireLicenseAcceptance>false</requireLicenseAcceptance>
        <license type="expression">MIT</license>
        <icon>icon512.png</icon>
        <projectUrl>https://github.com/SciSharp/LLamaSharp</projectUrl>
        <description>LLamaSharp.Backend.Cuda12.Linux contains the Linux binaries for LLamaSharp with Cuda12 support.</description>
        <releaseNotes></releaseNotes>
        <copyright>Copyright 2023 The llama.cpp Authors. All rights reserved.</copyright>
        <tags>LLamaSharp LLama LLM GPT AI ChatBot SciSharp</tags>

        <dependencies>
            <dependency id="LLamaSharp.Backend.Cpu" version="$version$" />
        </dependencies>
    </metadata>

    <files>
        <file src="LLamaSharpBackend.props" target="build/netstandard2.0/LLamaSharp.Backend.Cuda12.props" />

        <file src="runtimes/deps/cu12.2.0/libggml.so" target="runtimes/linux-x64/native/cuda12/libggml.so" />
        <file src="runtimes/deps/cu12.2.0/libggml-base.so" target="runtimes/linux-x64/native/cuda12/libggml-base.so" />
        <file src="runtimes/deps/cu12.2.0/libggml-cuda.so" target="runtimes/linux-x64/native/cuda12/libggml-cuda.so" />

        <file src="runtimes/deps/cu12.2.0/libllama.so" target="runtimes/linux-x64/native/cuda12/libllama.so" />
        <file src="runtimes/deps/cu12.2.0/libllava_shared.so" target="runtimes/linux-x64/native/cuda12/libllava_shared.so" />
        
        <file src="icon512.png" target="icon512.png" />
    </files>
</package>
