<?xml version="1.0" encoding="utf-8"?>
<package >
  <metadata>
    <id>LLamaSharp.Backend.Cpu</id>
    <version>$version$</version>
    <title>LLamaSharp.Backend.Cpu, the backend for LLamaSharp</title>
    <authors>llama.cpp Authors</authors>
    <requireLicenseAcceptance>false</requireLicenseAcceptance>
    <license type="expression">MIT</license>
    <icon>icon512.png</icon>
    <projectUrl>https://github.com/SciSharp/LLamaSharp</projectUrl>
    <description>LLamaSharp.Backend.Cpu is a backend for LLamaSharp to use with Cpu only.</description>
    <releaseNotes></releaseNotes>
    <copyright>Copyright 2023 The llama.cpp Authors. All rights reserved.</copyright>
    <tags>LLamaSharp LLama LLM GPT AI ChatBot SciSharp</tags>
  </metadata>

  <files>
    <file src="LLamaSharpBackend.props" target="build/netstandard2.0/LLamaSharp.Backend.Cpu.props" />

    <file src="runtimes/deps/llama.dll" target="runtimes\win-x64\native\llama.dll" />
    <file src="runtimes/deps/avx/llama.dll" target="runtimes\win-x64\native\avx\llama.dll" />
    <file src="runtimes/deps/avx2/llama.dll" target="runtimes\win-x64\native\avx2\llama.dll" />
    <file src="runtimes/deps/avx512/llama.dll" target="runtimes\win-x64\native\avx512\llama.dll" />

    <file src="runtimes/deps/libllama.so" target="runtimes\linux-x64\native\libllama.so" />
    <file src="runtimes/deps/avx/libllama.so" target="runtimes\linux-x64\native\avx\libllama.so" />
    <file src="runtimes/deps/avx2/libllama.so" target="runtimes\linux-x64\native\avx2\libllama.so" />
    <file src="runtimes/deps/avx512/libllama.so" target="runtimes\linux-x64\native\avx512\libllama.so" />
    
    <file src="runtimes/deps/osx-x64/libllama.dylib" target="runtimes\osx-x64\native\libllama.dylib" />
    <file src="runtimes/deps/osx-arm64/libllama.dylib" target="runtimes\osx-arm64\native\libllama.dylib" />
    <file src="runtimes/deps/osx-arm64/ggml-metal.metal" target="runtimes\osx-arm64\native\ggml-metal.metal" />
    
    <file src="icon512.png" target="icon512.png" />
  </files>
</package>
