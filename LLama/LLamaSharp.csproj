<Project Sdk="Microsoft.NET.Sdk">
  <PropertyGroup>
    <TargetFrameworks>netstandard2.0;net6.0;net8.0</TargetFrameworks>
    <RootNamespace>LLama</RootNamespace>
    <Nullable>enable</Nullable>
    <LangVersion>12</LangVersion>
    <Platforms>AnyCPU;x64;Arm64</Platforms>
    <AllowUnsafeBlocks>True</AllowUnsafeBlocks>

    <Version>0.13.0</Version>
    <Authors>Rinne, Martin Evans, jlsantiago and all the other contributors in https://github.com/SciSharp/LLamaSharp/graphs/contributors.</Authors>
    <Company>SciSharp STACK</Company>
    <GeneratePackageOnBuild>true</GeneratePackageOnBuild>
    <Copyright>MIT, SciSharp STACK $([System.DateTime]::UtcNow.ToString(yyyy))</Copyright>
    <RepositoryUrl>https://github.com/SciSharp/LLamaSharp</RepositoryUrl>
    <RepositoryType>git</RepositoryType>
    <PackageIconUrl>https://avatars3.githubusercontent.com/u/44989469?s=200&amp;v=4</PackageIconUrl>
    <PackageTags>LLama, LLM, GPT, ChatGPT, NLP, AI, Chat Bot, SciSharp</PackageTags>
    <Description>
      LLamaSharp is a cross-platform library to run ðŸ¦™LLaMA/LLaVA model (and others) in your local device. 
      Based on [llama.cpp](https://github.com/ggerganov/llama.cpp), inference with LLamaSharp is efficient on both CPU and GPU. 
      With the higher-level APIs and RAG support, it's convenient to deploy LLM (Large Language Model) in your application with LLamaSharp.
    </Description>
    <PackageReleaseNotes>
      Updated llama.cpp version to include better support for LLama3 tokenization.
    </PackageReleaseNotes>
    <PackageLicenseExpression>MIT</PackageLicenseExpression>
    <PackageOutputPath>packages</PackageOutputPath>
    <Platforms>AnyCPU;x64;Arm64</Platforms>
    <PackageId>LLamaSharp</PackageId>
    <Configurations>Debug;Release;GPU</Configurations>
    <GenerateAssemblyInfo>false</GenerateAssemblyInfo>
  </PropertyGroup>

  <PropertyGroup>
    <PackageReadmeFile>README.md</PackageReadmeFile>
    <GenerateDocumentationFile>True</GenerateDocumentationFile>
  </PropertyGroup>

  <ItemGroup>
    <None Include="..\README.md" Pack="true" PackagePath="\" />
  </ItemGroup>

  <ItemGroup Condition="'$(TargetFramework)' == 'netstandard2.0'">
    <PackageReference Include="IsExternalInit" Version="1.0.3" PrivateAssets="all" />
    <PackageReference Include="System.Memory" Version="4.5.5" PrivateAssets="all" />
    <PackageReference Include="System.Linq.Async" Version="6.0.1" />
    <PackageReference Include="System.Text.Json" Version="8.0.4" />
  </ItemGroup>

  <ItemGroup>
    <PackageReference Include="Microsoft.Extensions.Logging.Abstractions" Version="8.0.1" />
  </ItemGroup>

  <Target Name="DownloadReleaseBinariesInner">
    <PropertyGroup>
        <BinaryReleaseId>1c5eba6f8e62</BinaryReleaseId>
    </PropertyGroup>
	<Message Importance="High" Text="Download '$(BinaryReleaseId)/deps.zip' to 'runtimes'" />
	<DownloadFile
		SourceUrl="https://github.com/SciSharp/LLamaSharpBinaries/releases/download/$(BinaryReleaseId)/deps.zip"
		DestinationFolder="runtimes"
		DestinationFileName="deps.zip"
		SkipUnchangedFiles="true"
	/>

	<Message Importance="High" Text="Unzip '$(BinaryReleaseId)/deps.zip'" />
	<Unzip
		SourceFiles="runtimes/deps.zip"
		DestinationFolder="runtimes/deps/"
		OverwriteReadOnlyFiles="false"
		Include="*.dll;*.so;*.dylib;*.metal;"
	/>
  </Target>
  <Target Name="DownloadReleaseBinaries" BeforeTargets="DispatchToInnerBuilds;BeforeBuild">
    <MSBuild Projects="$(MSBuildProjectFile)" Targets="DownloadReleaseBinariesInner" Properties="TargetFramework=once" />
  </Target>
  
</Project>