{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "AllowedHosts": "*",
  "LLamaOptions": {
    "ModelLoadType": 0,
    "Models": [
      {
        "Name": "Qwen3-VL-2B-Instruct Q4_K_M",
        "MaxInstances": 20,
        "ModelPath": "Models/Qwen3VL-2B-Instruct-Q4_K_M.gguf",
        "ModelDownloadUrl": "https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF/resolve/main/Qwen3VL-2B-Instruct-Q4_K_M.gguf",
        "MmprojPath": "Models/mmproj-Qwen3VL-2B-Instruct-Q8_0.gguf",
        "MmprojDownloadUrl": "https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-2B-Instruct-Q8_0.gguf",
        "ContextSize": 4096,
        "BatchSize": 512,
        "UBatchSize": 512,
        "Threads": 4,
        "GpuLayerCount": 32,
        "UseMemorymap": true,
        "UseMemoryLock": false,
        "MainGpu": 0,
        "LowVram": false,
        "Seed": 1686349486,
        "UseFp16Memory": false,
        "Perplexity": false,
        "LoraAdapter": "",
        "LoraBase": "",
        "EmbeddingMode": false,
        "TensorSplits": null,
        "GroupedQueryAttention": 1,
        "RmsNormEpsilon": 0.000005,
        "RopeFrequencyBase": 10000.0,
        "RopeFrequencyScale": 1.0,
        "MulMatQ": false,
        "Encoding": "UTF-8"
      },
      {
        "Name": "Gemma-3-4B-IT Q4_K_M",
        "MaxInstances": 20,
        "ModelPath": "Models/google_gemma-3-4b-it-q4_k_m.gguf",
        "ModelDownloadUrl": "https://huggingface.co/ggml-org/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_M.gguf",
        "MmprojPath": "Models/google_gemma-3-4b-it-mmproj-f16.gguf",
        "MmprojDownloadUrl": "https://huggingface.co/ggml-org/gemma-3-4b-it-GGUF/resolve/main/mmproj-model-f16.gguf",
        "ContextSize": 4096,
        "BatchSize": 512,
        "UBatchSize": 512,
        "Threads": 4,
        "GpuLayerCount": 32,
        "UseMemorymap": true,
        "UseMemoryLock": false,
        "MainGpu": 0,
        "LowVram": false,
        "Seed": 1686349486,
        "UseFp16Memory": false,
        "Perplexity": false,
        "LoraAdapter": "",
        "LoraBase": "",
        "EmbeddingMode": false,
        "TensorSplits": null,
        "GroupedQueryAttention": 1,
        "RmsNormEpsilon": 0.000005,
        "RopeFrequencyBase": 10000.0,
        "RopeFrequencyScale": 1.0,
        "MulMatQ": false,
        "Encoding": "UTF-8"
      },
      {
        "Name": "Qwen2.5-Omni-3B Q4_K_M (Audio)",
        "MaxInstances": 20,
        "ModelPath": "Models/Qwen2.5-Omni-3B-Q4_K_M.gguf",
        "ModelDownloadUrl": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/Qwen2.5-Omni-3B-Q4_K_M.gguf",
        "MmprojPath": "Models/mmproj-Qwen2.5-Omni-3B-Q8_0.gguf",
        "MmprojDownloadUrl": "https://huggingface.co/ggml-org/Qwen2.5-Omni-3B-GGUF/resolve/main/mmproj-Qwen2.5-Omni-3B-Q8_0.gguf",
        "ContextSize": 4096,
        "BatchSize": 512,
        "UBatchSize": 512,
        "Threads": 4,
        "GpuLayerCount": 32,
        "UseMemorymap": true,
        "UseMemoryLock": false,
        "MainGpu": 0,
        "LowVram": false,
        "Seed": 1686349486,
        "UseFp16Memory": false,
        "Perplexity": false,
        "LoraAdapter": "",
        "LoraBase": "",
        "EmbeddingMode": false,
        "TensorSplits": null,
        "GroupedQueryAttention": 1,
        "RmsNormEpsilon": 0.000005,
        "RopeFrequencyBase": 10000.0,
        "RopeFrequencyScale": 1.0,
        "MulMatQ": false,
        "Encoding": "UTF-8"
      }
    ]
  }
}
