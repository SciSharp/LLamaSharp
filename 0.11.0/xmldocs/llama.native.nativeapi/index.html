
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../llama.native.llavaimageembed/">
      
      
        <link rel="next" href="../llama.native.nativelibraryconfig/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.20">
    
    
      
        <title>llama.native.nativeapi - LLamaSharp Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.eebd395e.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#nativeapi" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLamaSharp Documentation" class="md-header__button md-logo" aria-label="LLamaSharp Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLamaSharp Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              llama.native.nativeapi
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLamaSharp Documentation" class="md-nav__button md-logo" aria-label="LLamaSharp Documentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    LLamaSharp Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Overview
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../QuickStart/" class="md-nav__link">
        Quick Start
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../Architecture/" class="md-nav__link">
        Architecture
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../FAQ/" class="md-nav__link">
        FAQ
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../ContributingGuide/" class="md-nav__link">
        Contributing Guide
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          Tutorials
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Tutorials
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tutorials/NativeLibraryConfig/" class="md-nav__link">
        Customize the native library loading
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tutorials/Executors/" class="md-nav__link">
        Use executors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tutorials/ChatSession/" class="md-nav__link">
        Use ChatSession
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tutorials/UnderstandLLamaContext/" class="md-nav__link">
        Understand LLamaContext
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tutorials/GetEmbeddings/" class="md-nav__link">
        Get embeddings
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Tutorials/Quantization/" class="md-nav__link">
        Quantize the model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          Integrations
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Integrations
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Integrations/semantic-kernel/" class="md-nav__link">
        semantic-kernel integration
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Integrations/kernel-memory/" class="md-nav__link">
        kernel-memory integration
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Integrations/BotSharp.md" class="md-nav__link">
        BotSharp integration
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Integrations/Langchain.md" class="md-nav__link">
        Langchain integration
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
      
      
      
        <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
          Examples
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Examples
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/BatchedExecutorFork/" class="md-nav__link">
        Bacthed executor - multi-output to one input
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/BatchedExecutorGuidance/" class="md-nav__link">
        Batched executor - basic guidance
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/BatchedExecutorRewind/" class="md-nav__link">
        Batched executor - rewinding to an earlier state
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/ChatChineseGB2312/" class="md-nav__link">
        Chinese LLM - with GB2312 encoding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/ChatSessionStripRoleName/" class="md-nav__link">
        ChatSession - stripping role names
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/ChatSessionWithHistory/" class="md-nav__link">
        ChatSession - with history
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/ChatSessionWithRestart/" class="md-nav__link">
        ChatSession - restarting
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/ChatSessionWithRoleName/" class="md-nav__link">
        ChatSession - Basic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/CodingAssistant/" class="md-nav__link">
        Coding assistant
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/GetEmbeddings/" class="md-nav__link">
        Get embeddings
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/GrammarJsonResponse/" class="md-nav__link">
        Grammar - json response
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/InstructModeExecute/" class="md-nav__link">
        Instruct executor - basic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/InteractiveModeExecute/" class="md-nav__link">
        Interactive executor - basic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/KernelMemory/" class="md-nav__link">
        Kernel memory integration - basic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/KernelMemorySaveAndLoad/" class="md-nav__link">
        Kernel-memory - save & load
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/LLavaInteractiveModeExecute/" class="md-nav__link">
        LLaVA - basic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/LoadAndSaveSession/" class="md-nav__link">
        ChatSession - load & save
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/LoadAndSaveState/" class="md-nav__link">
        Executor - save/load state
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/QuantizeModel/" class="md-nav__link">
        Quantization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/SemanticKernelChat/" class="md-nav__link">
        Semantic-kernel - chat
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/SemanticKernelMemory/" class="md-nav__link">
        Semantic-kernel - with kernel-memory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/SemanticKernelPrompt/" class="md-nav__link">
        Semantic-kernel - basic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/StatelessModeExecute/" class="md-nav__link">
        Stateless executor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Examples/TalkToYourself/" class="md-nav__link">
        Talk to yourself
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_9" checked>
      
      
      
        <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
          API Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          API Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.adaptercollection/" class="md-nav__link">
        llama.abstractions.adaptercollection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.icontextparams/" class="md-nav__link">
        llama.abstractions.icontextparams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.ihistorytransform/" class="md-nav__link">
        llama.abstractions.ihistorytransform
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.iinferenceparams/" class="md-nav__link">
        llama.abstractions.iinferenceparams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.illamaexecutor/" class="md-nav__link">
        llama.abstractions.illamaexecutor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.illamaparams/" class="md-nav__link">
        llama.abstractions.illamaparams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.imodelparams/" class="md-nav__link">
        llama.abstractions.imodelparams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.itextstreamtransform/" class="md-nav__link">
        llama.abstractions.itextstreamtransform
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.itexttransform/" class="md-nav__link">
        llama.abstractions.itexttransform
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.loraadapter/" class="md-nav__link">
        llama.abstractions.loraadapter
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.metadataoverride/" class="md-nav__link">
        llama.abstractions.metadataoverride
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.metadataoverrideconverter/" class="md-nav__link">
        llama.abstractions.metadataoverrideconverter
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.tensorsplitscollection/" class="md-nav__link">
        llama.abstractions.tensorsplitscollection
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.abstractions.tensorsplitscollectionconverter/" class="md-nav__link">
        llama.abstractions.tensorsplitscollectionconverter
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.antipromptprocessor/" class="md-nav__link">
        llama.antipromptprocessor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.batched.alreadypromptedconversationexception/" class="md-nav__link">
        llama.batched.alreadypromptedconversationexception
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.batched.batchedexecutor/" class="md-nav__link">
        llama.batched.batchedexecutor
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.batched.cannotforkwhilerequiresinferenceexception/" class="md-nav__link">
        llama.batched.cannotforkwhilerequiresinferenceexception
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.batched.cannotmodifywhilerequiresinferenceexception/" class="md-nav__link">
        llama.batched.cannotmodifywhilerequiresinferenceexception
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.batched.cannotsamplerequiresinferenceexception/" class="md-nav__link">
        llama.batched.cannotsamplerequiresinferenceexception
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.batched.cannotsamplerequirespromptexception/" class="md-nav__link">
        llama.batched.cannotsamplerequirespromptexception
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.batched.conversation/" class="md-nav__link">
        llama.batched.conversation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.batched.conversationextensions/" class="md-nav__link">
        llama.batched.conversationextensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.batched.experimentalbatchedexecutorexception/" class="md-nav__link">
        llama.batched.experimentalbatchedexecutorexception
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.chatsession-1/" class="md-nav__link">
        llama.chatsession-1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.chatsession/" class="md-nav__link">
        llama.chatsession
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.common.authorrole/" class="md-nav__link">
        llama.common.authorrole
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.common.chathistory/" class="md-nav__link">
        llama.common.chathistory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.common.fixedsizequeue-1/" class="md-nav__link">
        llama.common.fixedsizequeue-1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.common.inferenceparams/" class="md-nav__link">
        llama.common.inferenceparams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.common.mirostattype/" class="md-nav__link">
        llama.common.mirostattype
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.common.modelparams/" class="md-nav__link">
        llama.common.modelparams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.grammarexpectedname/" class="md-nav__link">
        llama.exceptions.grammarexpectedname
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.grammarexpectednext/" class="md-nav__link">
        llama.exceptions.grammarexpectednext
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.grammarexpectedprevious/" class="md-nav__link">
        llama.exceptions.grammarexpectedprevious
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.grammarformatexception/" class="md-nav__link">
        llama.exceptions.grammarformatexception
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.grammarunexpectedcharaltelement/" class="md-nav__link">
        llama.exceptions.grammarunexpectedcharaltelement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.grammarunexpectedcharrngelement/" class="md-nav__link">
        llama.exceptions.grammarunexpectedcharrngelement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.grammarunexpectedendelement/" class="md-nav__link">
        llama.exceptions.grammarunexpectedendelement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.grammarunexpectedendofinput/" class="md-nav__link">
        llama.exceptions.grammarunexpectedendofinput
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.grammarunexpectedhexcharscount/" class="md-nav__link">
        llama.exceptions.grammarunexpectedhexcharscount
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.grammarunknownescapecharacter/" class="md-nav__link">
        llama.exceptions.grammarunknownescapecharacter
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.llamadecodeerror/" class="md-nav__link">
        llama.exceptions.llamadecodeerror
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.loadweightsfailedexception/" class="md-nav__link">
        llama.exceptions.loadweightsfailedexception
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.exceptions.runtimeerror/" class="md-nav__link">
        llama.exceptions.runtimeerror
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.extensions.icontextparamsextensions/" class="md-nav__link">
        llama.extensions.icontextparamsextensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.extensions.imodelparamsextensions/" class="md-nav__link">
        llama.extensions.imodelparamsextensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.grammars.grammar/" class="md-nav__link">
        llama.grammars.grammar
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.grammars.grammarrule/" class="md-nav__link">
        llama.grammars.grammarrule
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.ichatmodel/" class="md-nav__link">
        llama.ichatmodel
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.llamacache/" class="md-nav__link">
        llama.llamacache
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.llamaembedder/" class="md-nav__link">
        llama.llamaembedder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.llamamodel/" class="md-nav__link">
        llama.llamamodel
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.llamamodelv1/" class="md-nav__link">
        llama.llamamodelv1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.llamaparams/" class="md-nav__link">
        llama.llamaparams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.llamaquantizer/" class="md-nav__link">
        llama.llamaquantizer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.llamastate/" class="md-nav__link">
        llama.llamastate
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.llamatransforms/" class="md-nav__link">
        llama.llamatransforms
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.llavaweights/" class="md-nav__link">
        llama.llavaweights
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.decoderesult/" class="md-nav__link">
        llama.native.decoderesult
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.ggmltype/" class="md-nav__link">
        llama.native.ggmltype
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.gpusplitmode/" class="md-nav__link">
        llama.native.gpusplitmode
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamabatch/" class="md-nav__link">
        llama.native.llamabatch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamabeamsstate/" class="md-nav__link">
        llama.native.llamabeamsstate
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamabeamview/" class="md-nav__link">
        llama.native.llamabeamview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamachatmessage/" class="md-nav__link">
        llama.native.llamachatmessage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamacontextparams/" class="md-nav__link">
        llama.native.llamacontextparams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamaftype/" class="md-nav__link">
        llama.native.llamaftype
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamagrammarelement/" class="md-nav__link">
        llama.native.llamagrammarelement
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamagrammarelementtype/" class="md-nav__link">
        llama.native.llamagrammarelementtype
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamakvcacheview/" class="md-nav__link">
        llama.native.llamakvcacheview
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamakvcacheviewcell/" class="md-nav__link">
        llama.native.llamakvcacheviewcell
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamakvcacheviewsafehandle/" class="md-nav__link">
        llama.native.llamakvcacheviewsafehandle
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamaloglevel/" class="md-nav__link">
        llama.native.llamaloglevel
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamamodelkvoverridetype/" class="md-nav__link">
        llama.native.llamamodelkvoverridetype
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamamodelmetadataoverride/" class="md-nav__link">
        llama.native.llamamodelmetadataoverride
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamamodelparams/" class="md-nav__link">
        llama.native.llamamodelparams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamamodelquantizeparams/" class="md-nav__link">
        llama.native.llamamodelquantizeparams
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamanativebatch/" class="md-nav__link">
        llama.native.llamanativebatch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamapoolingtype/" class="md-nav__link">
        llama.native.llamapoolingtype
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamapos/" class="md-nav__link">
        llama.native.llamapos
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamaropetype/" class="md-nav__link">
        llama.native.llamaropetype
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamaseqid/" class="md-nav__link">
        llama.native.llamaseqid
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamatoken/" class="md-nav__link">
        llama.native.llamatoken
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamatokendata/" class="md-nav__link">
        llama.native.llamatokendata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamatokendataarray/" class="md-nav__link">
        llama.native.llamatokendataarray
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamatokendataarraynative/" class="md-nav__link">
        llama.native.llamatokendataarraynative
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamatokentype/" class="md-nav__link">
        llama.native.llamatokentype
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llamavocabtype/" class="md-nav__link">
        llama.native.llamavocabtype
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.llavaimageembed/" class="md-nav__link">
        llama.native.llavaimageembed
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          llama.native.nativeapi
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        llama.native.nativeapi
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav" aria-label="Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama_sample_token_mirostatsafellamacontexthandle-llamatokendataarraynative-single-single-int32-single" class="md-nav__link">
    llama_sample_token_mirostat(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Int32, Single&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_token_mirostat(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, Single, Int32, Single&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_token_mirostat_v2safellamacontexthandle-llamatokendataarraynative-single-single-single" class="md-nav__link">
    llama_sample_token_mirostat_v2(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Single&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_token_mirostat_v2(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, Single, Single&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_1" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_1" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_token_greedysafellamacontexthandle-llamatokendataarraynative" class="md-nav__link">
    llama_sample_token_greedy(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_token_greedy(SafeLLamaContextHandle, LLamaTokenDataArrayNative&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_2" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_2" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_tokensafellamacontexthandle-llamatokendataarraynative" class="md-nav__link">
    llama_sample_token(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_token(SafeLLamaContextHandle, LLamaTokenDataArrayNative&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_3" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_3" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_embeddingsg__llama_get_embeddings_native30_0safellamacontexthandle" class="md-nav__link">
    &lt;llama_get_embeddings&gt;g__llama_get_embeddings_native|30_0(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="<llama_get_embeddings>g__llama_get_embeddings_native|30_0(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_4" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_4" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_to_pieceg__llama_token_to_piece_native44_0safellamamodelhandle-llamatoken-byte-int32" class="md-nav__link">
    &lt;llama_token_to_piece&gt;g__llama_token_to_piece_native|44_0(SafeLlamaModelHandle, LLamaToken, Byte*, Int32)
  </a>
  
    <nav class="md-nav" aria-label="<llama_token_to_piece>g__llama_token_to_piece_native|44_0(SafeLlamaModelHandle, LLamaToken, Byte*, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_5" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_5" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tryloadlibrariesg__tryload84_0string" class="md-nav__link">
    &lt;TryLoadLibraries&gt;g__TryLoad|84_0(String)
  </a>
  
    <nav class="md-nav" aria-label="<TryLoadLibraries>g__TryLoad|84_0(String)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_6" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_6" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tryloadlibrariesg__tryfindpath84_1string-c__displayclass84_0" class="md-nav__link">
    &lt;TryLoadLibraries&gt;g__TryFindPath|84_1(String, &lt;&gt;c__DisplayClass84_0&amp;)
  </a>
  
    <nav class="md-nav" aria-label="<TryLoadLibraries>g__TryFindPath|84_1(String, <>c__DisplayClass84_0&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_7" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_7" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_set_n_threadssafellamacontexthandle-uint32-uint32" class="md-nav__link">
    llama_set_n_threads(SafeLLamaContextHandle, UInt32, UInt32)
  </a>
  
    <nav class="md-nav" aria-label="llama_set_n_threads(SafeLLamaContextHandle, UInt32, UInt32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_8" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_vocab_typesafellamamodelhandle" class="md-nav__link">
    llama_vocab_type(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_vocab_type(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_9" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_8" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_rope_typesafellamamodelhandle" class="md-nav__link">
    llama_rope_type(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_rope_type(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_10" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_9" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_grammar_initllamagrammarelement-uint64-uint64" class="md-nav__link">
    llama_grammar_init(LLamaGrammarElement, UInt64, UInt64)**
  </a>
  
    <nav class="md-nav" aria-label="llama_grammar_init(LLamaGrammarElement, UInt64, UInt64)**">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_11" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_10" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_grammar_freeintptr" class="md-nav__link">
    llama_grammar_free(IntPtr)
  </a>
  
    <nav class="md-nav" aria-label="llama_grammar_free(IntPtr)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_12" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_grammar_copysafellamagrammarhandle" class="md-nav__link">
    llama_grammar_copy(SafeLLamaGrammarHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_grammar_copy(SafeLLamaGrammarHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_13" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_11" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_grammarsafellamacontexthandle-llamatokendataarraynative-safellamagrammarhandle" class="md-nav__link">
    llama_sample_grammar(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, SafeLLamaGrammarHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_grammar(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, SafeLLamaGrammarHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_14" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_grammar_accept_tokensafellamacontexthandle-safellamagrammarhandle-llamatoken" class="md-nav__link">
    llama_grammar_accept_token(SafeLLamaContextHandle, SafeLLamaGrammarHandle, LLamaToken)
  </a>
  
    <nav class="md-nav" aria-label="llama_grammar_accept_token(SafeLLamaContextHandle, SafeLLamaGrammarHandle, LLamaToken)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_15" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava_validate_embed_sizesafellamacontexthandle-safellavamodelhandle" class="md-nav__link">
    llava_validate_embed_size(SafeLLamaContextHandle, SafeLlavaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llava_validate_embed_size(SafeLLamaContextHandle, SafeLlavaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_16" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_12" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava_image_embed_make_with_bytessafellavamodelhandle-int32-byte-int32" class="md-nav__link">
    llava_image_embed_make_with_bytes(SafeLlavaModelHandle, Int32, Byte[], Int32)
  </a>
  
    <nav class="md-nav" aria-label="llava_image_embed_make_with_bytes(SafeLlavaModelHandle, Int32, Byte[], Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_17" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_13" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava_image_embed_make_with_filenamesafellavamodelhandle-int32-string" class="md-nav__link">
    llava_image_embed_make_with_filename(SafeLlavaModelHandle, Int32, String)
  </a>
  
    <nav class="md-nav" aria-label="llava_image_embed_make_with_filename(SafeLlavaModelHandle, Int32, String)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_18" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_14" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava_image_embed_freeintptr" class="md-nav__link">
    llava_image_embed_free(IntPtr)
  </a>
  
    <nav class="md-nav" aria-label="llava_image_embed_free(IntPtr)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_19" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava_eval_image_embedsafellamacontexthandle-safellavaimageembedhandle-int32-int32" class="md-nav__link">
    llava_eval_image_embed(SafeLLamaContextHandle, SafeLlavaImageEmbedHandle, Int32, Int32&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llava_eval_image_embed(SafeLLamaContextHandle, SafeLlavaImageEmbedHandle, Int32, Int32&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_20" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_15" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_model_quantizestring-string-llamamodelquantizeparams" class="md-nav__link">
    llama_model_quantize(String, String, LLamaModelQuantizeParams*)
  </a>
  
    <nav class="md-nav" aria-label="llama_model_quantize(String, String, LLamaModelQuantizeParams*)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_21" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_16" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_repetition_penaltiessafellamacontexthandle-llamatokendataarraynative-llamatoken-uint64-single-single-single" class="md-nav__link">
    llama_sample_repetition_penalties(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, LLamaToken*, UInt64, Single, Single, Single)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_repetition_penalties(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, LLamaToken*, UInt64, Single, Single, Single)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_22" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_apply_guidancesafellamacontexthandle-spansingle-readonlyspansingle-single" class="md-nav__link">
    llama_sample_apply_guidance(SafeLLamaContextHandle, Span&lt;Single&gt;, ReadOnlySpan&lt;Single&gt;, Single)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_apply_guidance(SafeLLamaContextHandle, Span<Single>, ReadOnlySpan<Single>, Single)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_23" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_apply_guidancesafellamacontexthandle-single-single-single" class="md-nav__link">
    llama_sample_apply_guidance(SafeLLamaContextHandle, Single, Single, Single)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_apply_guidance(SafeLLamaContextHandle, Single, Single, Single)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_24" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_softmaxsafellamacontexthandle-llamatokendataarraynative" class="md-nav__link">
    llama_sample_softmax(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_softmax(SafeLLamaContextHandle, LLamaTokenDataArrayNative&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_25" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_top_ksafellamacontexthandle-llamatokendataarraynative-int32-uint64" class="md-nav__link">
    llama_sample_top_k(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Int32, UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_top_k(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Int32, UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_26" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_top_psafellamacontexthandle-llamatokendataarraynative-single-uint64" class="md-nav__link">
    llama_sample_top_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_top_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_27" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_min_psafellamacontexthandle-llamatokendataarraynative-single-uint64" class="md-nav__link">
    llama_sample_min_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_min_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_28" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_tail_freesafellamacontexthandle-llamatokendataarraynative-single-uint64" class="md-nav__link">
    llama_sample_tail_free(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_tail_free(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_29" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_typicalsafellamacontexthandle-llamatokendataarraynative-single-uint64" class="md-nav__link">
    llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_30" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_typicalsafellamacontexthandle-llamatokendataarraynative-single-single-single" class="md-nav__link">
    llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Single)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, Single, Single)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_31" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_tempsafellamacontexthandle-llamatokendataarraynative-single" class="md-nav__link">
    llama_sample_temp(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_temp(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_32" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_embeddingssafellamacontexthandle" class="md-nav__link">
    llama_get_embeddings(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_embeddings(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_33" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_17" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_chat_apply_templatesafellamamodelhandle-char-llamachatmessage-intptr-boolean-char-int32" class="md-nav__link">
    llama_chat_apply_template(SafeLlamaModelHandle, Char, LLamaChatMessage, IntPtr, Boolean, Char*, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_chat_apply_template(SafeLlamaModelHandle, Char, LLamaChatMessage, IntPtr, Boolean, Char*, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_34" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_18" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_bossafellamamodelhandle" class="md-nav__link">
    llama_token_bos(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_bos(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_35" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_19" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_eossafellamamodelhandle" class="md-nav__link">
    llama_token_eos(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_eos(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_36" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_20" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_nlsafellamamodelhandle" class="md-nav__link">
    llama_token_nl(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_nl(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_37" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_21" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_add_bos_tokensafellamamodelhandle" class="md-nav__link">
    llama_add_bos_token(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_add_bos_token(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_38" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_22" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_add_eos_tokensafellamamodelhandle" class="md-nav__link">
    llama_add_eos_token(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_add_eos_token(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_39" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_23" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_prefixsafellamamodelhandle" class="md-nav__link">
    llama_token_prefix(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_prefix(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_40" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_24" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_middlesafellamamodelhandle" class="md-nav__link">
    llama_token_middle(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_middle(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_41" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_25" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_suffixsafellamamodelhandle" class="md-nav__link">
    llama_token_suffix(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_suffix(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_42" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_26" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_eotsafellamamodelhandle" class="md-nav__link">
    llama_token_eot(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_eot(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_43" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_27" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_print_timingssafellamacontexthandle" class="md-nav__link">
    llama_print_timings(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_print_timings(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_44" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_reset_timingssafellamacontexthandle" class="md-nav__link">
    llama_reset_timings(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_reset_timings(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_45" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_print_system_info" class="md-nav__link">
    llama_print_system_info()
  </a>
  
    <nav class="md-nav" aria-label="llama_print_system_info()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_28" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_to_piecesafellamamodelhandle-llamatoken-spanbyte" class="md-nav__link">
    llama_token_to_piece(SafeLlamaModelHandle, LLamaToken, Span&lt;Byte&gt;)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_to_piece(SafeLlamaModelHandle, LLamaToken, Span<Byte>)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_46" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_29" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_tokenizesafellamamodelhandle-byte-int32-llamatoken-int32-boolean-boolean" class="md-nav__link">
    llama_tokenize(SafeLlamaModelHandle, Byte, Int32, LLamaToken, Int32, Boolean, Boolean)
  </a>
  
    <nav class="md-nav" aria-label="llama_tokenize(SafeLlamaModelHandle, Byte, Int32, LLamaToken, Int32, Boolean, Boolean)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_47" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_30" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_log_setllamalogcallback" class="md-nav__link">
    llama_log_set(LLamaLogCallback)
  </a>
  
    <nav class="md-nav" aria-label="llama_log_set(LLamaLogCallback)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_48" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_clearsafellamacontexthandle" class="md-nav__link">
    llama_kv_cache_clear(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_clear(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_49" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_rmsafellamacontexthandle-llamaseqid-llamapos-llamapos" class="md-nav__link">
    llama_kv_cache_seq_rm(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_rm(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_50" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_cpsafellamacontexthandle-llamaseqid-llamaseqid-llamapos-llamapos" class="md-nav__link">
    llama_kv_cache_seq_cp(SafeLLamaContextHandle, LLamaSeqId, LLamaSeqId, LLamaPos, LLamaPos)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_cp(SafeLLamaContextHandle, LLamaSeqId, LLamaSeqId, LLamaPos, LLamaPos)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_51" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_keepsafellamacontexthandle-llamaseqid" class="md-nav__link">
    llama_kv_cache_seq_keep(SafeLLamaContextHandle, LLamaSeqId)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_keep(SafeLLamaContextHandle, LLamaSeqId)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_52" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_addsafellamacontexthandle-llamaseqid-llamapos-llamapos-int32" class="md-nav__link">
    llama_kv_cache_seq_add(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_add(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_53" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_divsafellamacontexthandle-llamaseqid-llamapos-llamapos-int32" class="md-nav__link">
    llama_kv_cache_seq_div(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_div(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_54" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_pos_maxsafellamacontexthandle-llamaseqid" class="md-nav__link">
    llama_kv_cache_seq_pos_max(SafeLLamaContextHandle, LLamaSeqId)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_pos_max(SafeLLamaContextHandle, LLamaSeqId)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_55" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_31" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_defragsafellamacontexthandle" class="md-nav__link">
    llama_kv_cache_defrag(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_defrag(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_56" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_32" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_updatesafellamacontexthandle" class="md-nav__link">
    llama_kv_cache_update(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_update(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_57" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_batch_initint32-int32-int32" class="md-nav__link">
    llama_batch_init(Int32, Int32, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_batch_init(Int32, Int32, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_58" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_33" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_batch_freellamanativebatch" class="md-nav__link">
    llama_batch_free(LLamaNativeBatch)
  </a>
  
    <nav class="md-nav" aria-label="llama_batch_free(LLamaNativeBatch)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_59" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_decodesafellamacontexthandle-llamanativebatch" class="md-nav__link">
    llama_decode(SafeLLamaContextHandle, LLamaNativeBatch)
  </a>
  
    <nav class="md-nav" aria-label="llama_decode(SafeLLamaContextHandle, LLamaNativeBatch)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_60" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_34" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_view_initsafellamacontexthandle-int32" class="md-nav__link">
    llama_kv_cache_view_init(SafeLLamaContextHandle, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_view_init(SafeLLamaContextHandle, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_61" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_35" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_view_freellamakvcacheview" class="md-nav__link">
    llama_kv_cache_view_free(LLamaKvCacheView&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_view_free(LLamaKvCacheView&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_62" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_view_updatesafellamacontexthandle-llamakvcacheview" class="md-nav__link">
    llama_kv_cache_view_update(SafeLLamaContextHandle, LLamaKvCacheView&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_view_update(SafeLLamaContextHandle, LLamaKvCacheView&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_63" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_kv_cache_token_countsafellamacontexthandle" class="md-nav__link">
    llama_get_kv_cache_token_count(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_kv_cache_token_count(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_64" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_36" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_kv_cache_used_cellssafellamacontexthandle" class="md-nav__link">
    llama_get_kv_cache_used_cells(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_kv_cache_used_cells(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_65" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_37" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_beam_searchsafellamacontexthandle-llamabeamsearchcallback-intptr-uint64-int32-int32-int32" class="md-nav__link">
    llama_beam_search(SafeLLamaContextHandle, LLamaBeamSearchCallback, IntPtr, UInt64, Int32, Int32, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_beam_search(SafeLLamaContextHandle, LLamaBeamSearchCallback, IntPtr, UInt64, Int32, Int32, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_66" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_empty_call" class="md-nav__link">
    llama_empty_call()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_max_devices" class="md-nav__link">
    llama_max_devices()
  </a>
  
    <nav class="md-nav" aria-label="llama_max_devices()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_38" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_model_default_params" class="md-nav__link">
    llama_model_default_params()
  </a>
  
    <nav class="md-nav" aria-label="llama_model_default_params()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_39" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_context_default_params" class="md-nav__link">
    llama_context_default_params()
  </a>
  
    <nav class="md-nav" aria-label="llama_context_default_params()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_40" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_model_quantize_default_params" class="md-nav__link">
    llama_model_quantize_default_params()
  </a>
  
    <nav class="md-nav" aria-label="llama_model_quantize_default_params()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_41" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_supports_mmap" class="md-nav__link">
    llama_supports_mmap()
  </a>
  
    <nav class="md-nav" aria-label="llama_supports_mmap()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_42" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_supports_mlock" class="md-nav__link">
    llama_supports_mlock()
  </a>
  
    <nav class="md-nav" aria-label="llama_supports_mlock()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_43" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_supports_gpu_offload" class="md-nav__link">
    llama_supports_gpu_offload()
  </a>
  
    <nav class="md-nav" aria-label="llama_supports_gpu_offload()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_44" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_set_rng_seedsafellamacontexthandle-uint32" class="md-nav__link">
    llama_set_rng_seed(SafeLLamaContextHandle, UInt32)
  </a>
  
    <nav class="md-nav" aria-label="llama_set_rng_seed(SafeLLamaContextHandle, UInt32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_67" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_state_sizesafellamacontexthandle" class="md-nav__link">
    llama_get_state_size(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_state_size(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_68" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_45" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_copy_state_datasafellamacontexthandle-byte" class="md-nav__link">
    llama_copy_state_data(SafeLLamaContextHandle, Byte*)
  </a>
  
    <nav class="md-nav" aria-label="llama_copy_state_data(SafeLLamaContextHandle, Byte*)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_69" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_46" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_set_state_datasafellamacontexthandle-byte" class="md-nav__link">
    llama_set_state_data(SafeLLamaContextHandle, Byte*)
  </a>
  
    <nav class="md-nav" aria-label="llama_set_state_data(SafeLLamaContextHandle, Byte*)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_70" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_47" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_load_session_filesafellamacontexthandle-string-llamatoken-uint64-uint64" class="md-nav__link">
    llama_load_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64, UInt64&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_load_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64, UInt64&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_71" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_48" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_save_session_filesafellamacontexthandle-string-llamatoken-uint64" class="md-nav__link">
    llama_save_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_save_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_72" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_49" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_get_textsafellamamodelhandle-llamatoken" class="md-nav__link">
    llama_token_get_text(SafeLlamaModelHandle, LLamaToken)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_get_text(SafeLlamaModelHandle, LLamaToken)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_73" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_50" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_get_scoresafellamamodelhandle-llamatoken" class="md-nav__link">
    llama_token_get_score(SafeLlamaModelHandle, LLamaToken)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_get_score(SafeLlamaModelHandle, LLamaToken)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_74" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_51" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_get_typesafellamamodelhandle-llamatoken" class="md-nav__link">
    llama_token_get_type(SafeLlamaModelHandle, LLamaToken)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_get_type(SafeLlamaModelHandle, LLamaToken)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_75" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_52" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_n_ctxsafellamacontexthandle" class="md-nav__link">
    llama_n_ctx(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_n_ctx(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_76" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_53" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_n_batchsafellamacontexthandle" class="md-nav__link">
    llama_n_batch(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_n_batch(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_77" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_54" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_logitssafellamacontexthandle" class="md-nav__link">
    llama_get_logits(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_logits(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_78" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_55" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_logits_ithsafellamacontexthandle-int32" class="md-nav__link">
    llama_get_logits_ith(SafeLLamaContextHandle, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_logits_ith(SafeLLamaContextHandle, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_79" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_56" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_embeddings_ithsafellamacontexthandle-int32" class="md-nav__link">
    llama_get_embeddings_ith(SafeLLamaContextHandle, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_embeddings_ith(SafeLLamaContextHandle, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_80" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_57" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.nativelibraryconfig/" class="md-nav__link">
        llama.native.nativelibraryconfig
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.ropescalingtype/" class="md-nav__link">
        llama.native.ropescalingtype
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.safellamacontexthandle/" class="md-nav__link">
        llama.native.safellamacontexthandle
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.safellamagrammarhandle/" class="md-nav__link">
        llama.native.safellamagrammarhandle
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.safellamahandlebase/" class="md-nav__link">
        llama.native.safellamahandlebase
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.safellamamodelhandle/" class="md-nav__link">
        llama.native.safellamamodelhandle
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.safellavaimageembedhandle/" class="md-nav__link">
        llama.native.safellavaimageembedhandle
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.native.safellavamodelhandle/" class="md-nav__link">
        llama.native.safellavamodelhandle
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.quantizer/" class="md-nav__link">
        llama.quantizer
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.sampling.basesamplingpipeline/" class="md-nav__link">
        llama.sampling.basesamplingpipeline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.sampling.defaultsamplingpipeline/" class="md-nav__link">
        llama.sampling.defaultsamplingpipeline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.sampling.greedysamplingpipeline/" class="md-nav__link">
        llama.sampling.greedysamplingpipeline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.sampling.isamplingpipeline/" class="md-nav__link">
        llama.sampling.isamplingpipeline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.sampling.isamplingpipelineextensions/" class="md-nav__link">
        llama.sampling.isamplingpipelineextensions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.sampling.mirostate2samplingpipeline/" class="md-nav__link">
        llama.sampling.mirostate2samplingpipeline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.sampling.mirostatesamplingpipeline/" class="md-nav__link">
        llama.sampling.mirostatesamplingpipeline
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.sessionstate/" class="md-nav__link">
        llama.sessionstate
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.streamingtokendecoder/" class="md-nav__link">
        llama.streamingtokendecoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.chatcompletion/" class="md-nav__link">
        llama.types.chatcompletion
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.chatcompletionchoice/" class="md-nav__link">
        llama.types.chatcompletionchoice
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.chatcompletionchunk/" class="md-nav__link">
        llama.types.chatcompletionchunk
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.chatcompletionchunkchoice/" class="md-nav__link">
        llama.types.chatcompletionchunkchoice
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.chatcompletionchunkdelta/" class="md-nav__link">
        llama.types.chatcompletionchunkdelta
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.chatcompletionmessage/" class="md-nav__link">
        llama.types.chatcompletionmessage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.chatmessagerecord/" class="md-nav__link">
        llama.types.chatmessagerecord
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.chatrole/" class="md-nav__link">
        llama.types.chatrole
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.completion/" class="md-nav__link">
        llama.types.completion
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.completionchoice/" class="md-nav__link">
        llama.types.completionchoice
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.completionchunk/" class="md-nav__link">
        llama.types.completionchunk
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.completionlogprobs/" class="md-nav__link">
        llama.types.completionlogprobs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.completionusage/" class="md-nav__link">
        llama.types.completionusage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.embedding/" class="md-nav__link">
        llama.types.embedding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.embeddingdata/" class="md-nav__link">
        llama.types.embeddingdata
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../llama.types.embeddingusage/" class="md-nav__link">
        llama.types.embeddingusage
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../logger/" class="md-nav__link">
        logger
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
    <nav class="md-nav" aria-label="Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama_sample_token_mirostatsafellamacontexthandle-llamatokendataarraynative-single-single-int32-single" class="md-nav__link">
    llama_sample_token_mirostat(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Int32, Single&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_token_mirostat(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, Single, Int32, Single&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_token_mirostat_v2safellamacontexthandle-llamatokendataarraynative-single-single-single" class="md-nav__link">
    llama_sample_token_mirostat_v2(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Single&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_token_mirostat_v2(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, Single, Single&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_1" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_1" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_token_greedysafellamacontexthandle-llamatokendataarraynative" class="md-nav__link">
    llama_sample_token_greedy(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_token_greedy(SafeLLamaContextHandle, LLamaTokenDataArrayNative&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_2" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_2" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_tokensafellamacontexthandle-llamatokendataarraynative" class="md-nav__link">
    llama_sample_token(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_token(SafeLLamaContextHandle, LLamaTokenDataArrayNative&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_3" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_3" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_embeddingsg__llama_get_embeddings_native30_0safellamacontexthandle" class="md-nav__link">
    &lt;llama_get_embeddings&gt;g__llama_get_embeddings_native|30_0(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="<llama_get_embeddings>g__llama_get_embeddings_native|30_0(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_4" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_4" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_to_pieceg__llama_token_to_piece_native44_0safellamamodelhandle-llamatoken-byte-int32" class="md-nav__link">
    &lt;llama_token_to_piece&gt;g__llama_token_to_piece_native|44_0(SafeLlamaModelHandle, LLamaToken, Byte*, Int32)
  </a>
  
    <nav class="md-nav" aria-label="<llama_token_to_piece>g__llama_token_to_piece_native|44_0(SafeLlamaModelHandle, LLamaToken, Byte*, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_5" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_5" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tryloadlibrariesg__tryload84_0string" class="md-nav__link">
    &lt;TryLoadLibraries&gt;g__TryLoad|84_0(String)
  </a>
  
    <nav class="md-nav" aria-label="<TryLoadLibraries>g__TryLoad|84_0(String)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_6" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_6" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tryloadlibrariesg__tryfindpath84_1string-c__displayclass84_0" class="md-nav__link">
    &lt;TryLoadLibraries&gt;g__TryFindPath|84_1(String, &lt;&gt;c__DisplayClass84_0&amp;)
  </a>
  
    <nav class="md-nav" aria-label="<TryLoadLibraries>g__TryFindPath|84_1(String, <>c__DisplayClass84_0&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_7" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_7" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_set_n_threadssafellamacontexthandle-uint32-uint32" class="md-nav__link">
    llama_set_n_threads(SafeLLamaContextHandle, UInt32, UInt32)
  </a>
  
    <nav class="md-nav" aria-label="llama_set_n_threads(SafeLLamaContextHandle, UInt32, UInt32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_8" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_vocab_typesafellamamodelhandle" class="md-nav__link">
    llama_vocab_type(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_vocab_type(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_9" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_8" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_rope_typesafellamamodelhandle" class="md-nav__link">
    llama_rope_type(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_rope_type(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_10" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_9" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_grammar_initllamagrammarelement-uint64-uint64" class="md-nav__link">
    llama_grammar_init(LLamaGrammarElement, UInt64, UInt64)**
  </a>
  
    <nav class="md-nav" aria-label="llama_grammar_init(LLamaGrammarElement, UInt64, UInt64)**">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_11" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_10" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_grammar_freeintptr" class="md-nav__link">
    llama_grammar_free(IntPtr)
  </a>
  
    <nav class="md-nav" aria-label="llama_grammar_free(IntPtr)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_12" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_grammar_copysafellamagrammarhandle" class="md-nav__link">
    llama_grammar_copy(SafeLLamaGrammarHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_grammar_copy(SafeLLamaGrammarHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_13" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_11" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_grammarsafellamacontexthandle-llamatokendataarraynative-safellamagrammarhandle" class="md-nav__link">
    llama_sample_grammar(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, SafeLLamaGrammarHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_grammar(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, SafeLLamaGrammarHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_14" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_grammar_accept_tokensafellamacontexthandle-safellamagrammarhandle-llamatoken" class="md-nav__link">
    llama_grammar_accept_token(SafeLLamaContextHandle, SafeLLamaGrammarHandle, LLamaToken)
  </a>
  
    <nav class="md-nav" aria-label="llama_grammar_accept_token(SafeLLamaContextHandle, SafeLLamaGrammarHandle, LLamaToken)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_15" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava_validate_embed_sizesafellamacontexthandle-safellavamodelhandle" class="md-nav__link">
    llava_validate_embed_size(SafeLLamaContextHandle, SafeLlavaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llava_validate_embed_size(SafeLLamaContextHandle, SafeLlavaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_16" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_12" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava_image_embed_make_with_bytessafellavamodelhandle-int32-byte-int32" class="md-nav__link">
    llava_image_embed_make_with_bytes(SafeLlavaModelHandle, Int32, Byte[], Int32)
  </a>
  
    <nav class="md-nav" aria-label="llava_image_embed_make_with_bytes(SafeLlavaModelHandle, Int32, Byte[], Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_17" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_13" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava_image_embed_make_with_filenamesafellavamodelhandle-int32-string" class="md-nav__link">
    llava_image_embed_make_with_filename(SafeLlavaModelHandle, Int32, String)
  </a>
  
    <nav class="md-nav" aria-label="llava_image_embed_make_with_filename(SafeLlavaModelHandle, Int32, String)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_18" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_14" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava_image_embed_freeintptr" class="md-nav__link">
    llava_image_embed_free(IntPtr)
  </a>
  
    <nav class="md-nav" aria-label="llava_image_embed_free(IntPtr)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_19" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llava_eval_image_embedsafellamacontexthandle-safellavaimageembedhandle-int32-int32" class="md-nav__link">
    llava_eval_image_embed(SafeLLamaContextHandle, SafeLlavaImageEmbedHandle, Int32, Int32&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llava_eval_image_embed(SafeLLamaContextHandle, SafeLlavaImageEmbedHandle, Int32, Int32&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_20" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_15" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_model_quantizestring-string-llamamodelquantizeparams" class="md-nav__link">
    llama_model_quantize(String, String, LLamaModelQuantizeParams*)
  </a>
  
    <nav class="md-nav" aria-label="llama_model_quantize(String, String, LLamaModelQuantizeParams*)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_21" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_16" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_repetition_penaltiessafellamacontexthandle-llamatokendataarraynative-llamatoken-uint64-single-single-single" class="md-nav__link">
    llama_sample_repetition_penalties(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, LLamaToken*, UInt64, Single, Single, Single)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_repetition_penalties(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, LLamaToken*, UInt64, Single, Single, Single)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_22" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_apply_guidancesafellamacontexthandle-spansingle-readonlyspansingle-single" class="md-nav__link">
    llama_sample_apply_guidance(SafeLLamaContextHandle, Span&lt;Single&gt;, ReadOnlySpan&lt;Single&gt;, Single)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_apply_guidance(SafeLLamaContextHandle, Span<Single>, ReadOnlySpan<Single>, Single)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_23" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_apply_guidancesafellamacontexthandle-single-single-single" class="md-nav__link">
    llama_sample_apply_guidance(SafeLLamaContextHandle, Single, Single, Single)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_apply_guidance(SafeLLamaContextHandle, Single, Single, Single)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_24" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_softmaxsafellamacontexthandle-llamatokendataarraynative" class="md-nav__link">
    llama_sample_softmax(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_softmax(SafeLLamaContextHandle, LLamaTokenDataArrayNative&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_25" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_top_ksafellamacontexthandle-llamatokendataarraynative-int32-uint64" class="md-nav__link">
    llama_sample_top_k(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Int32, UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_top_k(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Int32, UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_26" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_top_psafellamacontexthandle-llamatokendataarraynative-single-uint64" class="md-nav__link">
    llama_sample_top_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_top_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_27" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_min_psafellamacontexthandle-llamatokendataarraynative-single-uint64" class="md-nav__link">
    llama_sample_min_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_min_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_28" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_tail_freesafellamacontexthandle-llamatokendataarraynative-single-uint64" class="md-nav__link">
    llama_sample_tail_free(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_tail_free(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_29" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_typicalsafellamacontexthandle-llamatokendataarraynative-single-uint64" class="md-nav__link">
    llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_30" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_typicalsafellamacontexthandle-llamatokendataarraynative-single-single-single" class="md-nav__link">
    llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Single)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single, Single, Single)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_31" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_sample_tempsafellamacontexthandle-llamatokendataarraynative-single" class="md-nav__link">
    llama_sample_temp(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single)
  </a>
  
    <nav class="md-nav" aria-label="llama_sample_temp(SafeLLamaContextHandle, LLamaTokenDataArrayNative&, Single)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_32" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_embeddingssafellamacontexthandle" class="md-nav__link">
    llama_get_embeddings(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_embeddings(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_33" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_17" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_chat_apply_templatesafellamamodelhandle-char-llamachatmessage-intptr-boolean-char-int32" class="md-nav__link">
    llama_chat_apply_template(SafeLlamaModelHandle, Char, LLamaChatMessage, IntPtr, Boolean, Char*, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_chat_apply_template(SafeLlamaModelHandle, Char, LLamaChatMessage, IntPtr, Boolean, Char*, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_34" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_18" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_bossafellamamodelhandle" class="md-nav__link">
    llama_token_bos(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_bos(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_35" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_19" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_eossafellamamodelhandle" class="md-nav__link">
    llama_token_eos(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_eos(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_36" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_20" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_nlsafellamamodelhandle" class="md-nav__link">
    llama_token_nl(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_nl(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_37" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_21" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_add_bos_tokensafellamamodelhandle" class="md-nav__link">
    llama_add_bos_token(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_add_bos_token(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_38" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_22" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_add_eos_tokensafellamamodelhandle" class="md-nav__link">
    llama_add_eos_token(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_add_eos_token(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_39" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_23" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_prefixsafellamamodelhandle" class="md-nav__link">
    llama_token_prefix(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_prefix(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_40" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_24" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_middlesafellamamodelhandle" class="md-nav__link">
    llama_token_middle(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_middle(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_41" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_25" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_suffixsafellamamodelhandle" class="md-nav__link">
    llama_token_suffix(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_suffix(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_42" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_26" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_eotsafellamamodelhandle" class="md-nav__link">
    llama_token_eot(SafeLlamaModelHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_eot(SafeLlamaModelHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_43" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_27" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_print_timingssafellamacontexthandle" class="md-nav__link">
    llama_print_timings(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_print_timings(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_44" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_reset_timingssafellamacontexthandle" class="md-nav__link">
    llama_reset_timings(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_reset_timings(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_45" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_print_system_info" class="md-nav__link">
    llama_print_system_info()
  </a>
  
    <nav class="md-nav" aria-label="llama_print_system_info()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_28" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_to_piecesafellamamodelhandle-llamatoken-spanbyte" class="md-nav__link">
    llama_token_to_piece(SafeLlamaModelHandle, LLamaToken, Span&lt;Byte&gt;)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_to_piece(SafeLlamaModelHandle, LLamaToken, Span<Byte>)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_46" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_29" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_tokenizesafellamamodelhandle-byte-int32-llamatoken-int32-boolean-boolean" class="md-nav__link">
    llama_tokenize(SafeLlamaModelHandle, Byte, Int32, LLamaToken, Int32, Boolean, Boolean)
  </a>
  
    <nav class="md-nav" aria-label="llama_tokenize(SafeLlamaModelHandle, Byte, Int32, LLamaToken, Int32, Boolean, Boolean)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_47" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_30" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_log_setllamalogcallback" class="md-nav__link">
    llama_log_set(LLamaLogCallback)
  </a>
  
    <nav class="md-nav" aria-label="llama_log_set(LLamaLogCallback)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_48" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_clearsafellamacontexthandle" class="md-nav__link">
    llama_kv_cache_clear(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_clear(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_49" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_rmsafellamacontexthandle-llamaseqid-llamapos-llamapos" class="md-nav__link">
    llama_kv_cache_seq_rm(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_rm(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_50" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_cpsafellamacontexthandle-llamaseqid-llamaseqid-llamapos-llamapos" class="md-nav__link">
    llama_kv_cache_seq_cp(SafeLLamaContextHandle, LLamaSeqId, LLamaSeqId, LLamaPos, LLamaPos)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_cp(SafeLLamaContextHandle, LLamaSeqId, LLamaSeqId, LLamaPos, LLamaPos)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_51" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_keepsafellamacontexthandle-llamaseqid" class="md-nav__link">
    llama_kv_cache_seq_keep(SafeLLamaContextHandle, LLamaSeqId)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_keep(SafeLLamaContextHandle, LLamaSeqId)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_52" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_addsafellamacontexthandle-llamaseqid-llamapos-llamapos-int32" class="md-nav__link">
    llama_kv_cache_seq_add(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_add(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_53" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_divsafellamacontexthandle-llamaseqid-llamapos-llamapos-int32" class="md-nav__link">
    llama_kv_cache_seq_div(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_div(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_54" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_seq_pos_maxsafellamacontexthandle-llamaseqid" class="md-nav__link">
    llama_kv_cache_seq_pos_max(SafeLLamaContextHandle, LLamaSeqId)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_seq_pos_max(SafeLLamaContextHandle, LLamaSeqId)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_55" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_31" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_defragsafellamacontexthandle" class="md-nav__link">
    llama_kv_cache_defrag(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_defrag(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_56" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_32" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_updatesafellamacontexthandle" class="md-nav__link">
    llama_kv_cache_update(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_update(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_57" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_batch_initint32-int32-int32" class="md-nav__link">
    llama_batch_init(Int32, Int32, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_batch_init(Int32, Int32, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_58" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_33" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_batch_freellamanativebatch" class="md-nav__link">
    llama_batch_free(LLamaNativeBatch)
  </a>
  
    <nav class="md-nav" aria-label="llama_batch_free(LLamaNativeBatch)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_59" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_decodesafellamacontexthandle-llamanativebatch" class="md-nav__link">
    llama_decode(SafeLLamaContextHandle, LLamaNativeBatch)
  </a>
  
    <nav class="md-nav" aria-label="llama_decode(SafeLLamaContextHandle, LLamaNativeBatch)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_60" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_34" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_view_initsafellamacontexthandle-int32" class="md-nav__link">
    llama_kv_cache_view_init(SafeLLamaContextHandle, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_view_init(SafeLLamaContextHandle, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_61" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_35" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_view_freellamakvcacheview" class="md-nav__link">
    llama_kv_cache_view_free(LLamaKvCacheView&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_view_free(LLamaKvCacheView&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_62" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_kv_cache_view_updatesafellamacontexthandle-llamakvcacheview" class="md-nav__link">
    llama_kv_cache_view_update(SafeLLamaContextHandle, LLamaKvCacheView&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_kv_cache_view_update(SafeLLamaContextHandle, LLamaKvCacheView&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_63" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_kv_cache_token_countsafellamacontexthandle" class="md-nav__link">
    llama_get_kv_cache_token_count(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_kv_cache_token_count(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_64" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_36" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_kv_cache_used_cellssafellamacontexthandle" class="md-nav__link">
    llama_get_kv_cache_used_cells(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_kv_cache_used_cells(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_65" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_37" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_beam_searchsafellamacontexthandle-llamabeamsearchcallback-intptr-uint64-int32-int32-int32" class="md-nav__link">
    llama_beam_search(SafeLLamaContextHandle, LLamaBeamSearchCallback, IntPtr, UInt64, Int32, Int32, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_beam_search(SafeLLamaContextHandle, LLamaBeamSearchCallback, IntPtr, UInt64, Int32, Int32, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_66" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_empty_call" class="md-nav__link">
    llama_empty_call()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_max_devices" class="md-nav__link">
    llama_max_devices()
  </a>
  
    <nav class="md-nav" aria-label="llama_max_devices()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_38" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_model_default_params" class="md-nav__link">
    llama_model_default_params()
  </a>
  
    <nav class="md-nav" aria-label="llama_model_default_params()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_39" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_context_default_params" class="md-nav__link">
    llama_context_default_params()
  </a>
  
    <nav class="md-nav" aria-label="llama_context_default_params()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_40" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_model_quantize_default_params" class="md-nav__link">
    llama_model_quantize_default_params()
  </a>
  
    <nav class="md-nav" aria-label="llama_model_quantize_default_params()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_41" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_supports_mmap" class="md-nav__link">
    llama_supports_mmap()
  </a>
  
    <nav class="md-nav" aria-label="llama_supports_mmap()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_42" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_supports_mlock" class="md-nav__link">
    llama_supports_mlock()
  </a>
  
    <nav class="md-nav" aria-label="llama_supports_mlock()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_43" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_supports_gpu_offload" class="md-nav__link">
    llama_supports_gpu_offload()
  </a>
  
    <nav class="md-nav" aria-label="llama_supports_gpu_offload()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#returns_44" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_set_rng_seedsafellamacontexthandle-uint32" class="md-nav__link">
    llama_set_rng_seed(SafeLLamaContextHandle, UInt32)
  </a>
  
    <nav class="md-nav" aria-label="llama_set_rng_seed(SafeLLamaContextHandle, UInt32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_67" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_state_sizesafellamacontexthandle" class="md-nav__link">
    llama_get_state_size(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_state_size(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_68" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_45" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_copy_state_datasafellamacontexthandle-byte" class="md-nav__link">
    llama_copy_state_data(SafeLLamaContextHandle, Byte*)
  </a>
  
    <nav class="md-nav" aria-label="llama_copy_state_data(SafeLLamaContextHandle, Byte*)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_69" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_46" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_set_state_datasafellamacontexthandle-byte" class="md-nav__link">
    llama_set_state_data(SafeLLamaContextHandle, Byte*)
  </a>
  
    <nav class="md-nav" aria-label="llama_set_state_data(SafeLLamaContextHandle, Byte*)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_70" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_47" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_load_session_filesafellamacontexthandle-string-llamatoken-uint64-uint64" class="md-nav__link">
    llama_load_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64, UInt64&amp;)
  </a>
  
    <nav class="md-nav" aria-label="llama_load_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64, UInt64&)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_71" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_48" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_save_session_filesafellamacontexthandle-string-llamatoken-uint64" class="md-nav__link">
    llama_save_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64)
  </a>
  
    <nav class="md-nav" aria-label="llama_save_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_72" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_49" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_get_textsafellamamodelhandle-llamatoken" class="md-nav__link">
    llama_token_get_text(SafeLlamaModelHandle, LLamaToken)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_get_text(SafeLlamaModelHandle, LLamaToken)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_73" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_50" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_get_scoresafellamamodelhandle-llamatoken" class="md-nav__link">
    llama_token_get_score(SafeLlamaModelHandle, LLamaToken)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_get_score(SafeLlamaModelHandle, LLamaToken)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_74" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_51" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_token_get_typesafellamamodelhandle-llamatoken" class="md-nav__link">
    llama_token_get_type(SafeLlamaModelHandle, LLamaToken)
  </a>
  
    <nav class="md-nav" aria-label="llama_token_get_type(SafeLlamaModelHandle, LLamaToken)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_75" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_52" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_n_ctxsafellamacontexthandle" class="md-nav__link">
    llama_n_ctx(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_n_ctx(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_76" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_53" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_n_batchsafellamacontexthandle" class="md-nav__link">
    llama_n_batch(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_n_batch(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_77" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_54" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_logitssafellamacontexthandle" class="md-nav__link">
    llama_get_logits(SafeLLamaContextHandle)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_logits(SafeLLamaContextHandle)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_78" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_55" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_logits_ithsafellamacontexthandle-int32" class="md-nav__link">
    llama_get_logits_ith(SafeLLamaContextHandle, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_logits_ith(SafeLLamaContextHandle, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_79" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_56" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama_get_embeddings_ithsafellamacontexthandle-int32" class="md-nav__link">
    llama_get_embeddings_ith(SafeLLamaContextHandle, Int32)
  </a>
  
    <nav class="md-nav" aria-label="llama_get_embeddings_ith(SafeLLamaContextHandle, Int32)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameters_80" class="md-nav__link">
    Parameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#returns_57" class="md-nav__link">
    Returns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="nativeapi">NativeApi</h1>
<p>Namespace: LLama.Native</p>
<p>Direct translation of the llama.cpp API</p>
<pre><code class="language-csharp">public static class NativeApi
</code></pre>
<p>Inheritance <a href="https://docs.microsoft.com/en-us/dotnet/api/system.object">Object</a>  <a href="./">NativeApi</a></p>
<h2 id="methods">Methods</h2>
<h3 id="llama_sample_token_mirostatsafellamacontexthandle-llamatokendataarraynative-single-single-int32-single"><strong>llama_sample_token_mirostat(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Int32, Single&amp;)</strong></h3>
<p>Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.</p>
<pre><code class="language-csharp">public static LLamaToken llama_sample_token_mirostat(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float tau, float eta, int m, Single&amp; mu)
</code></pre>
<h4 id="parameters">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
A vector of <code>llama_token_data</code> containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.</p>
<p><code>tau</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br>
The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</p>
<p><code>eta</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br>
The learning rate used to update <code>mu</code> based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause <code>mu</code> to be updated more quickly, while a smaller learning rate will result in slower updates.</p>
<p><code>m</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
The number of tokens considered in the estimation of <code>s_hat</code>. This is an arbitrary value that is used to calculate <code>s_hat</code>, which in turn helps to calculate the value of <code>k</code>. In the paper, they use <code>m = 100</code>, but you can experiment with different values to see how it affects the performance of the algorithm.</p>
<p><code>mu</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single&amp;">Single&amp;</a><br>
Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (<code>2 * tau</code>) and is updated in the algorithm based on the error between the target and observed surprisal.</p>
<h4 id="returns">Returns</h4>
<p><a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<h3 id="llama_sample_token_mirostat_v2safellamacontexthandle-llamatokendataarraynative-single-single-single"><strong>llama_sample_token_mirostat_v2(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Single&amp;)</strong></h3>
<p>Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.</p>
<pre><code class="language-csharp">public static LLamaToken llama_sample_token_mirostat_v2(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float tau, float eta, Single&amp; mu)
</code></pre>
<h4 id="parameters_1">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
A vector of <code>llama_token_data</code> containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.</p>
<p><code>tau</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br>
The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.</p>
<p><code>eta</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br>
The learning rate used to update <code>mu</code> based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause <code>mu</code> to be updated more quickly, while a smaller learning rate will result in slower updates.</p>
<p><code>mu</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single&amp;">Single&amp;</a><br>
Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (<code>2 * tau</code>) and is updated in the algorithm based on the error between the target and observed surprisal.</p>
<h4 id="returns_1">Returns</h4>
<p><a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<h3 id="llama_sample_token_greedysafellamacontexthandle-llamatokendataarraynative"><strong>llama_sample_token_greedy(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)</strong></h3>
<p>Selects the token with the highest probability.</p>
<pre><code class="language-csharp">public static LLamaToken llama_sample_token_greedy(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates)
</code></pre>
<h4 id="parameters_2">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
Pointer to LLamaTokenDataArray</p>
<h4 id="returns_2">Returns</h4>
<p><a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<h3 id="llama_sample_tokensafellamacontexthandle-llamatokendataarraynative"><strong>llama_sample_token(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)</strong></h3>
<p>Randomly selects a token from the candidates based on their probabilities.</p>
<pre><code class="language-csharp">public static LLamaToken llama_sample_token(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates)
</code></pre>
<h4 id="parameters_3">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
Pointer to LLamaTokenDataArray</p>
<h4 id="returns_3">Returns</h4>
<p><a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<h3 id="llama_get_embeddingsg__llama_get_embeddings_native30_0safellamacontexthandle"><strong>&lt;llama_get_embeddings&gt;g__llama_get_embeddings_native|30_0(SafeLLamaContextHandle)</strong></h3>
<pre><code class="language-csharp">internal static Single* &lt;llama_get_embeddings&gt;g__llama_get_embeddings_native|30_0(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_4">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h4 id="returns_4">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.single*">Single*</a><br></p>
<h3 id="llama_token_to_pieceg__llama_token_to_piece_native44_0safellamamodelhandle-llamatoken-byte-int32"><strong>&lt;llama_token_to_piece&gt;g__llama_token_to_piece_native|44_0(SafeLlamaModelHandle, LLamaToken, Byte*, Int32)</strong></h3>
<pre><code class="language-csharp">internal static int &lt;llama_token_to_piece&gt;g__llama_token_to_piece_native|44_0(SafeLlamaModelHandle model, LLamaToken llamaToken, Byte* buffer, int length)
</code></pre>
<h4 id="parameters_5">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<p><code>llamaToken</code> <a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<p><code>buffer</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.byte*">Byte*</a><br></p>
<p><code>length</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h4 id="returns_5">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h3 id="tryloadlibrariesg__tryload84_0string"><strong>&lt;TryLoadLibraries&gt;g__TryLoad|84_0(String)</strong></h3>
<pre><code class="language-csharp">internal static IntPtr &lt;TryLoadLibraries&gt;g__TryLoad|84_0(string path)
</code></pre>
<h4 id="parameters_6">Parameters</h4>
<p><code>path</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.string">String</a><br></p>
<h4 id="returns_6">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.intptr">IntPtr</a><br></p>
<h3 id="tryloadlibrariesg__tryfindpath84_1string-c__displayclass84_0"><strong>&lt;TryLoadLibraries&gt;g__TryFindPath|84_1(String, &lt;&gt;c__DisplayClass84_0&amp;)</strong></h3>
<pre><code class="language-csharp">internal static string &lt;TryLoadLibraries&gt;g__TryFindPath|84_1(string filename, &lt;&gt;c__DisplayClass84_0&amp; )
</code></pre>
<h4 id="parameters_7">Parameters</h4>
<p><code>filename</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.string">String</a><br></p>
<p>`` <a href="./llama.native.nativeapi.&lt;&gt;c__displayclass84_0&amp;.md">&lt;&gt;c__DisplayClass84_0&amp;</a><br></p>
<h4 id="returns_7">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.string">String</a><br></p>
<h3 id="llama_set_n_threadssafellamacontexthandle-uint32-uint32"><strong>llama_set_n_threads(SafeLLamaContextHandle, UInt32, UInt32)</strong></h3>
<p>Set the number of threads used for decoding</p>
<pre><code class="language-csharp">public static void llama_set_n_threads(SafeLLamaContextHandle ctx, uint n_threads, uint n_threads_batch)
</code></pre>
<h4 id="parameters_8">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>n_threads</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint32">UInt32</a><br>
n_threads is the number of threads used for generation (single token)</p>
<p><code>n_threads_batch</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint32">UInt32</a><br>
n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)</p>
<h3 id="llama_vocab_typesafellamamodelhandle"><strong>llama_vocab_type(SafeLlamaModelHandle)</strong></h3>
<pre><code class="language-csharp">public static LLamaVocabType llama_vocab_type(SafeLlamaModelHandle model)
</code></pre>
<h4 id="parameters_9">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<h4 id="returns_8">Returns</h4>
<p><a href="../llama.native.llamavocabtype/">LLamaVocabType</a><br></p>
<h3 id="llama_rope_typesafellamamodelhandle"><strong>llama_rope_type(SafeLlamaModelHandle)</strong></h3>
<pre><code class="language-csharp">public static LLamaRopeType llama_rope_type(SafeLlamaModelHandle model)
</code></pre>
<h4 id="parameters_10">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<h4 id="returns_9">Returns</h4>
<p><a href="../llama.native.llamaropetype/">LLamaRopeType</a><br></p>
<h3 id="llama_grammar_initllamagrammarelement-uint64-uint64"><strong>llama_grammar_init(LLamaGrammarElement</strong>, UInt64, UInt64)**</h3>
<p>Create a new grammar from the given set of grammar rules</p>
<pre><code class="language-csharp">public static IntPtr llama_grammar_init(LLamaGrammarElement** rules, ulong n_rules, ulong start_rule_index)
</code></pre>
<h4 id="parameters_11">Parameters</h4>
<p><code>rules</code> <a href="./llama.native.llamagrammarelement**.md">LLamaGrammarElement**</a><br></p>
<p><code>n_rules</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br></p>
<p><code>start_rule_index</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br></p>
<h4 id="returns_10">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.intptr">IntPtr</a><br></p>
<h3 id="llama_grammar_freeintptr"><strong>llama_grammar_free(IntPtr)</strong></h3>
<p>Free all memory from the given SafeLLamaGrammarHandle</p>
<pre><code class="language-csharp">public static void llama_grammar_free(IntPtr grammar)
</code></pre>
<h4 id="parameters_12">Parameters</h4>
<p><code>grammar</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.intptr">IntPtr</a><br></p>
<h3 id="llama_grammar_copysafellamagrammarhandle"><strong>llama_grammar_copy(SafeLLamaGrammarHandle)</strong></h3>
<p>Create a copy of an existing grammar instance</p>
<pre><code class="language-csharp">public static IntPtr llama_grammar_copy(SafeLLamaGrammarHandle grammar)
</code></pre>
<h4 id="parameters_13">Parameters</h4>
<p><code>grammar</code> <a href="../llama.native.safellamagrammarhandle/">SafeLLamaGrammarHandle</a><br></p>
<h4 id="returns_11">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.intptr">IntPtr</a><br></p>
<h3 id="llama_sample_grammarsafellamacontexthandle-llamatokendataarraynative-safellamagrammarhandle"><strong>llama_sample_grammar(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, SafeLLamaGrammarHandle)</strong></h3>
<p>Apply constraints from grammar</p>
<pre><code class="language-csharp">public static void llama_sample_grammar(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, SafeLLamaGrammarHandle grammar)
</code></pre>
<h4 id="parameters_14">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br></p>
<p><code>grammar</code> <a href="../llama.native.safellamagrammarhandle/">SafeLLamaGrammarHandle</a><br></p>
<h3 id="llama_grammar_accept_tokensafellamacontexthandle-safellamagrammarhandle-llamatoken"><strong>llama_grammar_accept_token(SafeLLamaContextHandle, SafeLLamaGrammarHandle, LLamaToken)</strong></h3>
<p>Accepts the sampled token into the grammar</p>
<pre><code class="language-csharp">public static void llama_grammar_accept_token(SafeLLamaContextHandle ctx, SafeLLamaGrammarHandle grammar, LLamaToken token)
</code></pre>
<h4 id="parameters_15">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>grammar</code> <a href="../llama.native.safellamagrammarhandle/">SafeLLamaGrammarHandle</a><br></p>
<p><code>token</code> <a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<h3 id="llava_validate_embed_sizesafellamacontexthandle-safellavamodelhandle"><strong>llava_validate_embed_size(SafeLLamaContextHandle, SafeLlavaModelHandle)</strong></h3>
<p>Sanity check for clip &lt;-&gt; llava embed size match</p>
<pre><code class="language-csharp">public static bool llava_validate_embed_size(SafeLLamaContextHandle ctxLlama, SafeLlavaModelHandle ctxClip)
</code></pre>
<h4 id="parameters_16">Parameters</h4>
<p><code>ctxLlama</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br>
LLama Context</p>
<p><code>ctxClip</code> <a href="../llama.native.safellavamodelhandle/">SafeLlavaModelHandle</a><br>
Llava Model</p>
<h4 id="returns_12">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.boolean">Boolean</a><br>
True if validate successfully</p>
<h3 id="llava_image_embed_make_with_bytessafellavamodelhandle-int32-byte-int32"><strong>llava_image_embed_make_with_bytes(SafeLlavaModelHandle, Int32, Byte[], Int32)</strong></h3>
<p>Build an image embed from image file bytes</p>
<pre><code class="language-csharp">public static SafeLlavaImageEmbedHandle llava_image_embed_make_with_bytes(SafeLlavaModelHandle ctx_clip, int n_threads, Byte[] image_bytes, int image_bytes_length)
</code></pre>
<h4 id="parameters_17">Parameters</h4>
<p><code>ctx_clip</code> <a href="../llama.native.safellavamodelhandle/">SafeLlavaModelHandle</a><br>
SafeHandle to the Clip Model</p>
<p><code>n_threads</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
Number of threads</p>
<p><code>image_bytes</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.byte">Byte[]</a><br>
Binary image in jpeg format</p>
<p><code>image_bytes_length</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
Bytes lenght of the image</p>
<h4 id="returns_13">Returns</h4>
<p><a href="../llama.native.safellavaimageembedhandle/">SafeLlavaImageEmbedHandle</a><br>
SafeHandle to the Embeddings</p>
<h3 id="llava_image_embed_make_with_filenamesafellavamodelhandle-int32-string"><strong>llava_image_embed_make_with_filename(SafeLlavaModelHandle, Int32, String)</strong></h3>
<p>Build an image embed from a path to an image filename</p>
<pre><code class="language-csharp">public static SafeLlavaImageEmbedHandle llava_image_embed_make_with_filename(SafeLlavaModelHandle ctx_clip, int n_threads, string image_path)
</code></pre>
<h4 id="parameters_18">Parameters</h4>
<p><code>ctx_clip</code> <a href="../llama.native.safellavamodelhandle/">SafeLlavaModelHandle</a><br>
SafeHandle to the Clip Model</p>
<p><code>n_threads</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
Number of threads</p>
<p><code>image_path</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.string">String</a><br>
Image filename (jpeg) to generate embeddings</p>
<h4 id="returns_14">Returns</h4>
<p><a href="../llama.native.safellavaimageembedhandle/">SafeLlavaImageEmbedHandle</a><br>
SafeHandel to the embeddings</p>
<h3 id="llava_image_embed_freeintptr"><strong>llava_image_embed_free(IntPtr)</strong></h3>
<p>Free an embedding made with llava_image_embed_make_*</p>
<pre><code class="language-csharp">public static void llava_image_embed_free(IntPtr embed)
</code></pre>
<h4 id="parameters_19">Parameters</h4>
<p><code>embed</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.intptr">IntPtr</a><br>
Embeddings to release</p>
<h3 id="llava_eval_image_embedsafellamacontexthandle-safellavaimageembedhandle-int32-int32"><strong>llava_eval_image_embed(SafeLLamaContextHandle, SafeLlavaImageEmbedHandle, Int32, Int32&amp;)</strong></h3>
<p>Write the image represented by embed into the llama context with batch size n_batch, starting at context
 pos n_past. on completion, n_past points to the next position in the context after the image embed.</p>
<pre><code class="language-csharp">public static bool llava_eval_image_embed(SafeLLamaContextHandle ctx_llama, SafeLlavaImageEmbedHandle embed, int n_batch, Int32&amp; n_past)
</code></pre>
<h4 id="parameters_20">Parameters</h4>
<p><code>ctx_llama</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br>
Llama Context</p>
<p><code>embed</code> <a href="../llama.native.safellavaimageembedhandle/">SafeLlavaImageEmbedHandle</a><br>
Embedding handle</p>
<p><code>n_batch</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<p><code>n_past</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32&amp;">Int32&amp;</a><br></p>
<h4 id="returns_15">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.boolean">Boolean</a><br>
True on success</p>
<h3 id="llama_model_quantizestring-string-llamamodelquantizeparams"><strong>llama_model_quantize(String, String, LLamaModelQuantizeParams*)</strong></h3>
<p>Returns 0 on success</p>
<pre><code class="language-csharp">public static uint llama_model_quantize(string fname_inp, string fname_out, LLamaModelQuantizeParams* param)
</code></pre>
<h4 id="parameters_21">Parameters</h4>
<p><code>fname_inp</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.string">String</a><br></p>
<p><code>fname_out</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.string">String</a><br></p>
<p><code>param</code> <a href="./llama.native.llamamodelquantizeparams*.md">LLamaModelQuantizeParams*</a><br></p>
<h4 id="returns_16">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint32">UInt32</a><br>
Returns 0 on success</p>
<h3 id="llama_sample_repetition_penaltiessafellamacontexthandle-llamatokendataarraynative-llamatoken-uint64-single-single-single"><strong>llama_sample_repetition_penalties(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, LLamaToken*, UInt64, Single, Single, Single)</strong></h3>
<p>Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.
 Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.</p>
<pre><code class="language-csharp">public static void llama_sample_repetition_penalties(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, LLamaToken* last_tokens, ulong last_tokens_size, float penalty_repeat, float penalty_freq, float penalty_present)
</code></pre>
<h4 id="parameters_22">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
Pointer to LLamaTokenDataArray</p>
<p><code>last_tokens</code> <a href="./llama.native.llamatoken*.md">LLamaToken*</a><br></p>
<p><code>last_tokens_size</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br></p>
<p><code>penalty_repeat</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br>
Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.</p>
<p><code>penalty_freq</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br>
Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.</p>
<p><code>penalty_present</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br>
Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.</p>
<h3 id="llama_sample_apply_guidancesafellamacontexthandle-spansingle-readonlyspansingle-single"><strong>llama_sample_apply_guidance(SafeLLamaContextHandle, Span&lt;Single&gt;, ReadOnlySpan&lt;Single&gt;, Single)</strong></h3>
<p>Apply classifier-free guidance to the logits as described in academic paper "Stay on topic with Classifier-Free Guidance" https://arxiv.org/abs/2306.17806</p>
<pre><code class="language-csharp">public static void llama_sample_apply_guidance(SafeLLamaContextHandle ctx, Span&lt;float&gt; logits, ReadOnlySpan&lt;float&gt; logits_guidance, float scale)
</code></pre>
<h4 id="parameters_23">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>logits</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.span-1">Span&lt;Single&gt;</a><br>
Logits extracted from the original generation context.</p>
<p><code>logits_guidance</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.readonlyspan-1">ReadOnlySpan&lt;Single&gt;</a><br>
Logits extracted from a separate context from the same model.
 Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.</p>
<p><code>scale</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br>
Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance.</p>
<h3 id="llama_sample_apply_guidancesafellamacontexthandle-single-single-single"><strong>llama_sample_apply_guidance(SafeLLamaContextHandle, Single<em>, Single</em>, Single)</strong></h3>
<p>Apply classifier-free guidance to the logits as described in academic paper "Stay on topic with Classifier-Free Guidance" https://arxiv.org/abs/2306.17806</p>
<pre><code class="language-csharp">public static void llama_sample_apply_guidance(SafeLLamaContextHandle ctx, Single* logits, Single* logits_guidance, float scale)
</code></pre>
<h4 id="parameters_24">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>logits</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single*">Single*</a><br>
Logits extracted from the original generation context.</p>
<p><code>logits_guidance</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single*">Single*</a><br>
Logits extracted from a separate context from the same model.
 Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.</p>
<p><code>scale</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br>
Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance.</p>
<h3 id="llama_sample_softmaxsafellamacontexthandle-llamatokendataarraynative"><strong>llama_sample_softmax(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;)</strong></h3>
<p>Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.</p>
<pre><code class="language-csharp">public static void llama_sample_softmax(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates)
</code></pre>
<h4 id="parameters_25">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
Pointer to LLamaTokenDataArray</p>
<h3 id="llama_sample_top_ksafellamacontexthandle-llamatokendataarraynative-int32-uint64"><strong>llama_sample_top_k(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Int32, UInt64)</strong></h3>
<p>Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751</p>
<pre><code class="language-csharp">public static void llama_sample_top_k(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, int k, ulong min_keep)
</code></pre>
<h4 id="parameters_26">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
Pointer to LLamaTokenDataArray</p>
<p><code>k</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<p><code>min_keep</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br></p>
<h3 id="llama_sample_top_psafellamacontexthandle-llamatokendataarraynative-single-uint64"><strong>llama_sample_top_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)</strong></h3>
<p>Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751</p>
<pre><code class="language-csharp">public static void llama_sample_top_p(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float p, ulong min_keep)
</code></pre>
<h4 id="parameters_27">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
Pointer to LLamaTokenDataArray</p>
<p><code>p</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br></p>
<p><code>min_keep</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br></p>
<h3 id="llama_sample_min_psafellamacontexthandle-llamatokendataarraynative-single-uint64"><strong>llama_sample_min_p(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)</strong></h3>
<p>Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841</p>
<pre><code class="language-csharp">public static void llama_sample_min_p(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float p, ulong min_keep)
</code></pre>
<h4 id="parameters_28">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
Pointer to LLamaTokenDataArray</p>
<p><code>p</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br></p>
<p><code>min_keep</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br></p>
<h3 id="llama_sample_tail_freesafellamacontexthandle-llamatokendataarraynative-single-uint64"><strong>llama_sample_tail_free(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)</strong></h3>
<p>Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.</p>
<pre><code class="language-csharp">public static void llama_sample_tail_free(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float z, ulong min_keep)
</code></pre>
<h4 id="parameters_29">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
Pointer to LLamaTokenDataArray</p>
<p><code>z</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br></p>
<p><code>min_keep</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br></p>
<h3 id="llama_sample_typicalsafellamacontexthandle-llamatokendataarraynative-single-uint64"><strong>llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, UInt64)</strong></h3>
<p>Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.</p>
<pre><code class="language-csharp">public static void llama_sample_typical(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float p, ulong min_keep)
</code></pre>
<h4 id="parameters_30">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
Pointer to LLamaTokenDataArray</p>
<p><code>p</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br></p>
<p><code>min_keep</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br></p>
<h3 id="llama_sample_typicalsafellamacontexthandle-llamatokendataarraynative-single-single-single"><strong>llama_sample_typical(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single, Single, Single)</strong></h3>
<p>Dynamic temperature implementation described in the paper https://arxiv.org/abs/2309.02772.</p>
<pre><code class="language-csharp">public static void llama_sample_typical(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float min_temp, float max_temp, float exponent_val)
</code></pre>
<h4 id="parameters_31">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br>
Pointer to LLamaTokenDataArray</p>
<p><code>min_temp</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br></p>
<p><code>max_temp</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br></p>
<p><code>exponent_val</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br></p>
<h3 id="llama_sample_tempsafellamacontexthandle-llamatokendataarraynative-single"><strong>llama_sample_temp(SafeLLamaContextHandle, LLamaTokenDataArrayNative&amp;, Single)</strong></h3>
<p>Modify logits by temperature</p>
<pre><code class="language-csharp">public static void llama_sample_temp(SafeLLamaContextHandle ctx, LLamaTokenDataArrayNative&amp; candidates, float temp)
</code></pre>
<h4 id="parameters_32">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>candidates</code> <a href="./llama.native.llamatokendataarraynative&amp;.md">LLamaTokenDataArrayNative&amp;</a><br></p>
<p><code>temp</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br></p>
<h3 id="llama_get_embeddingssafellamacontexthandle"><strong>llama_get_embeddings(SafeLLamaContextHandle)</strong></h3>
<p>Get the embeddings for the input</p>
<pre><code class="language-csharp">public static Span&lt;float&gt; llama_get_embeddings(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_33">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h4 id="returns_17">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.span-1">Span&lt;Single&gt;</a><br></p>
<h3 id="llama_chat_apply_templatesafellamamodelhandle-char-llamachatmessage-intptr-boolean-char-int32"><strong>llama_chat_apply_template(SafeLlamaModelHandle, Char<em>, LLamaChatMessage</em>, IntPtr, Boolean, Char*, Int32)</strong></h3>
<p>Apply chat template. Inspired by hf apply_chat_template() on python.
 Both "model" and "custom_template" are optional, but at least one is required. "custom_template" has higher precedence than "model"
 NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template</p>
<pre><code class="language-csharp">public static int llama_chat_apply_template(SafeLlamaModelHandle model, Char* tmpl, LLamaChatMessage* chat, IntPtr n_msg, bool add_ass, Char* buf, int length)
</code></pre>
<h4 id="parameters_34">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<p><code>tmpl</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.char*">Char*</a><br>
A Jinja template to use for this chat. If this is nullptr, the models default chat template will be used instead.</p>
<p><code>chat</code> <a href="./llama.native.llamachatmessage*.md">LLamaChatMessage*</a><br>
Pointer to a list of multiple llama_chat_message</p>
<p><code>n_msg</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.intptr">IntPtr</a><br>
Number of llama_chat_message in this chat</p>
<p><code>add_ass</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.boolean">Boolean</a><br>
Whether to end the prompt with the token(s) that indicate the start of an assistant message.</p>
<p><code>buf</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.char*">Char*</a><br>
A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)</p>
<p><code>length</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
The size of the allocated buffer</p>
<h4 id="returns_18">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template.</p>
<h3 id="llama_token_bossafellamamodelhandle"><strong>llama_token_bos(SafeLlamaModelHandle)</strong></h3>
<p>Get the "Beginning of sentence" token</p>
<pre><code class="language-csharp">public static LLamaToken llama_token_bos(SafeLlamaModelHandle model)
</code></pre>
<h4 id="parameters_35">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<h4 id="returns_19">Returns</h4>
<p><a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<h3 id="llama_token_eossafellamamodelhandle"><strong>llama_token_eos(SafeLlamaModelHandle)</strong></h3>
<p>Get the "End of sentence" token</p>
<pre><code class="language-csharp">public static LLamaToken llama_token_eos(SafeLlamaModelHandle model)
</code></pre>
<h4 id="parameters_36">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<h4 id="returns_20">Returns</h4>
<p><a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<h3 id="llama_token_nlsafellamamodelhandle"><strong>llama_token_nl(SafeLlamaModelHandle)</strong></h3>
<p>Get the "new line" token</p>
<pre><code class="language-csharp">public static LLamaToken llama_token_nl(SafeLlamaModelHandle model)
</code></pre>
<h4 id="parameters_37">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<h4 id="returns_21">Returns</h4>
<p><a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<h3 id="llama_add_bos_tokensafellamamodelhandle"><strong>llama_add_bos_token(SafeLlamaModelHandle)</strong></h3>
<p>Returns -1 if unknown, 1 for true or 0 for false.</p>
<pre><code class="language-csharp">public static int llama_add_bos_token(SafeLlamaModelHandle model)
</code></pre>
<h4 id="parameters_38">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<h4 id="returns_22">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h3 id="llama_add_eos_tokensafellamamodelhandle"><strong>llama_add_eos_token(SafeLlamaModelHandle)</strong></h3>
<p>Returns -1 if unknown, 1 for true or 0 for false.</p>
<pre><code class="language-csharp">public static int llama_add_eos_token(SafeLlamaModelHandle model)
</code></pre>
<h4 id="parameters_39">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<h4 id="returns_23">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h3 id="llama_token_prefixsafellamamodelhandle"><strong>llama_token_prefix(SafeLlamaModelHandle)</strong></h3>
<p>codellama infill tokens, Beginning of infill prefix</p>
<pre><code class="language-csharp">public static int llama_token_prefix(SafeLlamaModelHandle model)
</code></pre>
<h4 id="parameters_40">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<h4 id="returns_24">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h3 id="llama_token_middlesafellamamodelhandle"><strong>llama_token_middle(SafeLlamaModelHandle)</strong></h3>
<p>codellama infill tokens, Beginning of infill middle</p>
<pre><code class="language-csharp">public static int llama_token_middle(SafeLlamaModelHandle model)
</code></pre>
<h4 id="parameters_41">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<h4 id="returns_25">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h3 id="llama_token_suffixsafellamamodelhandle"><strong>llama_token_suffix(SafeLlamaModelHandle)</strong></h3>
<p>codellama infill tokens, Beginning of infill suffix</p>
<pre><code class="language-csharp">public static int llama_token_suffix(SafeLlamaModelHandle model)
</code></pre>
<h4 id="parameters_42">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<h4 id="returns_26">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h3 id="llama_token_eotsafellamamodelhandle"><strong>llama_token_eot(SafeLlamaModelHandle)</strong></h3>
<p>codellama infill tokens, End of infill middle</p>
<pre><code class="language-csharp">public static int llama_token_eot(SafeLlamaModelHandle model)
</code></pre>
<h4 id="parameters_43">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<h4 id="returns_27">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h3 id="llama_print_timingssafellamacontexthandle"><strong>llama_print_timings(SafeLLamaContextHandle)</strong></h3>
<p>Print out timing information for this context</p>
<pre><code class="language-csharp">public static void llama_print_timings(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_44">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h3 id="llama_reset_timingssafellamacontexthandle"><strong>llama_reset_timings(SafeLLamaContextHandle)</strong></h3>
<p>Reset all collected timing information for this context</p>
<pre><code class="language-csharp">public static void llama_reset_timings(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_45">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h3 id="llama_print_system_info"><strong>llama_print_system_info()</strong></h3>
<p>Print system information</p>
<pre><code class="language-csharp">public static IntPtr llama_print_system_info()
</code></pre>
<h4 id="returns_28">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.intptr">IntPtr</a><br></p>
<h3 id="llama_token_to_piecesafellamamodelhandle-llamatoken-spanbyte"><strong>llama_token_to_piece(SafeLlamaModelHandle, LLamaToken, Span&lt;Byte&gt;)</strong></h3>
<p>Convert a single token into text</p>
<pre><code class="language-csharp">public static int llama_token_to_piece(SafeLlamaModelHandle model, LLamaToken llamaToken, Span&lt;byte&gt; buffer)
</code></pre>
<h4 id="parameters_46">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<p><code>llamaToken</code> <a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<p><code>buffer</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.span-1">Span&lt;Byte&gt;</a><br>
buffer to write string into</p>
<h4 id="returns_29">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
The length written, or if the buffer is too small a negative that indicates the length required</p>
<h3 id="llama_tokenizesafellamamodelhandle-byte-int32-llamatoken-int32-boolean-boolean"><strong>llama_tokenize(SafeLlamaModelHandle, Byte<em>, Int32, LLamaToken</em>, Int32, Boolean, Boolean)</strong></h3>
<p>Convert text into tokens</p>
<pre><code class="language-csharp">public static int llama_tokenize(SafeLlamaModelHandle model, Byte* text, int text_len, LLamaToken* tokens, int n_max_tokens, bool add_bos, bool special)
</code></pre>
<h4 id="parameters_47">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<p><code>text</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.byte*">Byte*</a><br></p>
<p><code>text_len</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<p><code>tokens</code> <a href="./llama.native.llamatoken*.md">LLamaToken*</a><br></p>
<p><code>n_max_tokens</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<p><code>add_bos</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.boolean">Boolean</a><br></p>
<p><code>special</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.boolean">Boolean</a><br>
Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext. Does not insert a leading space.</p>
<h4 id="returns_30">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
Returns the number of tokens on success, no more than n_max_tokens.
 Returns a negative number on failure - the number of tokens that would have been returned</p>
<h3 id="llama_log_setllamalogcallback"><strong>llama_log_set(LLamaLogCallback)</strong></h3>
<p>Register a callback to receive llama log messages</p>
<pre><code class="language-csharp">public static void llama_log_set(LLamaLogCallback logCallback)
</code></pre>
<h4 id="parameters_48">Parameters</h4>
<p><code>logCallback</code> <a href="./llama.native.llamalogcallback.md">LLamaLogCallback</a><br></p>
<h3 id="llama_kv_cache_clearsafellamacontexthandle"><strong>llama_kv_cache_clear(SafeLLamaContextHandle)</strong></h3>
<p>Clear the KV cache</p>
<pre><code class="language-csharp">public static void llama_kv_cache_clear(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_49">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h3 id="llama_kv_cache_seq_rmsafellamacontexthandle-llamaseqid-llamapos-llamapos"><strong>llama_kv_cache_seq_rm(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos)</strong></h3>
<p>Removes all tokens that belong to the specified sequence and have positions in [p0, p1)</p>
<pre><code class="language-csharp">public static void llama_kv_cache_seq_rm(SafeLLamaContextHandle ctx, LLamaSeqId seq, LLamaPos p0, LLamaPos p1)
</code></pre>
<h4 id="parameters_50">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>seq</code> <a href="../llama.native.llamaseqid/">LLamaSeqId</a><br></p>
<p><code>p0</code> <a href="../llama.native.llamapos/">LLamaPos</a><br></p>
<p><code>p1</code> <a href="../llama.native.llamapos/">LLamaPos</a><br></p>
<h3 id="llama_kv_cache_seq_cpsafellamacontexthandle-llamaseqid-llamaseqid-llamapos-llamapos"><strong>llama_kv_cache_seq_cp(SafeLLamaContextHandle, LLamaSeqId, LLamaSeqId, LLamaPos, LLamaPos)</strong></h3>
<p>Copy all tokens that belong to the specified sequence to another sequence
 Note that this does not allocate extra KV cache memory - it simply assigns the tokens to the new sequence</p>
<pre><code class="language-csharp">public static void llama_kv_cache_seq_cp(SafeLLamaContextHandle ctx, LLamaSeqId src, LLamaSeqId dest, LLamaPos p0, LLamaPos p1)
</code></pre>
<h4 id="parameters_51">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>src</code> <a href="../llama.native.llamaseqid/">LLamaSeqId</a><br></p>
<p><code>dest</code> <a href="../llama.native.llamaseqid/">LLamaSeqId</a><br></p>
<p><code>p0</code> <a href="../llama.native.llamapos/">LLamaPos</a><br></p>
<p><code>p1</code> <a href="../llama.native.llamapos/">LLamaPos</a><br></p>
<h3 id="llama_kv_cache_seq_keepsafellamacontexthandle-llamaseqid"><strong>llama_kv_cache_seq_keep(SafeLLamaContextHandle, LLamaSeqId)</strong></h3>
<p>Removes all tokens that do not belong to the specified sequence</p>
<pre><code class="language-csharp">public static void llama_kv_cache_seq_keep(SafeLLamaContextHandle ctx, LLamaSeqId seq)
</code></pre>
<h4 id="parameters_52">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>seq</code> <a href="../llama.native.llamaseqid/">LLamaSeqId</a><br></p>
<h3 id="llama_kv_cache_seq_addsafellamacontexthandle-llamaseqid-llamapos-llamapos-int32"><strong>llama_kv_cache_seq_add(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)</strong></h3>
<p>Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)
 If the KV cache is RoPEd, the KV data is updated accordingly:
 - lazily on next llama_decode()
 - explicitly with llama_kv_cache_update()</p>
<pre><code class="language-csharp">public static void llama_kv_cache_seq_add(SafeLLamaContextHandle ctx, LLamaSeqId seq, LLamaPos p0, LLamaPos p1, int delta)
</code></pre>
<h4 id="parameters_53">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>seq</code> <a href="../llama.native.llamaseqid/">LLamaSeqId</a><br></p>
<p><code>p0</code> <a href="../llama.native.llamapos/">LLamaPos</a><br></p>
<p><code>p1</code> <a href="../llama.native.llamapos/">LLamaPos</a><br></p>
<p><code>delta</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h3 id="llama_kv_cache_seq_divsafellamacontexthandle-llamaseqid-llamapos-llamapos-int32"><strong>llama_kv_cache_seq_div(SafeLLamaContextHandle, LLamaSeqId, LLamaPos, LLamaPos, Int32)</strong></h3>
<p>Integer division of the positions by factor of <code>d &amp;gt; 1</code>
 If the KV cache is RoPEd, the KV data is updated accordingly:
 - lazily on next llama_decode()
 - explicitly with llama_kv_cache_update()
 <br>
 p0 &lt; 0 : [0, p1]
 <br>
 p1 &lt; 0 : [p0, inf)</p>
<pre><code class="language-csharp">public static void llama_kv_cache_seq_div(SafeLLamaContextHandle ctx, LLamaSeqId seq, LLamaPos p0, LLamaPos p1, int d)
</code></pre>
<h4 id="parameters_54">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>seq</code> <a href="../llama.native.llamaseqid/">LLamaSeqId</a><br></p>
<p><code>p0</code> <a href="../llama.native.llamapos/">LLamaPos</a><br></p>
<p><code>p1</code> <a href="../llama.native.llamapos/">LLamaPos</a><br></p>
<p><code>d</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h3 id="llama_kv_cache_seq_pos_maxsafellamacontexthandle-llamaseqid"><strong>llama_kv_cache_seq_pos_max(SafeLLamaContextHandle, LLamaSeqId)</strong></h3>
<p>Returns the largest position present in the KV cache for the specified sequence</p>
<pre><code class="language-csharp">public static LLamaPos llama_kv_cache_seq_pos_max(SafeLLamaContextHandle ctx, LLamaSeqId seq)
</code></pre>
<h4 id="parameters_55">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>seq</code> <a href="../llama.native.llamaseqid/">LLamaSeqId</a><br></p>
<h4 id="returns_31">Returns</h4>
<p><a href="../llama.native.llamapos/">LLamaPos</a><br></p>
<h3 id="llama_kv_cache_defragsafellamacontexthandle"><strong>llama_kv_cache_defrag(SafeLLamaContextHandle)</strong></h3>
<p>Defragment the KV cache. This will be applied:
 - lazily on next llama_decode()
 - explicitly with llama_kv_cache_update()</p>
<pre><code class="language-csharp">public static LLamaPos llama_kv_cache_defrag(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_56">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h4 id="returns_32">Returns</h4>
<p><a href="../llama.native.llamapos/">LLamaPos</a><br></p>
<h3 id="llama_kv_cache_updatesafellamacontexthandle"><strong>llama_kv_cache_update(SafeLLamaContextHandle)</strong></h3>
<p>Apply the KV cache updates (such as K-shifts, defragmentation, etc.)</p>
<pre><code class="language-csharp">public static void llama_kv_cache_update(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_57">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h3 id="llama_batch_initint32-int32-int32"><strong>llama_batch_init(Int32, Int32, Int32)</strong></h3>
<p>Allocates a batch of tokens on the heap
 Each token can be assigned up to n_seq_max sequence ids
 The batch has to be freed with llama_batch_free()
 If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)
 Otherwise, llama_batch.token will be allocated to store n_tokens llama_token
 The rest of the llama_batch members are allocated with size n_tokens
 All members are left uninitialized</p>
<pre><code class="language-csharp">public static LLamaNativeBatch llama_batch_init(int n_tokens, int embd, int n_seq_max)
</code></pre>
<h4 id="parameters_58">Parameters</h4>
<p><code>n_tokens</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<p><code>embd</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<p><code>n_seq_max</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
Each token can be assigned up to n_seq_max sequence ids</p>
<h4 id="returns_33">Returns</h4>
<p><a href="../llama.native.llamanativebatch/">LLamaNativeBatch</a><br></p>
<h3 id="llama_batch_freellamanativebatch"><strong>llama_batch_free(LLamaNativeBatch)</strong></h3>
<p>Frees a batch of tokens allocated with llama_batch_init()</p>
<pre><code class="language-csharp">public static void llama_batch_free(LLamaNativeBatch batch)
</code></pre>
<h4 id="parameters_59">Parameters</h4>
<p><code>batch</code> <a href="../llama.native.llamanativebatch/">LLamaNativeBatch</a><br></p>
<h3 id="llama_decodesafellamacontexthandle-llamanativebatch"><strong>llama_decode(SafeLLamaContextHandle, LLamaNativeBatch)</strong></h3>
<pre><code class="language-csharp">public static int llama_decode(SafeLLamaContextHandle ctx, LLamaNativeBatch batch)
</code></pre>
<h4 id="parameters_60">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>batch</code> <a href="../llama.native.llamanativebatch/">LLamaNativeBatch</a><br></p>
<h4 id="returns_34">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
Positive return values does not mean a fatal error, but rather a warning:<br>
 - 0: success<br>
 - 1: could not find a KV slot for the batch (try reducing the size of the batch or increase the context)<br>
 - &lt; 0: error<br></p>
<h3 id="llama_kv_cache_view_initsafellamacontexthandle-int32"><strong>llama_kv_cache_view_init(SafeLLamaContextHandle, Int32)</strong></h3>
<p>Create an empty KV cache view. (use only for debugging purposes)</p>
<pre><code class="language-csharp">public static LLamaKvCacheView llama_kv_cache_view_init(SafeLLamaContextHandle ctx, int n_max_seq)
</code></pre>
<h4 id="parameters_61">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>n_max_seq</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h4 id="returns_35">Returns</h4>
<p><a href="../llama.native.llamakvcacheview/">LLamaKvCacheView</a><br></p>
<h3 id="llama_kv_cache_view_freellamakvcacheview"><strong>llama_kv_cache_view_free(LLamaKvCacheView&amp;)</strong></h3>
<p>Free a KV cache view. (use only for debugging purposes)</p>
<pre><code class="language-csharp">public static void llama_kv_cache_view_free(LLamaKvCacheView&amp; view)
</code></pre>
<h4 id="parameters_62">Parameters</h4>
<p><code>view</code> <a href="./llama.native.llamakvcacheview&amp;.md">LLamaKvCacheView&amp;</a><br></p>
<h3 id="llama_kv_cache_view_updatesafellamacontexthandle-llamakvcacheview"><strong>llama_kv_cache_view_update(SafeLLamaContextHandle, LLamaKvCacheView&amp;)</strong></h3>
<p>Update the KV cache view structure with the current state of the KV cache. (use only for debugging purposes)</p>
<pre><code class="language-csharp">public static void llama_kv_cache_view_update(SafeLLamaContextHandle ctx, LLamaKvCacheView&amp; view)
</code></pre>
<h4 id="parameters_63">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>view</code> <a href="./llama.native.llamakvcacheview&amp;.md">LLamaKvCacheView&amp;</a><br></p>
<h3 id="llama_get_kv_cache_token_countsafellamacontexthandle"><strong>llama_get_kv_cache_token_count(SafeLLamaContextHandle)</strong></h3>
<p>Returns the number of tokens in the KV cache (slow, use only for debug)
 If a KV cell has multiple sequences assigned to it, it will be counted multiple times</p>
<pre><code class="language-csharp">public static int llama_get_kv_cache_token_count(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_64">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h4 id="returns_36">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h3 id="llama_get_kv_cache_used_cellssafellamacontexthandle"><strong>llama_get_kv_cache_used_cells(SafeLLamaContextHandle)</strong></h3>
<p>Returns the number of used KV cells (i.e. have at least one sequence assigned to them)</p>
<pre><code class="language-csharp">public static int llama_get_kv_cache_used_cells(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_65">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h4 id="returns_37">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h3 id="llama_beam_searchsafellamacontexthandle-llamabeamsearchcallback-intptr-uint64-int32-int32-int32"><strong>llama_beam_search(SafeLLamaContextHandle, LLamaBeamSearchCallback, IntPtr, UInt64, Int32, Int32, Int32)</strong></h3>
<p>Deterministically returns entire sentence constructed by a beam search.</p>
<pre><code class="language-csharp">public static void llama_beam_search(SafeLLamaContextHandle ctx, LLamaBeamSearchCallback callback, IntPtr callback_data, ulong n_beams, int n_past, int n_predict, int n_threads)
</code></pre>
<h4 id="parameters_66">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br>
Pointer to the llama_context.</p>
<p><code>callback</code> <a href="./llama.native.nativeapi.llamabeamsearchcallback.md">LLamaBeamSearchCallback</a><br>
Invoked for each iteration of the beam_search loop, passing in beams_state.</p>
<p><code>callback_data</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.intptr">IntPtr</a><br>
A pointer that is simply passed back to callback.</p>
<p><code>n_beams</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br>
Number of beams to use.</p>
<p><code>n_past</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
Number of tokens already evaluated.</p>
<p><code>n_predict</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
Maximum number of tokens to predict. EOS may occur earlier.</p>
<p><code>n_threads</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br>
Number of threads.</p>
<h3 id="llama_empty_call"><strong>llama_empty_call()</strong></h3>
<p>A method that does nothing. This is a native method, calling it will force the llama native dependencies to be loaded.</p>
<pre><code class="language-csharp">public static void llama_empty_call()
</code></pre>
<h3 id="llama_max_devices"><strong>llama_max_devices()</strong></h3>
<p>Get the maximum number of devices supported by llama.cpp</p>
<pre><code class="language-csharp">public static long llama_max_devices()
</code></pre>
<h4 id="returns_38">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.int64">Int64</a><br></p>
<h3 id="llama_model_default_params"><strong>llama_model_default_params()</strong></h3>
<p>Create a LLamaModelParams with default values</p>
<pre><code class="language-csharp">public static LLamaModelParams llama_model_default_params()
</code></pre>
<h4 id="returns_39">Returns</h4>
<p><a href="../llama.native.llamamodelparams/">LLamaModelParams</a><br></p>
<h3 id="llama_context_default_params"><strong>llama_context_default_params()</strong></h3>
<p>Create a LLamaContextParams with default values</p>
<pre><code class="language-csharp">public static LLamaContextParams llama_context_default_params()
</code></pre>
<h4 id="returns_40">Returns</h4>
<p><a href="../llama.native.llamacontextparams/">LLamaContextParams</a><br></p>
<h3 id="llama_model_quantize_default_params"><strong>llama_model_quantize_default_params()</strong></h3>
<p>Create a LLamaModelQuantizeParams with default values</p>
<pre><code class="language-csharp">public static LLamaModelQuantizeParams llama_model_quantize_default_params()
</code></pre>
<h4 id="returns_41">Returns</h4>
<p><a href="../llama.native.llamamodelquantizeparams/">LLamaModelQuantizeParams</a><br></p>
<h3 id="llama_supports_mmap"><strong>llama_supports_mmap()</strong></h3>
<p>Check if memory mapping is supported</p>
<pre><code class="language-csharp">public static bool llama_supports_mmap()
</code></pre>
<h4 id="returns_42">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.boolean">Boolean</a><br></p>
<h3 id="llama_supports_mlock"><strong>llama_supports_mlock()</strong></h3>
<p>Check if memory locking is supported</p>
<pre><code class="language-csharp">public static bool llama_supports_mlock()
</code></pre>
<h4 id="returns_43">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.boolean">Boolean</a><br></p>
<h3 id="llama_supports_gpu_offload"><strong>llama_supports_gpu_offload()</strong></h3>
<p>Check if GPU offload is supported</p>
<pre><code class="language-csharp">public static bool llama_supports_gpu_offload()
</code></pre>
<h4 id="returns_44">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.boolean">Boolean</a><br></p>
<h3 id="llama_set_rng_seedsafellamacontexthandle-uint32"><strong>llama_set_rng_seed(SafeLLamaContextHandle, UInt32)</strong></h3>
<p>Sets the current rng seed.</p>
<pre><code class="language-csharp">public static void llama_set_rng_seed(SafeLLamaContextHandle ctx, uint seed)
</code></pre>
<h4 id="parameters_67">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>seed</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint32">UInt32</a><br></p>
<h3 id="llama_get_state_sizesafellamacontexthandle"><strong>llama_get_state_size(SafeLLamaContextHandle)</strong></h3>
<p>Returns the maximum size in bytes of the state (rng, logits, embedding
 and kv_cache) - will often be smaller after compacting tokens</p>
<pre><code class="language-csharp">public static ulong llama_get_state_size(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_68">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h4 id="returns_45">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br></p>
<h3 id="llama_copy_state_datasafellamacontexthandle-byte"><strong>llama_copy_state_data(SafeLLamaContextHandle, Byte*)</strong></h3>
<p>Copies the state to the specified destination address.
 Destination needs to have allocated enough memory.</p>
<pre><code class="language-csharp">public static ulong llama_copy_state_data(SafeLLamaContextHandle ctx, Byte* dest)
</code></pre>
<h4 id="parameters_69">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>dest</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.byte*">Byte*</a><br></p>
<h4 id="returns_46">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br>
the number of bytes copied</p>
<h3 id="llama_set_state_datasafellamacontexthandle-byte"><strong>llama_set_state_data(SafeLLamaContextHandle, Byte*)</strong></h3>
<p>Set the state reading from the specified address</p>
<pre><code class="language-csharp">public static ulong llama_set_state_data(SafeLLamaContextHandle ctx, Byte* src)
</code></pre>
<h4 id="parameters_70">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>src</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.byte*">Byte*</a><br></p>
<h4 id="returns_47">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br>
the number of bytes read</p>
<h3 id="llama_load_session_filesafellamacontexthandle-string-llamatoken-uint64-uint64"><strong>llama_load_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64, UInt64&amp;)</strong></h3>
<p>Load session file</p>
<pre><code class="language-csharp">public static bool llama_load_session_file(SafeLLamaContextHandle ctx, string path_session, LLamaToken[] tokens_out, ulong n_token_capacity, UInt64&amp; n_token_count_out)
</code></pre>
<h4 id="parameters_71">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>path_session</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.string">String</a><br></p>
<p><code>tokens_out</code> <a href="../llama.native.llamatoken/">LLamaToken[]</a><br></p>
<p><code>n_token_capacity</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br></p>
<p><code>n_token_count_out</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64&amp;">UInt64&amp;</a><br></p>
<h4 id="returns_48">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.boolean">Boolean</a><br></p>
<h3 id="llama_save_session_filesafellamacontexthandle-string-llamatoken-uint64"><strong>llama_save_session_file(SafeLLamaContextHandle, String, LLamaToken[], UInt64)</strong></h3>
<p>Save session file</p>
<pre><code class="language-csharp">public static bool llama_save_session_file(SafeLLamaContextHandle ctx, string path_session, LLamaToken[] tokens, ulong n_token_count)
</code></pre>
<h4 id="parameters_72">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>path_session</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.string">String</a><br></p>
<p><code>tokens</code> <a href="../llama.native.llamatoken/">LLamaToken[]</a><br></p>
<p><code>n_token_count</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint64">UInt64</a><br></p>
<h4 id="returns_49">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.boolean">Boolean</a><br></p>
<h3 id="llama_token_get_textsafellamamodelhandle-llamatoken"><strong>llama_token_get_text(SafeLlamaModelHandle, LLamaToken)</strong></h3>
<pre><code class="language-csharp">public static Byte* llama_token_get_text(SafeLlamaModelHandle model, LLamaToken token)
</code></pre>
<h4 id="parameters_73">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<p><code>token</code> <a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<h4 id="returns_50">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.byte*">Byte*</a><br></p>
<h3 id="llama_token_get_scoresafellamamodelhandle-llamatoken"><strong>llama_token_get_score(SafeLlamaModelHandle, LLamaToken)</strong></h3>
<pre><code class="language-csharp">public static float llama_token_get_score(SafeLlamaModelHandle model, LLamaToken token)
</code></pre>
<h4 id="parameters_74">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<p><code>token</code> <a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<h4 id="returns_51">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.single">Single</a><br></p>
<h3 id="llama_token_get_typesafellamamodelhandle-llamatoken"><strong>llama_token_get_type(SafeLlamaModelHandle, LLamaToken)</strong></h3>
<pre><code class="language-csharp">public static LLamaTokenType llama_token_get_type(SafeLlamaModelHandle model, LLamaToken token)
</code></pre>
<h4 id="parameters_75">Parameters</h4>
<p><code>model</code> <a href="../llama.native.safellamamodelhandle/">SafeLlamaModelHandle</a><br></p>
<p><code>token</code> <a href="../llama.native.llamatoken/">LLamaToken</a><br></p>
<h4 id="returns_52">Returns</h4>
<p><a href="../llama.native.llamatokentype/">LLamaTokenType</a><br></p>
<h3 id="llama_n_ctxsafellamacontexthandle"><strong>llama_n_ctx(SafeLLamaContextHandle)</strong></h3>
<p>Get the size of the context window for the model for this context</p>
<pre><code class="language-csharp">public static uint llama_n_ctx(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_76">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h4 id="returns_53">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint32">UInt32</a><br></p>
<h3 id="llama_n_batchsafellamacontexthandle"><strong>llama_n_batch(SafeLLamaContextHandle)</strong></h3>
<p>Get the batch size for this context</p>
<pre><code class="language-csharp">public static uint llama_n_batch(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_77">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h4 id="returns_54">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.uint32">UInt32</a><br></p>
<h3 id="llama_get_logitssafellamacontexthandle"><strong>llama_get_logits(SafeLLamaContextHandle)</strong></h3>
<p>Token logits obtained from the last call to llama_decode
 The logits for the last token are stored in the last row
 Can be mutated in order to change the probabilities of the next token.<br>
 Rows: n_tokens<br>
 Cols: n_vocab</p>
<pre><code class="language-csharp">public static Single* llama_get_logits(SafeLLamaContextHandle ctx)
</code></pre>
<h4 id="parameters_78">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<h4 id="returns_55">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.single*">Single*</a><br></p>
<h3 id="llama_get_logits_ithsafellamacontexthandle-int32"><strong>llama_get_logits_ith(SafeLLamaContextHandle, Int32)</strong></h3>
<p>Logits for the ith token. Equivalent to: llama_get_logits(ctx) + i*n_vocab</p>
<pre><code class="language-csharp">public static Single* llama_get_logits_ith(SafeLLamaContextHandle ctx, int i)
</code></pre>
<h4 id="parameters_79">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>i</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h4 id="returns_56">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.single*">Single*</a><br></p>
<h3 id="llama_get_embeddings_ithsafellamacontexthandle-int32"><strong>llama_get_embeddings_ith(SafeLLamaContextHandle, Int32)</strong></h3>
<p>Get the embeddings for the ith sequence. Equivalent to: llama_get_embeddings(ctx) + i*n_embd</p>
<pre><code class="language-csharp">public static Single* llama_get_embeddings_ith(SafeLLamaContextHandle ctx, int i)
</code></pre>
<h4 id="parameters_80">Parameters</h4>
<p><code>ctx</code> <a href="../llama.native.safellamacontexthandle/">SafeLLamaContextHandle</a><br></p>
<p><code>i</code> <a href="https://docs.microsoft.com/en-us/dotnet/api/system.int32">Int32</a><br></p>
<h4 id="returns_57">Returns</h4>
<p><a href="https://docs.microsoft.com/en-us/dotnet/api/system.single*">Single*</a><br></p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.220ee61c.min.js"></script>
      
    
  </body>
</html>